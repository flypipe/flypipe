{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b48a36d-b260-4193-bdbd-94c91d6f37b7",
   "metadata": {},
   "source": [
    "## Unit testing a node\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* pytest>=7.1.3\n",
    "* pytest-mock>=3.9.0\n",
    "* pyspark-test>=0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6d1a8a-532f-4a32-8bfa-245b89e3d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pandas as pd\n",
    "from pyspark_test import assert_pyspark_df_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "from flypipe.node import node\n",
    "from flypipe.datasource.spark import Spark\n",
    "from flypipe.schema.column import Column\n",
    "from flypipe.schema.schema import Schema\n",
    "from flypipe.schema.types import Long\n",
    "\n",
    "#Fixtures\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def spark():\n",
    "    # If running local, please set up your Spark environment\n",
    "    from flypipe.tests.spark import spark\n",
    "\n",
    "    # create a temporary view\n",
    "    (\n",
    "        spark.createDataFrame(\n",
    "            schema=(\"c1\", \"c2\", \"c3\"), data=[(1, 2, 3)]\n",
    "        ).createOrReplaceTempView(\"dummy_table\")\n",
    "    )\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d1735-8c5e-40f1-b176-4e6635bbe8e0",
   "metadata": {},
   "source": [
    "### Pyspark Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425b698e-5c08-407f-a694-2eb92e610dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPyspark:\n",
    "    \n",
    "    def test_(self, spark):\n",
    "        \"\"\"\n",
    "        Pyspark node test\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(1,)])\n",
    "\n",
    "        @node(\n",
    "            type=\"pyspark\",\n",
    "            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n",
    "            output=Schema([Column(\"c1\", Long())]),\n",
    "        )\n",
    "        def t1(dummy_table):\n",
    "            return dummy_table\n",
    "\n",
    "        df = t1.run(spark)\n",
    "        assert_pyspark_df_equal(df, expected_df)\n",
    "        \n",
    "    def test_pypsark_with_provided_inputs(self, spark):\n",
    "        \"\"\"\n",
    "        Pyspark node test with provided inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})\n",
    "        expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(10,)])\n",
    "\n",
    "        @node(\n",
    "            type=\"pyspark\",\n",
    "            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n",
    "            output=Schema([Column(\"c1\", Long())]),\n",
    "        )\n",
    "        def t1(dummy_table):\n",
    "            return dummy_table\n",
    "\n",
    "        df = t1.run(\n",
    "            spark,\n",
    "            inputs={\n",
    "                Spark(\"dummy_table\"): dummy_table_df\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        assert_pyspark_df_equal(df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90a788-5d7f-4aa3-b867-3d3f6ef44494",
   "metadata": {},
   "source": [
    "### Pandas on Spark Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8cc451-c92a-4461-97fc-35a770d79c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPandasOnSpark:\n",
    "    \n",
    "    def test_pandas_on_spark_node(self, spark):\n",
    "        \"\"\"\n",
    "        Pandas on Spark node test\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(1,)]).pandas_api()\n",
    "\n",
    "        @node(\n",
    "            type=\"pandas_on_spark\",\n",
    "            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n",
    "            output=Schema([Column(\"c1\", Long())]),\n",
    "        )\n",
    "        def t1(dummy_table):\n",
    "            return dummy_table\n",
    "\n",
    "        df = t1.run(spark)\n",
    "        assert_pyspark_df_equal(df.to_spark(), expected_df.to_spark())\n",
    "        \n",
    "    def test_pandas_on_spark_node_without_spark_context(self):\n",
    "        \"\"\"\n",
    "        Pandas on Spark node test\n",
    "        \"\"\"\n",
    "        \n",
    "        dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})\n",
    "        expected_df = pd.DataFrame(data={'c1': [10]})\n",
    "\n",
    "        @node(\n",
    "            type=\"pandas_on_spark\",\n",
    "            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n",
    "            output=Schema([Column(\"c1\", Long())]),\n",
    "        )\n",
    "        def t1(dummy_table):\n",
    "            return dummy_table\n",
    "\n",
    "        df = t1.run(\n",
    "            pandas_on_spark_use_pandas=True, # <-- \n",
    "            inputs={\n",
    "                Spark(\"dummy_table\"): dummy_table_df\n",
    "            }\n",
    "        )\n",
    "        assert_frame_equal(df, expected_df)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbef81c-a5c2-40be-8b17-869c1bf4a217",
   "metadata": {},
   "source": [
    "### Pandas Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d6fe35-a070-460b-a887-3ac669efc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPandasNode:\n",
    "    \n",
    "    def test_(self, spark):\n",
    "        \"\"\"\n",
    "        Pandas node test\n",
    "        \"\"\"\n",
    "        \n",
    "        expected_df = pd.DataFrame(data={'c1': [1]})\n",
    "\n",
    "        @node(\n",
    "            type=\"pandas\",\n",
    "            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n",
    "            output=Schema([Column(\"c1\", Long())]),\n",
    "        )\n",
    "        def t1(dummy_table):\n",
    "            return dummy_table\n",
    "\n",
    "        df = t1.run(spark)\n",
    "        assert_frame_equal(df, expected_df)\n",
    "        \n",
    "        \n",
    "    def test_pandas_with_provided_inputs(self):\n",
    "        \"\"\"\n",
    "        Pandas node test with provided inputs\n",
    "        \n",
    "        NOTE: observe that spark is not used here\n",
    "        \"\"\"\n",
    "        \n",
    "        dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})\n",
    "        expected_df = pd.DataFrame(data={'c1': [10]})\n",
    "\n",
    "        @node(\n",
    "            type=\"pandas\",\n",
    "            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n",
    "            output=Schema([Column(\"c1\", Long())]),\n",
    "        )\n",
    "        def t1(dummy_table):\n",
    "            return dummy_table\n",
    "\n",
    "        df = t1.run(\n",
    "            inputs={\n",
    "                Spark(\"dummy_table\"): dummy_table_df\n",
    "            }\n",
    "        )\n",
    "        assert_frame_equal(df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a9e71-83c1-49e6-a192-98a2060782b8",
   "metadata": {},
   "source": [
    "Running tests (**jupyter notebooks only**)\n",
    "\n",
    "`pip install ipytest>=0.13.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476fb262-e7bc-458b-b30b-e5140fc93e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.9.17, pytest-7.1.3, pluggy-1.2.0\n",
      "rootdir: /notebooks/docs/tutorial\n",
      "plugins: anyio-3.7.0, mock-3.9.0, xdist-3.3.1\n",
      "collected 6 items\n",
      "\n",
      "t_61fe84b0633f469880cbe1488792adea.py "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
      "t_61fe84b0633f469880cbe1488792adea.py: 14 warnings\n",
      "  /usr/local/lib/python3.9/site-packages/pyspark/pandas/typedef/typehints.py:181: DeprecationWarning: Converting `np.character` to a dtype is deprecated. The current result is `np.dtype(np.str_)` which is not strictly correct. Note that `np.character` is generally deprecated and 'S1' should be used.\n",
      "    elif tpe in (bytes, np.character, np.bytes_, np.string_):\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================================== \u001b[32m6 passed\u001b[0m, \u001b[33m\u001b[1m14 warnings\u001b[0m\u001b[33m in 6.14s\u001b[0m\u001b[33m ==================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipytest\n",
    "ipytest.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
