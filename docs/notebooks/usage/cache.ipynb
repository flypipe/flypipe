{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0cbb43-f766-443a-908d-d413671246fc",
   "metadata": {},
   "source": [
    "Some nodes can represent tables or can be quite expensive to re-compute on every run. In these cases, it is possible to cache or persist the output of the node as you want.\n",
    "\n",
    "If you want to enable cache for a specific node you have to crate a `class` that tells Flypipe how to `read`, `write` and check if the cache or persisted data `exists`.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "``` py\n",
    "from flypipe.cache import Cache\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MyCustomPersistance(Cache):\n",
    "    def __init__(self, csv_path_name: str):\n",
    "        self.csv_path_name = csv_path_name\n",
    "\n",
    "    def read(self, spark):\n",
    "        \"\"\"\n",
    "        Reads the persisted/cached data into a dataframe\n",
    "        \"\"\"\n",
    "        return pd.read_csv(self.csv_path_name)\n",
    "\n",
    "    def write(self, spark, df):\n",
    "        \"\"\"\n",
    "        Cache or persist the data\n",
    "        \"\"\"\n",
    "        df.to_csv(self.csv_path_name, index=False)\n",
    "        \n",
    "    def exists(self, spark):\n",
    "        \"\"\"\n",
    "        Check if the data has been cached or persisted.\n",
    "        \"\"\"\n",
    "        return os.path.exists(self.csv_path_name)\n",
    "```\n",
    "\n",
    "Having defined your cache/persistance class, you can start marking nodes to be cached, for instance:\n",
    "\n",
    "``` py\n",
    "@node(\n",
    "    ...\n",
    "    cache = MyCustomPersistance(\"data.csv\")\n",
    "    ...\n",
    ")\n",
    "def t0():\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5d7fc-8c07-4d66-911d-bb2fa72e7d0a",
   "metadata": {},
   "source": [
    "## Cache/Persistance workflow\n",
    "\n",
    "For every node that has cache set up, Flypipe will do the following:\n",
    "\n",
    "Node has cache?<br>\n",
    "    &emsp;Yes -> cache exists (runs the method `exists`)?<br>\n",
    "          &emsp;&emsp;Yes -> runs `read` method and returns the dataframe<br>\n",
    "          &emsp;&emsp;No -> runs the node, collects the output dataframe and runs `write` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e59eb83-dd91-4071-857f-6497a6032a57",
   "metadata": {},
   "source": [
    "## Example 1: CSV persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe353d-8bb0-4caf-bfc3-fa069b133162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from flypipe.cache import Cache\n",
    "\n",
    "\n",
    "class SaveAsCSV(Cache):\n",
    "    def __init__(self, csv_path_name: str):\n",
    "        self.csv_path_name = csv_path_name\n",
    "\n",
    "    def read(self, spark):\n",
    "        print(f\"Reading CSV `{self.csv_path_name}`...\")\n",
    "        return pd.read_csv(self.csv_path_name)\n",
    "\n",
    "    def write(self, spark, df):\n",
    "        print(f\"Writing CSV `{self.csv_path_name}`...\")\n",
    "        df.to_csv(self.csv_path_name, index=False)\n",
    "        \n",
    "    def exists(self, spark):\n",
    "        csv_exists = os.path.exists(self.csv_path_name)\n",
    "        print(f\"CSV `{self.csv_path_name}` exists?\", csv_exists)\n",
    "        return os.path.exists(self.csv_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816257d1-ac91-4163-ad1d-fa089b6b04d4",
   "metadata": {},
   "source": [
    "## Example 2: Spark persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3573216-09c0-4e56-b960-e477524a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from flypipe.cache import Cache\n",
    "\n",
    "\n",
    "class SparkTable(Cache):\n",
    "    def __init__(self, \n",
    "                 table_name: str, \n",
    "                 schema: str,\n",
    "                 merge_keys: List[str] = None,\n",
    "                 partition_columns: List[str] = None,\n",
    "                 schema_location: str = None):\n",
    "        \n",
    "        self.table_name = table_name\n",
    "        self.schema = schema\n",
    "        self.merge_keys = merge_keys\n",
    "        self.partition_columns = partition_columns\n",
    "        self.schema_location = schema_location\n",
    "    \n",
    "    @property\n",
    "    def table(self):\n",
    "        return f\"{self.schema}.{self.table_name}\"\n",
    "    \n",
    "    def read(self, spark):\n",
    "        return spark.table(self.table)\n",
    "\n",
    "    def write(self, spark, df):\n",
    "        \n",
    "        # check if database exists\n",
    "        if not spark.catalog.databaseExists(self.schema):\n",
    "            print(f\"Creating database `{self.schema}`\")\n",
    "            location = f\"LOCATION '{self.schema_location}'\" if self.schema_location else \"\"\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.schema} {location}\")\n",
    "            \n",
    "        # check if table exists\n",
    "        if not spark.catalog.tableExists(self.table_name, self.schema):\n",
    "            print(f\"Creating table `{self.table}`\")\n",
    "            df = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "            \n",
    "            if self.partition_columns:\n",
    "                df = df.partitionBy(*self.partition_columns)\n",
    "                \n",
    "            df.saveAsTable(self.table)            \n",
    "        else:\n",
    "            # table already exists, merge into\n",
    "            print(f\"Merging into table `{self.table}`\")\n",
    "            df.createOrReplaceTempView(\"updates\")\n",
    "            keys = \" AND \".join([f\"s.{col} = t.{col}\" for col in self.merge_keys])\n",
    "            \n",
    "            merge_query = f\"\"\"\n",
    "                MERGE INTO {self.table} t\n",
    "                USING updates s\n",
    "                ON {keys}\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\"\n",
    "            df._jdf.sparkSession().sql(merge_query)\n",
    "        \n",
    "    def exists(self, spark):\n",
    "        table_exists = spark.catalog.tableExists(self.table_name, self.schema)\n",
    "        print(f\"Table {self.table} exists?\", table_exists)\n",
    "        return table_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df54111-8406-4ae5-beaf-e3bbdab8a902",
   "metadata": {},
   "source": [
    "## Execution Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad92933-c705-4377-b252-c82cd413dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flypipe import node\n",
    "from flypipe.cache import Cache \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "@node(\n",
    "    type=\"pandas\",\n",
    "    cache = SaveAsCSV(\"/tmp/data.csv\")\n",
    ")\n",
    "def csv_cache():\n",
    "    return pd.DataFrame(data={\"id\": [1, 2], \"sales\":[100.0, 34.1]})\n",
    "\n",
    "@node(\n",
    "    type=\"pyspark\",\n",
    "    cache = SparkTable(\"my_table\", \"tmp\", merge_keys=[\"id\"], schema_location=\"/tmp\"),\n",
    "    dependencies=[csv_cache.select(\"id\", \"sales\").alias(\"df\")]\n",
    ")\n",
    "def spark_cache(df):\n",
    "    return df.withColumn(\"above_50\", F.col(\"sales\") > 50.0)\n",
    "\n",
    "\n",
    "@node(\n",
    "    type=\"pyspark\",\n",
    "    dependencies=[spark_cache.select(\"id\", \"sales\", \"above_50\").alias(\"df\")]\n",
    ")\n",
    "def t0(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ef4ff-33fb-4ea3-b902-162a0ea1170e",
   "metadata": {},
   "source": [
    "### 1st run\n",
    "\n",
    "When no cache exists all cache nodes will be active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741d017-e153-4205-8096-345e93f3d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "displayHTML(t0.html(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf4c1f-49d9-4d40-9f20-e1abc2e9e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0.run(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302587a-81b2-4930-9a9a-be082eb28a3e",
   "metadata": {},
   "source": [
    "After the 1st run, all caches will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb8027-e566-4df3-b358-ff05d883e788",
   "metadata": {},
   "source": [
    "### Subsequent runs\n",
    "\n",
    "As caches have been saved, the nodes will be inactive as the caches will be loaded on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b46f2b-c7cc-4d74-8632-d8ab242a68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "displayHTML(t0.html(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c13a9f-39b2-4908-a267-a3983556ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0.run(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c5e2f-c11c-4468-9197-800acbfbe382",
   "metadata": {},
   "source": [
    "Flypipe will only load necessary caches, for instance, loading the cache of node `csv_cache` was skipped, as only cache of `spark_cache` was necessary to run `t0`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50d66e-e9b5-4269-9963-07b9c8b76470",
   "metadata": {},
   "source": [
    "## Merging Data\n",
    "\n",
    "Often we need to merge the data, insert new rows and update rows if data already exists.\n",
    "This behaviour will happen accordingly to `merge_keys`.\n",
    "\n",
    "``` py\n",
    "@node(\n",
    "    ...\n",
    "    cache = SparkTable(\n",
    "        \"my_table\", \n",
    "        \"tmp\", \n",
    "        merge_keys=[\"id\"], # <--\n",
    "        schema_location=\"/tmp\"),\n",
    "    ...\n",
    ")\n",
    "def spark_cache(df):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Rows with non-existent ids in `tmp.my_table` will be added to `tmp.my_table`, rows with existent ids, will be updated.\n",
    "\n",
    "Independent of the write mode (insert or update) the rows needs to me re-transformed by the graph, so nodes that were previously `skipped`, shall be `active` by changing the `CacheMode` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b4192-ce8c-46da-82a1-fbf1df828886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flypipe.cache import CacheMode\n",
    "\n",
    "displayHTML(\n",
    "    t0.html(\n",
    "        spark,\n",
    "        cache={\n",
    "            csv_cache: CacheMode.MERGE,\n",
    "            spark_cache: CacheMode.MERGE\n",
    "        }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6f8f8-77d5-4dca-abae-f8661795e4aa",
   "metadata": {},
   "source": [
    "Currently `tmp.my_table` data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50186fc7-973f-4a91-9118-459fde1f7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from tmp.my_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614b73a-2742-45e2-9d92-6f5c88a801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flypipe.cache import CacheMode\n",
    "\n",
    "t0.run(\n",
    "    spark,\n",
    "    cache={\n",
    "        csv_cache: CacheMode.MERGE,\n",
    "        spark_cache: CacheMode.MERGE\n",
    "    },\n",
    "    inputs={\n",
    "        csv_cache: pd.DataFrame(data={\"id\": [1, 3], \"sales\":[17.25, 547.39]})\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fecd0a-112b-4fdd-9a09-98d7b2b9b890",
   "metadata": {},
   "source": [
    "Checking the data in `tmp.my_table` we can see that:\n",
    "\n",
    "* sales changed from `100.0` to `17.25` and `above_50` to `false`.\n",
    "* row of id 2 remaining unchanged\n",
    "* added 1 row id 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997755c-26d7-4d0b-830b-c86681fb79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from tmp.my_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
