{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d74ca99-2887-4661-92c4-029c6f7da922",
   "metadata": {},
   "source": [
    "# Low-Latency Transformations with Sparkleframe\n",
    "\n",
    "Apache Spark is designed for distributed, large-scale data processing, but it is not optimized for low-latency use cases. There are scenarios, however, where you need to quickly re-compute certain data—for example, regenerating features for a machine learning model in real time or near-real time.\n",
    "\n",
    "For such cases, you can leverage [Sparkleframe](https://github.com/flypipe/sparkleframe).\n",
    "\n",
    "What Is Sparkleframe?\n",
    "\n",
    "Sparkleframe is an experimental backend for Flypipe that maps PySpark transformations to [Polars DataFrame](https://docs.pola.rs/api/python/stable/reference/index.html) operations. Polars is a high-performance, multi-threaded DataFrame library written in Rust, and is significantly faster than Spark for small to medium-scale data in local execution.\n",
    "\n",
    "By activating Sparkleframe, your existing Flypipe nodes defined as `type=\"pyspark\"` will execute using Polars under the hood—without any changes to your transformation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96ebce-3ba5-446d-8a45-e044920068f4",
   "metadata": {},
   "source": [
    "## Basic Example: Spark vs Sparkleframe\n",
    "\n",
    "Below is a simple example using Flypipe and a Spark node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe10ddfd-bd41-4b8c-b68f-8db62867c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a\n",
      "0  11\n",
      "\n",
      "Type of dataframe returned <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "===> Time taken: 1909.74 milliseconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from flypipe import node\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "@node(\n",
    "    type=\"pyspark\",\n",
    "    spark_context=True,\n",
    ")\n",
    "def my_node(spark):\n",
    "    return spark.createDataFrame([{\"a\": 1}])\n",
    "\n",
    "@node(\n",
    "    type=\"pyspark\",\n",
    "    dependencies=[my_node.alias(\"df\")]\n",
    ")\n",
    "def add_10(df):\n",
    "    return df.withColumn(\"a\", col(\"a\") + 10)\n",
    "\n",
    "start = time()\n",
    "df = add_10.run(spark)\n",
    "print(df.toPandas())\n",
    "print(f\"\\nType of dataframe returned {type(df)}\")\n",
    "print(f\"===> Time taken: {round((time() - start)*1000, 2)} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52bdd04-9cac-4dcf-81ee-921be2fa282e",
   "metadata": {},
   "source": [
    "## Enabling Sparkleframe\n",
    "\n",
    "To switch to the Sparkleframe backend and drastically reduce execution time, just activate it at the **beginning of your script**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc400ee-76c4-428d-8839-a82dcbf342c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkleframe.activate import activate\n",
    "activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cfc63-2975-4c5d-bfb3-f66e1c189c8b",
   "metadata": {},
   "source": [
    "Run the same code again, and the transformation will execute using Polars instead of Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d49b8e0-eba7-4efa-b6b0-2b326be22874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a\n",
      "0  11\n",
      "\n",
      "Type of dataframe returned <class 'sparkleframe.polarsdf.dataframe.DataFrame'>\n",
      "===> Time taken: 33.05 milliseconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from flypipe import node\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "@node(\n",
    "    type=\"pyspark\",\n",
    "    spark_context=True,\n",
    ")\n",
    "def my_node(spark):\n",
    "    return spark.createDataFrame([{\"a\": 1}])\n",
    "\n",
    "@node(\n",
    "    type=\"pyspark\",\n",
    "    dependencies=[my_node.alias(\"df\")]\n",
    ")\n",
    "def add_10(df):\n",
    "    return df.withColumn(\"a\", col(\"a\") + 10)\n",
    "\n",
    "start = time()\n",
    "df = add_10.run(spark)\n",
    "print(df.toPandas())\n",
    "print(f\"\\nType of dataframe returned {type(df)}\")\n",
    "print(f\"===> Time taken: {round((time() - start)*1000, 2)} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19199dbc-3bbc-4bf3-be72-9d54656d95f4",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Sparkleframe is still under development, and not all PySpark operations are currently supported.\n",
    "* If you encounter any transformation that is not implemented, please open an [issue on GitHub](https://github.com/flypipe/sparkleframe/issues) so it can be prioritized.\n",
    "* Sparkleframe is especially useful for unit testing, feature prototyping, or serving small pipelines in microservices.\n",
    "\n",
    "You can learn more about the design motivation behind Sparkleframe in this [discussion thread](https://github.com/eakmanrq/sqlframe/issues/409)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
