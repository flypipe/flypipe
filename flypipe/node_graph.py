from copy import copy, deepcopy
from enum import Enum
from typing import List

import networkx as nx

from flypipe.node import Node
from flypipe.node_run_data import NodeRunData, RunStatus
from flypipe.node_type import NodeType
from flypipe.utils import DataFrameType


class NodeGraph:

    def __init__(self, transformation: Node, graph=None, skipped_node_keys=None,
                 pandas_on_spark_use_pandas=False, provided_parameters=None):
        """
        Given a transformation node, traverse the transformations the node is dependant upon and build a graph from
        this.
        """

        if graph:
            self.graph = graph
        else:
            self.graph = self._build_graph(transformation, pandas_on_spark_use_pandas, provided_parameters)

        if not skipped_node_keys:
            skipped_node_keys = []
        self.skipped_node_keys = skipped_node_keys
        self.generate_nodes(pandas_on_spark_use_pandas=pandas_on_spark_use_pandas)
        self.calculate_graph_run_status()

    def _build_graph(self, transformation: Node, pandas_on_spark_use_pandas: bool, provided_parameters: dict=None) -> nx.DiGraph:
        transformation = deepcopy(transformation)
        provided_parameters = provided_parameters or {}
        graph = nx.DiGraph()

        # TODO- move this to pandas_on_spark_node once we figure out how to get context to work
        # TODO- create a copy of the node, as in databricks it keeps the objects with type changed until the state is cleared
        if pandas_on_spark_use_pandas and transformation.type == DataFrameType.PANDAS_ON_SPARK:
            transformation.type = DataFrameType.PANDAS

        transformation_parameters = provided_parameters[transformation.key] if transformation.key in provided_parameters else {}
        graph.add_node(
            transformation.key,
            transformation=transformation,
            run_data=NodeRunData(transformation.output_schema, transformation_parameters),
        )
        # We implicitly select all of the available output columns for the final node
        graph.nodes[transformation.key]['run_data'].add_output_columns(None)

        frontier = [transformation]
        while frontier:
            current_transformation = frontier.pop()

            if current_transformation.input_nodes:
                for input_node in current_transformation.input_nodes:
                    input_node = deepcopy(input_node)
                    if input_node.key not in graph.nodes:

                        # TODO- move this to pandas_on_spark_node once we figure out how to get context to work
                        # TODO- create a copy of the node, as in databricks it keeps the objects with type changed until the state is cleared
                        if pandas_on_spark_use_pandas and input_node.node.type == DataFrameType.PANDAS_ON_SPARK:
                            input_node.node.type = DataFrameType.PANDAS

                        transformation_parameters = provided_parameters[input_node.key] \
                            if input_node.key in provided_parameters else {}

                        graph.add_node(
                            input_node.key,
                            transformation=input_node.node,
                            run_data=NodeRunData(input_node.node.output_schema, transformation_parameters),
                        )
                    graph.nodes[input_node.key]['run_data'].add_output_columns(input_node.selected_columns)

                    graph.add_edge(
                        input_node.key,
                        current_transformation.key,
                        selected_columns=input_node.selected_columns
                    )
                    frontier.insert(0, input_node.node)
        return graph

    def generate_nodes(self, pandas_on_spark_use_pandas: bool):
        nodes_generated = {}

        # generate nodes with graph output columns
        for node_name in self.graph:

            node = self.graph.nodes[node_name]

            if node['transformation'].node_type == NodeType.GENERATOR:
                generator_parameters = node['run_data'].parameters

                generator_key = node['transformation'].key

                if node['transformation'].requested_columns:
                    generated_node = node['transformation'].function(node['run_data'].output_columns,
                                                                     **generator_parameters)
                else:
                    generated_node = node['transformation'].function(**generator_parameters)
                node['run_data'].clear_parameters()
                # build the graph generated by this generator
                new_node_graph = self._build_graph(generated_node, pandas_on_spark_use_pandas)

                # graph had node generator before, need to relable this key with the node generated key
                self.graph = nx.relabel_nodes(self.graph, {node_name: generated_node.key})

                # save the old generator key and the generated node to update node inputs bellow
                nodes_generated[generator_key] = generated_node

                # compose a new graph
                # the order when compose graph matters as we want new_node_graph
                # to take precedent over attributes from self.graph
                # more info see https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.operators.binary.compose.html
                # save graph requested columns from the graph to the generated output node
                node_data = {}
                for n in new_node_graph.nodes:
                    if n in self.graph:
                        node_data[n] = self.graph.nodes[n]['run_data']
                nx.set_node_attributes(new_node_graph, node_data, 'run_data')

                self.graph = nx.compose(self.graph, new_node_graph)

        # update any node input of node_type genrator with the node generatated
        for node_name in self.graph:

            if pandas_on_spark_use_pandas and self.graph.nodes[node_name]['transformation'].type == DataFrameType.PANDAS_ON_SPARK:
                self.graph.nodes[node_name]['transformation'].type = DataFrameType.PANDAS

            input_nodes = self.graph.nodes[node_name]['transformation'].input_nodes
            for i, input_node in enumerate(input_nodes):
                if input_node.node.node_type == NodeType.GENERATOR:
                    self.graph.nodes[node_name]['transformation'].input_nodes[i].node = \
                    nodes_generated[input_node.node.key]

                    if pandas_on_spark_use_pandas and \
                            self.graph.nodes[node_name]['transformation'].input_nodes[i].node.type == DataFrameType.PANDAS_ON_SPARK:
                        self.graph.nodes[node_name]['transformation'].input_nodes[i].node.type = DataFrameType.PANDAS



    def get_node(self, name: str):
        return self.graph.nodes[name]

    def get_edges(self):
        return self.graph.edges

    def get_edge_data(self, source_node_name, target_node_name):
        return self.graph.get_edge_data(source_node_name, target_node_name)

    def get_transformation(self, name: str) -> Node:
        return self.get_node(name)['transformation']

    def get_end_node_name(self):
        for name in self.graph:
            if self.graph.out_degree[name] == 0:
                return name

    def calculate_graph_run_status(self):
        # because the last node can be a generator, we have to get the last node node
        # after building the graph
        node_name = self.get_end_node_name()
        skipped_node_keys = set(self.skipped_node_keys)

        frontier = [(node_name, RunStatus.SKIP if node_name in skipped_node_keys else RunStatus.ACTIVE)]
        while len(frontier) != 0:
            current_node_name, descendent_status = frontier.pop()
            current_node = self.graph.nodes[current_node_name]
            if descendent_status == RunStatus.ACTIVE:
                current_node['run_data'].status = RunStatus.ACTIVE
                for ancestor_name in self.graph.predecessors(current_node_name):
                    if ancestor_name in skipped_node_keys:
                        frontier.append((ancestor_name, RunStatus.SKIP))
                    else:
                        frontier.append((ancestor_name, RunStatus.ACTIVE))
            else:
                current_node['run_data'].status = RunStatus.SKIP
                for ancestor_name in self.graph.predecessors(current_node_name):
                    if self.graph.nodes[ancestor_name]['run_data'].status != RunStatus.ACTIVE:
                        frontier.append((ancestor_name, RunStatus.SKIP))


    def get_dependency_map(self):
        dependencies = {}
        for node in self.graph.nodes:
            dependencies[node] = set()
        for source, destination in self.graph.edges:
            dependencies[destination].add(source)
        return dependencies

    def get_nodes_depth(self):
        """
        Return a map of node names to their depth in the graph, depth being the minimal distance to the root/first node.
        """
        # TODO- this function is failing unit tests, I can see 3 issues:
        # - The depth of each node is maximal not minimal, that is if there is a path of length 2 to the start node and a
        # path of length 1 it uses 2 as the depth where it should be 1.
        # - Depth is 1 more than it ought to be. The start node should have depth 0 not depth 1.
        # - We ought to be having the node name as the key and not the value.
        end_node = [node_name for node_name, num_out_edges in self.graph.out_degree if num_out_edges==0][0]

        nodes_depth = {}
        for node in self.graph:
            depth = len(
                max(list(nx.all_simple_paths(self.graph, node, end_node)), key=lambda x: len(x), default=[end_node]))

            if depth not in nodes_depth:
                nodes_depth[depth] = [node]
            else:
                nodes_depth[depth] += [node]

        max_depth = max(nodes_depth.keys())

        return {-1 * k + max_depth + 1: v for k, v in nodes_depth.items()}

    def pop_runnable_transformations(self) -> List[Node]:
        candidate_node_names = [node_name for node_name in self.graph if self.graph.in_degree(node_name)==0]
        runnable_node_names = filter(lambda node_name: self.graph.nodes[node_name]['run_data'].status == RunStatus.ACTIVE,
                                     candidate_node_names)
        runnable_nodes = [self.get_node(node_name) for node_name in runnable_node_names]
        for node_name in candidate_node_names:
            self.graph.remove_node(node_name)
        return runnable_nodes

    def is_empty(self):
        return nx.number_of_nodes(self.graph) == 0

    def plot(self):

        from matplotlib import pyplot as plt

        plt.title(f'Transformation Graph')
        nx.draw(self.graph, with_labels=True)
        plt.show()

    def copy(self):
        return NodeGraph(None, graph=self.graph.copy(), skipped_node_keys=self.skipped_node_keys)
