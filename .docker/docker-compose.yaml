version: "3.9"

services:
  # ----------------
  # Spark Master
  # ----------------
  spark-master:
    image: spark:3.5.7-python3@sha256:5b75b64a42010ba072c2b0a987cc8d1c4b9ec70e0e99b1c72c7f1871855ff631
    container_name: spark-master
    hostname: spark-master
    command: >
      bash -lc "
      ${SPARK_HOME:-/opt/spark}/bin/spark-class
      org.apache.spark.deploy.master.Master
      --host spark-master --port 7077 --webui-port 8080"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_LOG_DIR=/opt/spark/logs
    volumes:
      - ./logs/spark-master:/opt/spark/logs
    ports:
      - "7077:7077"   # Spark master RPC
      - "8080:8080"   # Spark master UI
    networks: [sparknet]

  # ----------------
  # Spark Worker
  # ----------------
  spark-worker:
    image: spark:3.5.7-python3@sha256:5b75b64a42010ba072c2b0a987cc8d1c4b9ec70e0e99b1c72c7f1871855ff631
    container_name: spark-worker
    hostname: spark-worker
    depends_on: [spark-master]
    command: >
      bash -lc "
      ${SPARK_HOME:-/opt/spark}/bin/spark-class
      org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
      --webui-port 8081"
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_NO_DAEMONIZE=true
      - SPARK_LOG_DIR=/opt/spark/logs
    volumes:
      - ./logs/spark-worker:/opt/spark/logs
    ports:
      - "8081:8081"   # Spark worker UI
    networks: [sparknet]

  # ----------------
  # Spark Connect
  # ----------------
  spark-connect:
    image: spark:3.5.7-python3@sha256:5b75b64a42010ba072c2b0a987cc8d1c4b9ec70e0e99b1c72c7f1871855ff631
    container_name: spark-connect
    hostname: spark-connect
    user: "root"
    environment:
      - HOME=/root
      - SPARK_LOG_DIR=/opt/spark/logs
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./logs/spark-connect:/opt/spark/logs
    depends_on: [spark-master]
    command: >
      bash -lc "
      ${SPARK_HOME:-/opt/spark}/sbin/start-connect-server.sh --wait
      --master spark://spark-master:7077
      --packages org.apache.spark:spark-connect_2.12:3.5.7,io.delta:delta-spark_2.12:3.3.0
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.connect.grpc.binding.address=0.0.0.0
      --conf spark.connect.grpc.binding.port=15002
      --conf spark.ui.port=4041
      "
    ports:
      - "15002:15002"
      - "4041:4041"
    healthcheck:
      test: [ "CMD-SHELL", "ss -lnt | awk '{print $4}' | grep -q ':15002' || netstat -lnt | awk '{print $4}' | grep -q ':15002'" ]
      interval: 5s
      timeout: 3s
      retries: 20
    networks: [sparknet]

  # ----------------
  # Jupyter / Dev box
  # ----------------
  flypipe-jupyter:
    build:
      context: ../
      dockerfile: .docker/Dockerfile
    container_name: flypipe-jupyter
    hostname: jupyter
    depends_on: [spark-master, spark-worker, spark-connect]
    ports:
      - "8888:8888"
      - "4040:4040"   # optional: Spark UI if you use local[*]
    environment:
      FLYPIPE_DEFAULT_RUN_MODE: sequential
      GIT_PYTHON_REFRESH: quiet
    volumes:
      - ../.pylintrc:/.pylintrc
      - ../.coverage:/flypipe/.coverage
      - ../flypipe:/flypipe
      - ../scripts:/scripts
      - ./start.py:/etc/ipython/startup/start.py
      - ../docs/notebooks:/notebooks
    networks: [sparknet]
    entrypoint: >
      bash -lc "
      /wait-for-it.sh spark-master:7077 -t 60 &&
      /wait-for-it.sh spark-worker:8081 -t 60 &&
      /wait-for-it.sh spark-connect:15002 -t 120 &&
      jupyter-lab --notebook-dir /notebooks --allow-root
      --no-browser --port 8888 --ip 0.0.0.0
      --NotebookApp.token='' --NotebookApp.password=''
      "

networks:
  sparknet:
    driver: bridge
