{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Flypipe is a Python framework to simplify development, management and maintenance of transformation pipelines, which are  commonly used in the data, feature and ML model space.</p> <p>Each transformation is implemented in a small, composable function, a special decorator is then used to define it as a  Flypipe node, which is the primary model Flypipe uses. Metadata on the node decorator allows for multiple nodes to be  linked together into a Directed Acyclic Graph (DAG). </p> <p>As each node (transformation) is connected to its ancestors, we can easily view the pipeline graphically in a html page  (<code>my_graph.html()</code>) or execute it by invoking <code>my_graph.run()</code></p> <p></p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install flypipe\n</code></pre> <p>Package published in PyPI.</p>"},{"location":"#example-usage","title":"Example Usage","text":"<p><pre><code>import pandas as pd\nfrom flypipe.node import node\n\n\n@node(\n  type=\"pandas\"\n)\ndef t0():\n  return pd.DataFrame(data={\"fruit\": [\"mango\", \"lemon\"]})\n\n\n@node(\n  type=\"pandas\",\n  dependencies=[t0.select(\"fruit\").alias(\"df\")]\n)\ndef t1(df):\n  categories = {'mango': 'sweet', 'lemon': 'sour'}\n  df['flavour'] = df['fruit']\n  df = df.replace({'flavour': categories})\n  return df\n\ndf = t1.run()\n</code></pre> <pre><code>&gt;&gt;&gt; print(df)\n+----+---------+-----------+\n|    | fruit   | flavour   |\n+====+=========+===========+\n|  0 | mango   | sweet     |\n+----+---------+-----------+\n|  1 | lemon   | sour      |\n+----+---------+-----------+\n</code></pre></p>"},{"location":"#what-flypipe-aims-to-facilitate","title":"What Flypipe aims to facilitate?","text":"<ul> <li>Free open-source tool for data transformations</li> <li>Facilitate streaming pipeline development (improved use of caches)  </li> <li>Increase pipeline stability (better use of unittests)</li> <li>End-to-end transformation lineage</li> <li>Create development standards for Data Engineers, Machine Learning Engineers and Data Scientists</li> <li>Improve re-usability of transformations in different pipelines &amp; contexts via composable nodes</li> <li>Faster integration and portability of pipelines to different contexts with different available technology stacks:</li> <li>Flexibility to use and mix up pyspark/pandas on spark/pandas in transformations seamlessly</li> <li>As a simple wheel package, it's very lightweight and unopinionated about runtime environment. This allows for it to    be easily integrated into Databricks and independently of Databricks. </li> <li>Low latency for on-demand feature generation and predictions</li> <li>Framework level optimisations and dynamic transformations help to make even complex transformation pipelines low  latency. This in turn allows for on-demand feature generation/predictions.</li> </ul>"},{"location":"#commonly-used","title":"Commonly used","text":""},{"location":"#source-code","title":"Source Code","text":"<p>API code is available at https://github.com/flypipe/flypipe.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation is available at https://flypipe.github.io/flypipe/.</p>"},{"location":"cache-flow/","title":"Cache Handling Process","text":"<p>The following flow diagrams demonstrate how Flypipe interacts with a node to handle its cache management.</p>"},{"location":"cache-flow/#case-1-nodecache-is-none","title":"Case 1: <code>node.cache is None</code>","text":"<pre><code>sequenceDiagram\n  autonumber\n  Flypipe-&gt;&gt;node: node.cache is None?\n  node-&gt;&gt;Flypipe: No\n  Flypipe-&gt;&gt;node: run node\n  node-&gt;&gt;Flypipe: retrieve dataframe </code></pre>"},{"location":"cache-flow/#case-2-nodecache-is-not-none-and-cache-exists","title":"Case 2: <code>node.cache is not None</code> and cache exists","text":"<pre><code>sequenceDiagram\n  autonumber\n  Flypipe-&gt;&gt;node: node.cache is None?\n  node-&gt;&gt;Flypipe: Yes\n  Flypipe-&gt;&gt;node: Cache exists (run method `exists`)?\n  node-&gt;&gt;Flypipe: Yes\n  Flypipe-&gt;&gt;node: Read cache (run method `read`)\n  node-&gt;&gt;Flypipe: retrieve dataframe </code></pre>"},{"location":"cache-flow/#case-2-nodecache-is-not-none-and-cache-does-not-exist","title":"Case 2: <code>node.cache is not None</code> and cache does NOT exist","text":"<pre><code>sequenceDiagram\n  autonumber\n  Flypipe-&gt;&gt;node: node.cache is None?\n  node-&gt;&gt;Flypipe: Yes\n  Flypipe-&gt;&gt;node: Cache exists (run method `exists`)?\n  node-&gt;&gt;Flypipe: No\n  Flypipe-&gt;&gt;node: run node\n  node-&gt;&gt;Flypipe: retrieve dataframe\n  Flypipe-&gt;&gt;node: Save cache (run method `write`)?\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"release/4.3.4 <ul> <li>200 execute jupyter notebooks for documentation</li> </ul> release/4.3.3 <ul> <li>200 execute jupyter notebooks for documentation</li> </ul> release/4.3.2 <ul> <li>192 Separate deploy docs from deploy wheel</li> <li>190 change documentation from sphinx to mkdocs</li> </ul> release/4.3.1 <ul> <li>188 documentation failing to deploy</li> </ul> release/4.3.0 <ul> <li>186 fix changelog</li> <li>177 Integrate sparkleframe</li> </ul> release/4.2.0 <ul> <li>173 Apply Preprocess node inputs functions</li> <li>175 Fix Calculate version and changelog</li> <li>184 fix test</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>There are a number of configuration variables that can be set to control Flypipe behaviour at a global level. </p> <p>These can be set via Environment Variables or via the Context Manager </p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Below is a list of the available variables: </p>"},{"location":"configuration/#flypipe_require_node_description","title":"FLYPIPE_REQUIRE_NODE_DESCRIPTION","text":"<p>Enforces declaration of node description</p> <p>type ==boolean default <code>False</code></p>"},{"location":"configuration/#flypipe_require_schema_description","title":"FLYPIPE_REQUIRE_SCHEMA_DESCRIPTION","text":"<p>Enforces declaration of node output schema</p> <p>type boolean default <code>False</code></p>"},{"location":"configuration/#flypipe_default_run_mode","title":"FLYPIPE_DEFAULT_RUN_MODE","text":"<p>Defines the default execution mode for Flypipe pipelines:</p> <ul> <li>sequential: will process nodes sequentially</li> <li>parallel: permit Flypipe to schedule multiple nodes to be processed concurrently, note that for a node to be  processed all the usual rules about ancestors having already been executed will apply. </li> </ul> <p>type string default <code>sequential</code></p>"},{"location":"configuration/#flypipe_node_run_max_workers","title":"FLYPIPE_NODE_RUN_MAX_WORKERS","text":"<p>Sets the maximum number of workers Flypipe will use when running transformations in parallel execution mode. </p> <p>type integer default <code>os.cpu_count()</code></p> <pre><code>Beware- at the moment we don't anticipate parallel execution of nodes to be faster than sequential except for Pandas \nnodes running IO operations such as in datasource nodes. This is because most other node operations are CPU-bound, and \nPython only permits a single thread per process to execute Python bytecode. \n</code></pre>"},{"location":"configuration/#flypipe_default_dependencies_preprocess_module","title":"FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_MODULE","text":"<p>Flypipe uses <code>FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_MODULE</code> and  <code>FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_FUNCTION</code> to import the preprocess function to apply on all nodes dependencies  (if the node dependency does not have a preprocess function already set).</p> <p>for example if your function import looks like:</p> <p><code>from my_project.utils import global_preprocess</code></p> <p>the environment variables would look like:</p> <pre><code>FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_MODULE=my_project.utils\nFLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_FUNCTION=global_preprocess\n</code></pre> <p>type string default <code>None</code></p>"},{"location":"configuration/#flypipe_default_dependencies_preprocess_function","title":"FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_FUNCTION","text":"<p>See <code>FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_MODULE</code> description above </p> <p>type string default <code>None</code></p>"},{"location":"configuration/#catalog","title":"Catalog","text":""},{"location":"configuration/#catalog_count_box_tags","title":"catalog_count_box_tags","text":"<p>Which tags to show at the top of the Flypipe Catalog, seperated by commas. For each tag Flypipe will search through the  nodes in the Catalog to obtain the number of nodes that have that tag. </p> <p>type string default <code>bronze,silver,gold</code></p> <pre><code>If you are working in Databricks, you can configure environment variables for specific clusters \n(https://docs.databricks.com/clusters/configure.html#environment-variables). Commonly different teams will be using \ndifferent clusters so you can easily setup different configurations by team with this approach.  \n</code></pre>"},{"location":"configuration/#context-manager","title":"Context Manager","text":"<p>Naturally when using the context manager the configuration will only persist for the code under the context.  The environment variable map of a flypipe variable is always prefixed with FLYPIPE and uses uppercase. </p> <p>For example, to switch on the configuration <code>require_node_description</code> we can either set the environment variable  FLYPIPE_REQUIRE_NODE_DESCRIPTION=True or in the code with: </p> <pre><code>from flypipe.config import config_context\n\nwith config_context(require_node_description=True):\n    ...\n</code></pre> <p>Note that you can query the value of a configuration variable with the utility method <code>flypipe.config.get_config</code>. </p>"},{"location":"data-sources/","title":"Load Spark Table","text":"<p>flypipe.datasource.spark.Spark</p> <p>Creates a flypipe node that loads a Spark table</p> <p>Attributes:</p> Name Type Description <code>table</code> <code>str</code> <p>name of the spark table table</p> <p>Returns:</p> Type Description <p>Node</p> <p>Usage: <pre><code>@node(\n    ...\n    dependencies = [\n        Spark(\"your_spark_table_name_here\").select(\"column1\", \"column2\",...).alias(\"any_alias_df\")\n    ]\n    ...\n)\ndef my_transformation(any_alias_df):\n    return any_alias_df\n</code></pre></p> Source code in <code>flypipe/datasource/spark.py</code> <pre><code>def Spark(table):\n    \"\"\"Creates a flypipe node that loads a Spark table\n\n    Attributes:\n        table (str): name of the spark table table\n\n    Returns:\n        Node\n\n    Usage:\n    ``` py\n    @node(\n        ...\n        dependencies = [\n            Spark(\"your_spark_table_name_here\").select(\"column1\", \"column2\",...).alias(\"any_alias_df\")\n        ]\n        ...\n    )\n    def my_transformation(any_alias_df):\n        return any_alias_df\n    ```\n    \"\"\"\n\n    @node(\n        type=\"pyspark\",\n        description=f\"Spark datasource on table {table}\",\n        tags=[\"datasource\"],\n        spark_context=True,\n    )\n    def spark_datasource(spark):\n        if spark is None:\n            raise ValueError(\"Please provide a spark session, i.e. node.run(spark)\")\n\n        return spark.table(table)\n\n    spark_datasource.function.__name__ = table\n\n    key = f\"spark.{table}\"\n    spark_datasource.key = re.sub(r\"[^\\da-zA-Z]\", \"_\", key)\n    spark_datasource.node_type = NodeType.DATASOURCE\n    return spark_datasource\n</code></pre>"},{"location":"graph-catalog-ui/","title":"Graph Catalog UI","text":"<p>Calling <code>&lt;node&gt;.html()</code> on the end node of a pipeline brings up a graph of the pipeline in the Graph Catalog UI. This is  helpful for seeing the structure of a pipeline, and is particularly helpful with complex pipelines that comprise dozens  of nodes. Each node/edge is able to be interacted with in the UI to see details such as descriptions, source code, tags,  etc. </p> <pre><code># Pipeline definition\n\nimport pandas as pd\nfrom flypipe import node\n\n\n@node(\n    type='pandas',\n)\ndef a():\n    return pd.DataFrame({'a': [1,2,3]})\n\n@node(\n    type='pandas',\n    dependencies=[a]\n)\ndef b(a):\n    return a\n\n@node(\n    type='pandas',\n    dependencies=[a],\n    group=\"group_c\"\n)\ndef c1(a):\n    return a\n\n@node(\n    type='pandas',\n    dependencies=[c1],\n    group=\"group_c\"\n)\ndef c2(c1):\n    return c1\n\n@node(\n    type='pandas',\n    dependencies=[b, c2]\n)\ndef d(b, c2):\n    return pd.concat([b, c2], axis=1)\n</code></pre> <pre><code># Visualise the pipeline by invoking the html method\n# Note- in Databricks there's a displayHTML that allows for direct rendering of html:\ndisplayHTML(d.html(height=850))\n# Outside of Databricks, we'll need to put the output into a html file and load it in a browser: \nwith open('test.html', 'w', encoding='utf-8') as f:\n    f.write(d.html(height=850))\n# &lt;open test.html in browser after running this&gt;\n</code></pre> <p></p> <p>Note by giving an input sections of the graph can be skipped, this shows on the visualisation with the relevant nodes  being marked as skipped and their edges being marked differently. </p>"},{"location":"license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy][name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"node-function/","title":"Node Function","text":"<p>flypipe.node_function.node_function</p> <p>Decorator factory that returns the given function wrapped inside a NodeFunction class</p> <p>Parameters:</p> Name Type Description Default <code>requested_columns</code> <code>(bool, optional)</code> <p>List of requested columns that successors nodes are demanding from the node function, if True will retrieve <code>requested_columns</code> as named argument. Defaults to <code>False</code>.</p> required <code>node_dependencies</code> <code>(List[Node or NodeFunction], optional)</code> <p>List of external nodes that the node function is dependent on. Any node retrieved by the node function (called internal node) can only be dependent on any internal node or any node inside <code>node_dependencies</code>. True, returns spark context as argument to the function. Defaults to <code>False</code>.</p> required <code>output</code> <code>(Schema, optional)</code> <p>Defines the output schema of the node. Defaults to <code>None</code>.</p> required <p>Returns:</p> Type Description <code>List[Node]</code> <p>a list of nodes created internally</p>"},{"location":"node-function/#flypipe.node_function.node_function--examples","title":"Examples","text":"<pre><code># Syntax\n@node_function(\n    requested_columns=True,\n    node_dependencies=[\n        Spark(\"table\")\n    ]\n)\ndef my_node_function(requested_columns):\n\n    @node(\n        type=\"pandas\",\n        dependencies=[\n            Spark(\"table\").select(requested_columns).alias(\"df\")\n        ]\n    )\n    def internal_node_1(df):\n        return df\n\n    @node(\n        type=\"pandas\",\n        dependencies=[\n            internal_node_1.alias(\"df\")\n        ]\n    )\n    def internal_node_2(df):\n        return df\n\n    return internal_node_1, internal_node_2 # &lt;-- ALL INTERNAL NODES CREATED MUST BE RETURNED\n</code></pre> Source code in <code>flypipe/node_function.py</code> <pre><code>def node_function(*args, **kwargs):\n    \"\"\"\n    Decorator factory that returns the given function wrapped inside a NodeFunction class\n\n    Parameters:\n        requested_columns (bool,optional): List of requested columns that successors nodes are demanding from the node function, if True will retrieve `requested_columns` as named argument. Defaults to `False`.\n        node_dependencies (List[Node or NodeFunction],optional): List of external nodes that the node function is dependent on. Any node retrieved by the node function (called internal node) can only be dependent on any internal node or any node inside `node_dependencies`. True, returns spark context as argument to the function. Defaults to `False`.\n        output (Schema,optional): Defines the output schema of the node. Defaults to `None`.\n\n    Returns:\n        (List[Node]): a list of nodes created internally\n\n    Raises:\n        ValueError\n            If any internal node is of type NodeFunction; if any internal node has a dependency that is not to another\n            internal node and not declared in node_dependencies\n\n    # Examples\n\n    ``` py\n    # Syntax\n    @node_function(\n        requested_columns=True,\n        node_dependencies=[\n            Spark(\"table\")\n        ]\n    )\n    def my_node_function(requested_columns):\n\n        @node(\n            type=\"pandas\",\n            dependencies=[\n                Spark(\"table\").select(requested_columns).alias(\"df\")\n            ]\n        )\n        def internal_node_1(df):\n            return df\n\n        @node(\n            type=\"pandas\",\n            dependencies=[\n                internal_node_1.alias(\"df\")\n            ]\n        )\n        def internal_node_2(df):\n            return df\n\n        return internal_node_1, internal_node_2 # &lt;-- ALL INTERNAL NODES CREATED MUST BE RETURNED\n    ```\n\n    \"\"\"\n\n    def decorator(func):\n        return NodeFunction(func, *args, **kwargs)\n\n    return decorator\n</code></pre>"},{"location":"node/","title":"Node","text":"<p>flypipe.node.node</p> <p>Nodes are the fundamental building block of Flypipe. Simply apply the node function as a decorator to a transformation function in order to declare the transformation as a Flypipe node.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>Type of the node transformation \"pandas\", \"pandas_on_spark\", \"pyspark\", \"spark_sql\"</p> required <code>description</code> <code>(str, optional)</code> <p>Description of the node. Defaults to <code>None</code>.</p> required <code>group</code> <code>(str, optional)</code> <p>Group the node falls under, nodes in the same group are clustered together in the Catalog UI. Defaults to <code>None</code>.</p> required <code>tags</code> <code>(List[str], optional)</code> <p>List of tags for the node. Defaults to <code>None</code>.</p> required <code>dependencies</code> <code>(List[Node], optional)</code> <p>List of other dependent nodes. Defaults to <code>None</code>.</p> required <code>output</code> <code>(Schema, optional)</code> <p>Defines the output schema of the node. Defaults to <code>None</code>.</p> required <code>spark_context</code> <code>(bool, optional)</code> <p>True, returns spark context as argument to the function. Defaults to <code>False</code>.</p> required <p>Examples</p> <pre><code># Syntax\n@node(\n    type=\"pyspark\", \"pandas_on_spark\" or \"pandas\",\n    description=\"this is a description of what this node does\",\n    tags=[\"a\", \"list\", \"of\", \"tags\"],\n    dependencies=[other_node_1, other_node_2, ...],\n    output=Schema(\n        Column(\"col_name\", String(), \"a description of the column\"),\n    ),\n    spark_context = True or False\n)\ndef your_function_name(other_node_1, other_node_2, ...):\n    # YOUR TRANSFORMATION LOGIC HERE\n    # use pandas syntax if type is `pandas` or `pandas_on_spark`\n    # use PySpark syntax if type is `pyspark`\n    return dataframe\n</code></pre> <pre><code># Node without dependency\nfrom flypipe.node import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pandas as pd\n@node(\n    type=\"pandas\",\n    description=\"Only outputs a pandas dataframe\",\n    output=Schema(\n        t0.output.get(\"fruit\"),\n        Column(\"flavour\", String(), \"fruit flavour\")\n    )\n)\ndef t1(df):\n    return pd.DataFrame({\"fruit\": [\"mango\"], \"flavour\": [\"sweet\"]})\n</code></pre> <pre><code># Node with dependency\nfrom flypipe.node import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pandas as pd\n@node(\n    type=\"pandas\",\n    description=\"Only outputs a pandas dataframe\",\n    dependencies = [\n        t0.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        t0.output.get(\"fruit\"),\n        Column(\"flavour\", String(), \"fruit flavour\")\n    )\n)\ndef t1(df):\n    categories = {'mango': 'sweet', 'lemon': 'citric'}\n    df['flavour'] = df['fruit']\n    df = df.replace({'flavour': categories})\n    return df\n</code></pre> Source code in <code>flypipe/node.py</code> <pre><code>def node(type, *args, **kwargs):\n    \"\"\"Nodes are the fundamental building block of Flypipe. Simply apply the node function as a decorator to a\n    transformation function in order to declare the transformation as a Flypipe node.\n\n    Parameters:\n        type (str): Type of the node transformation \"pandas\", \"pandas_on_spark\", \"pyspark\", \"spark_sql\"\n        description (str,optional): Description of the node. Defaults to `None`.\n        group (str,optional): Group the node falls under, nodes in the same group are clustered together in the Catalog UI. Defaults to `None`.\n        tags (List[str],optional): List of tags for the node. Defaults to `None`.\n        dependencies (List[Node],optional): List of other dependent nodes. Defaults to `None`.\n        output (Schema,optional): Defines the output schema of the node. Defaults to `None`.\n        spark_context (bool,optional): True, returns spark context as argument to the function. Defaults to `False`.\n\n    Examples\n\n    ``` py\n    # Syntax\n    @node(\n        type=\"pyspark\", \"pandas_on_spark\" or \"pandas\",\n        description=\"this is a description of what this node does\",\n        tags=[\"a\", \"list\", \"of\", \"tags\"],\n        dependencies=[other_node_1, other_node_2, ...],\n        output=Schema(\n            Column(\"col_name\", String(), \"a description of the column\"),\n        ),\n        spark_context = True or False\n    )\n    def your_function_name(other_node_1, other_node_2, ...):\n        # YOUR TRANSFORMATION LOGIC HERE\n        # use pandas syntax if type is `pandas` or `pandas_on_spark`\n        # use PySpark syntax if type is `pyspark`\n        return dataframe\n    ```\n\n    ``` py\n    # Node without dependency\n    from flypipe.node import node\n    from flypipe.schema import Schema, Column\n    from flypipe.schema.types import String\n    import pandas as pd\n    @node(\n        type=\"pandas\",\n        description=\"Only outputs a pandas dataframe\",\n        output=Schema(\n            t0.output.get(\"fruit\"),\n            Column(\"flavour\", String(), \"fruit flavour\")\n        )\n    )\n    def t1(df):\n        return pd.DataFrame({\"fruit\": [\"mango\"], \"flavour\": [\"sweet\"]})\n    ```\n\n    ``` py\n    # Node with dependency\n    from flypipe.node import node\n    from flypipe.schema import Schema, Column\n    from flypipe.schema.types import String\n    import pandas as pd\n    @node(\n        type=\"pandas\",\n        description=\"Only outputs a pandas dataframe\",\n        dependencies = [\n            t0.select(\"fruit\").alias(\"df\")\n        ],\n        output=Schema(\n            t0.output.get(\"fruit\"),\n            Column(\"flavour\", String(), \"fruit flavour\")\n        )\n    )\n    def t1(df):\n        categories = {'mango': 'sweet', 'lemon': 'citric'}\n        df['flavour'] = df['fruit']\n        df = df.replace({'flavour': categories})\n        return df\n    ```\n\n    \"\"\"\n\n    def decorator(func):\n        kwargs[\"type\"] = type\n        return Node(func, *args, **kwargs)\n\n    return decorator\n</code></pre>"},{"location":"preprocessing-flow/","title":"How it works?","text":"<p>You can apply functions on the dataframes of the dependencies before they are available for your node. </p> <p>Suppose you have a <code>nodeB</code> that has <code>nodeA</code> as dependency, once the <code>nodeA</code> outputs the dataframe Flypipe will check  if there is a preprocess function set on the dependency <code>nodeA</code>, if exists, it will apply the preprocess function and  pass the result on to <code>nodeB</code>.</p> <p>Here is the syntax to activate preprocess:</p> Preprocessing Syntax<pre><code>def filter_date(df):\n    return df.filter(df.value &gt;= '2025-01-03')\n\n@node(...)\ndef nodeA():\n    return df\n\n@node(\n    dependencies=[\n        nodeA.preprocess(filter_date).alias(\"df\") # (1)\n    ]\n)\ndef nodeB(df):\n    return df\n\n@node(\n    dependencies=[\n        nodeA.alias(\"df\")\n    ]\n)\ndef nodeC(df):\n    return df    \n</code></pre> <ol> <li>Flypipe will apply <code>filter_date</code> on the output dataframe of <code>nodeA</code> to <code>nodeB</code></li> </ol>"},{"location":"preprocessing-flow/#how-dataframes-are-propagated-throughout-the-nodes","title":"How dataframes are propagated throughout the nodes?","text":"<pre><code>---\nconfig:\n  flowchart:\n    htmlLabels: true\n---\nflowchart LR\n  nodeA(nodeA) e1@----&gt; table([\"\n    &lt;table style='border-color:gray; border-style:solid; border-width:1px; font-size:10px;'&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;id&lt;/th&gt;\n                &lt;th&gt;date&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            &lt;tr&gt;\n                &lt;td&gt;1&lt;/td&gt;\n                &lt;td&gt;2025-01-01&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;2&lt;/td&gt;\n                &lt;td&gt;2025-01-02&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;3&lt;/td&gt;\n                &lt;td&gt;2025-01-03&lt;/td&gt;\n            &lt;/tr&gt;      \n        &lt;/tbody&gt;\n    &lt;/table&gt;\n  \"]);\n  e1@{ animate: true };\n\n  table e2@----&gt; preprocessing@{ shape: das, label: \"preprocess&lt;br/&gt;filter_date(df)\" } \n  e2@{ animate: true };\n\n  preprocessing e3@----&gt; preprocessedTable([\"\n     &lt;table style='border-color:gray; border-style:solid; border-width:1px; font-size:10px;'&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;id&lt;/th&gt;\n                &lt;th&gt;date&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            &lt;tr&gt;\n                &lt;td&gt;3&lt;/td&gt;\n                &lt;td&gt;2025-01-03&lt;/td&gt;\n            &lt;/tr&gt;\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n  \"]);\n  e3@{ animate: true };\n\n  preprocessedTable e4@----&gt; nodeB(nodeB);\n  e4@{ animate: true };\n\n  table e5@----&gt; nodeC(nodeC);\n  e5@{ animate: true };\n\n  style table fill:None,stroke-width:0px;\n  style preprocessedTable fill:None,stroke-width:0px;</code></pre>"},{"location":"preprocessing-flow/#disabling-preprocess","title":"Disabling preprocess","text":""},{"location":"preprocessing-flow/#disable-all-preprocess-in-the-graph","title":"Disable all preprocess in the graph","text":"<pre><code>from flypipe.mode import PreprocessMode\n\nany_node.run(preprocess=PreprocessMode.DISABLE)\n</code></pre>"},{"location":"preprocessing-flow/#disable-specific-nodes-preprocess-dependencies","title":"Disable specific nodes preprocess dependencies","text":"<pre><code>from flypipe.mode import PreprocessMode\n\ndf = any_node.run(preprocess={    \n    other_node: {node_depencency: PreprocessMode.DISABLE}\n})\n</code></pre>"},{"location":"preprocessing-flow/#enable-preprocess-for-all-dependencies-by-default","title":"Enable preprocess for all dependencies by default","text":"<p>Some cases you just might need to apply preprocess on all nodes dependencies, for example, you want to run a preprocess function that filters only new data for CDC (change data capture) pipelines.</p> <p>Setting environment variables <code>FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_MODULE</code> and <code>FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_FUNCTION</code>,  will tell flypipe to use and apply the function to all dependencies of all nodes</p> <p>for example if your function import looks like:</p> <p><code>from my_project.utils import global_preprocess</code></p> <p>the environment variables would look like:</p> <pre><code>FLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_MODULE=my_project.utils\nFLYPIPE_DEFAULT_DEPENDENCIES_PREPROCESS_FUNCTION=global_preprocess\n</code></pre> <p>Note</p> <p>For explicit preprocess defined on node dependencies, Flypipe will prefer them over default preprocess function</p>"},{"location":"preprocessing-flow/#chaining-preprocessing-functions","title":"Chaining preprocessing functions","text":"<p>Multiples preprocess functions, i.e. <code>.preprocess(func1, func2...)</code>, can be set.</p> <p>All preprocess functions will be called in the order defined: - <code>.preprocess(func1, func2)</code> will call <code>func1</code>, then the output dataframe from <code>func1</code> will be passed to <code>func2</code>, and so on. - <code>.preprocess(func2, func1)</code> will call <code>func2</code>, then the output dataframe from <code>func2</code> will be passed to <code>func1</code>, and so on.</p> <p>Example:</p> Preprocessing Syntax<pre><code>def filter_date(df):\n    return df.filter(df.value &gt;= '2025-01-02')\n\ndef filter_id(df):\n    return df.filter(df.id &gt;= 3)\n\n@node(...)\ndef nodeA():\n    return df\n\n@node(\n    dependencies=[\n        nodeA.preprocess(filter_date, filter_id).alias(\"df\") # (1)\n    ]\n)\ndef nodeB(df):\n    return df\n\n@node(\n    dependencies=[\n        nodeA.alias(\"df\")\n    ]\n)\ndef nodeC(df):\n    return df    \n</code></pre> <ol> <li>Flypipe will apply <code>filter_date</code> and then <code>filter_id</code> on the output dataframe of <code>nodeA</code> to <code>nodeB</code></li> </ol> <pre><code>---\nconfig:\n  flowchart:\n    htmlLabels: true\n---\nflowchart LR\n  nodeA(nodeA) e1@----&gt; table([\"\n    &lt;table style='border-color:gray; border-style:solid; border-width:1px; font-size:10px;'&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;id&lt;/th&gt;\n                &lt;th&gt;date&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            &lt;tr&gt;\n                &lt;td&gt;1&lt;/td&gt;\n                &lt;td&gt;2025-01-01&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;2&lt;/td&gt;\n                &lt;td&gt;2025-01-02&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;3&lt;/td&gt;\n                &lt;td&gt;2025-01-03&lt;/td&gt;\n            &lt;/tr&gt;      \n        &lt;/tbody&gt;\n    &lt;/table&gt;\n  \"]);\n  e1@{ animate: true };\n\n  table e2@----&gt; preprocessing@{ shape: das, label: \"preprocess&lt;br/&gt;filter_date(df)\" };\n  e2@{ animate: true };\n\n  preprocessing e3@----&gt; preprocessedTable([\"\n     &lt;table style='border-color:gray; border-style:solid; border-width:1px; font-size:10px;'&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;id&lt;/th&gt;\n                &lt;th&gt;date&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            &lt;tr&gt;\n                &lt;td&gt;2&lt;/td&gt;\n                &lt;td&gt;2025-01-02&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;3&lt;/td&gt;\n                &lt;td&gt;2025-01-03&lt;/td&gt;\n            &lt;/tr&gt;\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n  \"]);\n  e3@{ animate: true };\n\n  preprocessedTable e4@----&gt; preprocessing2@{ shape: das, label: \"preprocess&lt;br/&gt;filter_id(df)\" };\n  e4@{ animate: true };\n\n  preprocessing2 e5@----&gt; preprocessedTable2([\"\n     &lt;table style='border-color:gray; border-style:solid; border-width:1px; font-size:10px;'&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;id&lt;/th&gt;\n                &lt;th&gt;date&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            &lt;tr&gt;\n                &lt;td&gt;3&lt;/td&gt;\n                &lt;td&gt;2025-01-03&lt;/td&gt;\n            &lt;/tr&gt;\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n  \"]);\n  e5@{ animate: true };\n\n  preprocessedTable2 e6@----&gt; nodeB(nodeB);\n  e6@{ animate: true };\n\n  table e7@----&gt; nodeC(nodeC);\n  e7@{ animate: true };\n\n  style table fill:None,stroke-width:0px;\n  style preprocessedTable fill:None,stroke-width:0px;\n  style preprocessedTable2 fill:None,stroke-width:0px;</code></pre>"},{"location":"questions-suggestions/","title":"Questions and suggestions","text":"<p>If you have any question, suggestion, facing any problems, bugs, etc. - please use  GitHub Issues.</p>"},{"location":"relationships/","title":"Relationships","text":"<p>You can document relationships between node outputs. These relationships differ from node dependencies, as they describe how the outputs of different nodes are related to each other.</p> <p>Documenting these relationships helps your team better understand how nodes interact, making it easier to work with materialized tables. Additionally, these relationships facilitate the generation of Entity-Relationship Diagrams (ERDs).</p> <p>Advantages of Having ERD Diagrams Available for the Team</p> <ul> <li>Improved Data Understanding: ERDs provide a clear visual representation of how data flows between different nodes, making it easier for team members to grasp the structure of the data.</li> <li>Faster Onboarding: New team members can quickly understand the relationships between different tables and nodes, reducing the learning curve.</li> <li>Better Collaboration: A shared ERD diagram enables data engineers, analysts, and stakeholders to discuss and align on data relationships efficiently.</li> <li>Reduced Errors: Understanding relationships between nodes helps prevent inconsistencies and ensures that transformations and aggregations align with business logic.</li> <li>Optimized Query Performance: By visualizing relationships, teams can identify redundant joins or inefficient queries and optimize database performance accordingly.</li> </ul> <p>To document the relationships between nodes or tables, you can declare them as follows:</p> <pre><code>@node(\n    ...\n    output=Schema(\n        Column(\"col_name\", String(), \"description)\n\n        # declare the relationship with other nodes output columns\n        .one_to_many(another_node.output.col, \"relationship description\")\n    )\ndef my_node(...):\n    ...\n</code></pre> <p>The possible relationships are <code>.one_to_one(...)</code>, <code>.one_to_many(...)</code>, <code>.many_to_one(...)</code> and <code>.many_to_many(...)</code></p> <p>Important</p> <p>Relationships are solely for documentation purposes; they do not enforce any constraints at runtime. Therefore, not documenting relationships has no impact on execution.</p> <p>Here is a more detailed implementation of these relationships and its usage:</p> <p>flypipe.schema.column.Column</p> <p>Defines a column in the output of a flypipe node.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the column.</p> required <code>type</code> <code>Type</code> <p>Data type of the column.</p> required <code>description</code> <code>(str, optional)</code> <p>A description of the column.</p> <code>''</code> <code>pk</code> <code>(bool, optional)</code> <p>Marks the column as primary key or not. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>flypipe/schema/column.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    type: Type,\n    description: str = \"\",\n    pk: bool = False,\n):\n    self.name = name\n    self.type = type\n    if not description and get_config(\"require_schema_description\"):\n        raise ValueError(\n            f\"Descriptions on schema columns configured as mandatory but no description provided for column \"\n            f\"{self.name}\"\n        )\n    self.description = description\n\n    # Each column knows who is the node that it is associated with, it is used to map the Relationship between\n    # node1.output.col1 and node2.output.col2. In this way, col1 knows that belongs to node1\n    # and col2 to node2\n    self.parent = None\n\n    # this is a dict that holds the foreign keys between this column and other nodes columns,\n    # the key is other Column, and the value holds Relationship\n    self.relationships = {}\n\n    self.pk = pk\n</code></pre>"},{"location":"relationships/#flypipe.schema.column.Column.many_to_many","title":"<code>many_to_many(other, description=None)</code>","text":"<p>Adds a N:N relationship between this column and other node output column</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Column</code> <p>Node output column.</p> required <code>description</code> <code>(str, optional)</code> <p>A description of the relationship between this column and other node output column.</p> <code>None</code> <p>Usage:</p> <pre><code>@node(\n    ...\n    output=Schema(\n        ...\n        Column(\"col_name\", String(), \"description)\n        .many_to_many(another_node.output.col, \"relationship description\")\n        ...\n    )\n)\ndef my_node(...):\n    ...\n</code></pre> Source code in <code>flypipe/schema/column.py</code> <pre><code>def many_to_many(self, other: \"Column\", description: str = None):\n    \"\"\"Adds a N:N relationship between this column and other node output column\n\n    Args:\n        other (Column): Node output column.\n        description (str,optional): A description of the relationship between this column and other node output column.\n\n    Usage:\n\n    ``` py\n    @node(\n        ...\n        output=Schema(\n            ...\n            Column(\"col_name\", String(), \"description)\n            .many_to_many(another_node.output.col, \"relationship description\")\n            ...\n        )\n    )\n    def my_node(...):\n        ...\n    ```\n    \"\"\"\n\n    return self._add_relationship(other, RelationshipType.MANY_TO_MANY, description)\n</code></pre>"},{"location":"relationships/#flypipe.schema.column.Column.many_to_one","title":"<code>many_to_one(other, description=None)</code>","text":"<p>Adds a N:1 relationship between this column and other node output column</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Column</code> <p>Node output column.</p> required <code>description</code> <code>(str, optional)</code> <p>A description of the relationship between this column and other node output column.</p> <code>None</code> <p>Usage:</p> <pre><code>@node(\n    ...\n    output=Schema(\n        ...\n        Column(\"col_name\", String(), \"description)\n        .many_to_one(another_node.output.col, \"relationship description\")\n        ...\n    )\n)\ndef my_node(...):\n    ...\n</code></pre> Source code in <code>flypipe/schema/column.py</code> <pre><code>def many_to_one(self, other: \"Column\", description: str = None):\n    \"\"\"Adds a N:1 relationship between this column and other node output column\n\n    Args:\n        other (Column): Node output column.\n        description (str,optional): A description of the relationship between this column and other node output column.\n\n    Usage:\n\n    ``` py\n    @node(\n        ...\n        output=Schema(\n            ...\n            Column(\"col_name\", String(), \"description)\n            .many_to_one(another_node.output.col, \"relationship description\")\n            ...\n        )\n    )\n    def my_node(...):\n        ...\n    ```\n    \"\"\"\n    return self._add_relationship(other, RelationshipType.MANY_TO_ONE, description)\n</code></pre>"},{"location":"relationships/#flypipe.schema.column.Column.one_to_many","title":"<code>one_to_many(other, description=None)</code>","text":"<p>Adds a 1:N relationship between this column and other node output column</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Column</code> <p>Node output column.</p> required <code>description</code> <code>(str, optional)</code> <p>A description of the relationship between this column and other node output column.</p> <code>None</code> <p>Usage:</p> <pre><code>@node(\n    ...\n    output=Schema(\n        ...\n        Column(\"col_name\", String(), \"description)\n        .one_to_many(another_node.output.col, \"relationship description\")\n        ...\n    )\n)\ndef my_node(...):\n    ...\n</code></pre> Source code in <code>flypipe/schema/column.py</code> <pre><code>def one_to_many(self, other: \"Column\", description: str = None):\n    \"\"\"Adds a 1:N relationship between this column and other node output column\n\n    Args:\n        other (Column): Node output column.\n        description (str,optional): A description of the relationship between this column and other node output column.\n\n    Usage:\n\n    ``` py\n    @node(\n        ...\n        output=Schema(\n            ...\n            Column(\"col_name\", String(), \"description)\n            .one_to_many(another_node.output.col, \"relationship description\")\n            ...\n        )\n    )\n    def my_node(...):\n        ...\n    ```\n    \"\"\"\n\n    return self._add_relationship(other, RelationshipType.ONE_TO_MANY, description)\n</code></pre>"},{"location":"relationships/#flypipe.schema.column.Column.one_to_one","title":"<code>one_to_one(other, description=None)</code>","text":"<p>Adds a 1:1 relationship between this column and other node output column</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Column</code> <p>Node output column.</p> required <code>description</code> <code>(str, optional)</code> <p>A description of the relationship between this column and other node output column.</p> <code>None</code> <p>Usage:</p> <pre><code>@node(\n    ...\n    output=Schema(\n        ...\n        Column(\"col_name\", String(), \"description)\n        .one_to_one(another_node.output.col, \"relationship description\")\n        ...\n    )\n)\ndef my_node(...):\n    ...\n</code></pre> Source code in <code>flypipe/schema/column.py</code> <pre><code>def one_to_one(self, other: \"Column\", description: str = None):\n    \"\"\"Adds a 1:1 relationship between this column and other node output column\n\n    Args:\n        other (Column): Node output column.\n        description (str,optional): A description of the relationship between this column and other node output column.\n\n    Usage:\n\n    ``` py\n    @node(\n        ...\n        output=Schema(\n            ...\n            Column(\"col_name\", String(), \"description)\n            .one_to_one(another_node.output.col, \"relationship description\")\n            ...\n        )\n    )\n    def my_node(...):\n        ...\n    ```\n    \"\"\"\n    return self._add_relationship(other, RelationshipType.ONE_TO_ONE, description)\n</code></pre>"},{"location":"notebooks/clean/","title":"Clean","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport pathlib\n</pre> import os import pathlib In\u00a0[\u00a0]: Copied! <pre>import nbformat\n</pre> import nbformat In\u00a0[\u00a0]: Copied! <pre>def clean_notebook(notebook_path):\n    \"\"\"\n    Cleans a single .ipynb file: removes output cells and resets execution counts.\n    \"\"\"\n    try:\n        with open(notebook_path, 'r', encoding='utf-8') as f:\n            nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n\n        changed = False\n        for cell in nb.cells:\n            if cell.cell_type == 'code':\n                if cell.get('outputs'):\n                    cell['outputs'] = []\n                    changed = True\n                if cell.get('execution_count') is not None:\n                    cell['execution_count'] = None\n                    changed = True\n\n        if changed:\n            with open(notebook_path, 'w', encoding='utf-8') as f:\n                nbformat.write(nb, f)\n            print(f\"Cleaned: {notebook_path}\")\n        else:\n            print(f\"Already clean: {notebook_path}\")\n    except Exception as e:\n        print(f\"Error processing {notebook_path}: {e}\")\n</pre> def clean_notebook(notebook_path):     \"\"\"     Cleans a single .ipynb file: removes output cells and resets execution counts.     \"\"\"     try:         with open(notebook_path, 'r', encoding='utf-8') as f:             nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)          changed = False         for cell in nb.cells:             if cell.cell_type == 'code':                 if cell.get('outputs'):                     cell['outputs'] = []                     changed = True                 if cell.get('execution_count') is not None:                     cell['execution_count'] = None                     changed = True          if changed:             with open(notebook_path, 'w', encoding='utf-8') as f:                 nbformat.write(nb, f)             print(f\"Cleaned: {notebook_path}\")         else:             print(f\"Already clean: {notebook_path}\")     except Exception as e:         print(f\"Error processing {notebook_path}: {e}\") In\u00a0[\u00a0]: Copied! <pre>def clean_all_notebooks(root_dir):\n    \"\"\"\n    Recursively finds and cleans all .ipynb files starting from root_dir.\n    \"\"\"\n    for dirpath, _, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if filename.endswith('.ipynb'):\n                notebook_path = os.path.join(dirpath, filename)\n                clean_notebook(notebook_path)\n</pre> def clean_all_notebooks(root_dir):     \"\"\"     Recursively finds and cleans all .ipynb files starting from root_dir.     \"\"\"     for dirpath, _, filenames in os.walk(root_dir):         for filename in filenames:             if filename.endswith('.ipynb'):                 notebook_path = os.path.join(dirpath, filename)                 clean_notebook(notebook_path) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    clean_all_notebooks(os.path.join(pathlib.Path(__file__).resolve().parent))\n</pre> if __name__ == \"__main__\":     clean_all_notebooks(os.path.join(pathlib.Path(__file__).resolve().parent))"},{"location":"notebooks/concept/dag/","title":"Dag","text":"In\u00a0[\u00a0]: Copied! <pre>from flypipe import node\nfrom flypipe.datasource.spark import Spark\n\n@node(\n    type=\"pyspark\",\n    dependencies=[Spark(\"bronze\")],\n    group=\"data_pipeline\"\n)\ndef silver(bronze):\n    return bronze\n\n\n@node(\n    type=\"pyspark\",\n    dependencies=[silver],\n    group=\"data_pipeline\"\n)\ndef gold(silver):\n    return silver\n\n@node(\n    type=\"pandas_on_spark\",\n    group=\"feature_pipeline\",\n    dependencies=[gold]\n)\ndef feature1(gold):\n    return gold\n\n@node(\n    type=\"pandas_on_spark\",\n    group=\"feature_pipeline\",\n    dependencies=[gold]\n)\ndef feature2(gold):\n    return gold\n\n\n@node(\n    type=\"pandas_on_spark\",\n    group=\"feature_pipeline\",\n    dependencies=[gold]\n)\ndef feature3(gold):\n    return gold\n\n\n@node(\n    type=\"pandas_on_spark\",\n    dependencies=[\n        feature1, feature2, feature3\n    ]\n)\ndef model_features(feature1, feature2, feature3):\n    return feature1\n\n\n@node(\n    type=\"pandas_on_spark\",\n    group=\"model_training\",      \n    dependencies=[model_features]\n)\ndef split(model_features):\n    return model_features\n\n\n@node(\n    type=\"pandas_on_spark\",\n    group=\"model_training\",\n    dependencies=[split]\n)\ndef train_scale(split):\n    return split\n\n@node(\n    type=\"pandas\",\n    group=\"model_training\",\n    dependencies=[train_scale]\n)\ndef model_train(train_scale):\n    return train_scale\n\n\n@node(\n    type=\"pandas\",\n    group=\"model_training\",\n    dependencies=[model_train]\n)\ndef evaluate(model_train):\n    return model_train\n\n@node(\n    type=\"pandas\",\n    group=\"prediction\",\n    dependencies=[model_features]\n)\ndef scale(model_features):\n    return model_features\n\n\n@node(\n    type=\"pandas\",\n    group=\"prediction\",\n    dependencies=[scale]\n)\ndef predict(scale):\n    return scale\n\n\n@node(\n    type=\"pandas\",\n    dependencies=[predict, evaluate]\n)\ndef my_graph(predict, evaluate):\n    return predict\n\ndisplayHTML(my_graph.html())\n</pre> from flypipe import node from flypipe.datasource.spark import Spark  @node(     type=\"pyspark\",     dependencies=[Spark(\"bronze\")],     group=\"data_pipeline\" ) def silver(bronze):     return bronze   @node(     type=\"pyspark\",     dependencies=[silver],     group=\"data_pipeline\" ) def gold(silver):     return silver  @node(     type=\"pandas_on_spark\",     group=\"feature_pipeline\",     dependencies=[gold] ) def feature1(gold):     return gold  @node(     type=\"pandas_on_spark\",     group=\"feature_pipeline\",     dependencies=[gold] ) def feature2(gold):     return gold   @node(     type=\"pandas_on_spark\",     group=\"feature_pipeline\",     dependencies=[gold] ) def feature3(gold):     return gold   @node(     type=\"pandas_on_spark\",     dependencies=[         feature1, feature2, feature3     ] ) def model_features(feature1, feature2, feature3):     return feature1   @node(     type=\"pandas_on_spark\",     group=\"model_training\",           dependencies=[model_features] ) def split(model_features):     return model_features   @node(     type=\"pandas_on_spark\",     group=\"model_training\",     dependencies=[split] ) def train_scale(split):     return split  @node(     type=\"pandas\",     group=\"model_training\",     dependencies=[train_scale] ) def model_train(train_scale):     return train_scale   @node(     type=\"pandas\",     group=\"model_training\",     dependencies=[model_train] ) def evaluate(model_train):     return model_train  @node(     type=\"pandas\",     group=\"prediction\",     dependencies=[model_features] ) def scale(model_features):     return model_features   @node(     type=\"pandas\",     group=\"prediction\",     dependencies=[scale] ) def predict(scale):     return scale   @node(     type=\"pandas\",     dependencies=[predict, evaluate] ) def my_graph(predict, evaluate):     return predict  displayHTML(my_graph.html())"},{"location":"notebooks/ddp/parallel/","title":"Parallel","text":"In\u00a0[\u00a0]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, Integer\n\nimport pandas as pd\nfrom sklearn import datasets\n\n@node(type=\"pandas\")\ndef node_0():\n    return pd.DataFrame(data = {'col1': ['val1']})\n\n\n@node(type=\"pandas\",\n     dependencies=[node_0])\ndef node_1(node_0):\n    return node_0\n    \n@node(type=\"pandas\",\n     dependencies=[node_0])\ndef node_2(node_0):\n    return node_0\n\n@node(type=\"pandas\",\n     dependencies=[node_0])\ndef node_3(node_0):\n    return node_0\n\n\n@node(type=\"pandas\",\n     dependencies=[\n         node_1,\n         node_2,\n         node_3\n     ])\ndef node_4(node_1, node_2, node_3):\n    return node_1\n    \ndisplayHTML(node_4.html())\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, Integer  import pandas as pd from sklearn import datasets  @node(type=\"pandas\") def node_0():     return pd.DataFrame(data = {'col1': ['val1']})   @node(type=\"pandas\",      dependencies=[node_0]) def node_1(node_0):     return node_0      @node(type=\"pandas\",      dependencies=[node_0]) def node_2(node_0):     return node_0  @node(type=\"pandas\",      dependencies=[node_0]) def node_3(node_0):     return node_0   @node(type=\"pandas\",      dependencies=[          node_1,          node_2,          node_3      ]) def node_4(node_1, node_2, node_3):     return node_1      displayHTML(node_4.html())"},{"location":"notebooks/ddp/parallel/#parallel","title":"Parallel\u00b6","text":""},{"location":"notebooks/ddp/serial/","title":"Serial","text":"In\u00a0[\u00a0]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, Integer\n\nimport pandas as pd\nfrom sklearn import datasets\n\n\n@node(type=\"pandas\")\ndef node_1():\n    return pd.DataFrame(data = {'col1': ['val1']})\n    \n\n@node(type=\"pandas\",\n     dependencies=[node_1])\ndef node_2(node_1):\n    node_1['col2'] = 'val2'\n    return node_1\n\n\n@node(type=\"pandas\",\n     dependencies=[node_2])\ndef node_3(node_2):\n    node_2['col3'] = 'val3'\n    return node_2\n    \ndisplayHTML(node_3.html())\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, Integer  import pandas as pd from sklearn import datasets   @node(type=\"pandas\") def node_1():     return pd.DataFrame(data = {'col1': ['val1']})       @node(type=\"pandas\",      dependencies=[node_1]) def node_2(node_1):     node_1['col2'] = 'val2'     return node_1   @node(type=\"pandas\",      dependencies=[node_2]) def node_3(node_2):     node_2['col3'] = 'val3'     return node_2      displayHTML(node_3.html())"},{"location":"notebooks/ddp/serial/#serial","title":"Serial\u00b6","text":""},{"location":"notebooks/how-to/ml-model-with-imports/","title":"Ml model with imports","text":"In\u00a0[\u00a0]: Copied! <pre>from flypipe.examples.pipeline.model.demo.graph import graph\ndisplayHTML(graph.html())\n</pre> from flypipe.examples.pipeline.model.demo.graph import graph displayHTML(graph.html()) In\u00a0[\u00a0]: Copied! <pre>import os\nimport mlflow\nfrom flypipe.examples.pipeline.model.demo.train.evaluate import evaluate\nfrom flypipe.examples.pipeline.model.demo.config import _config\n\nARTIFACT_LOCATION = _config[\"artifact_location\"]\nEXPERIMENT_PATH = _config[\"experiment_path\"]\nEXPERIMENT_NAME = _config[\"experiment_name\"]\n\n\nexperiment_name = os.path.join(EXPERIMENT_PATH, EXPERIMENT_NAME)\n# Ends any actve mlflow run\ntry:\n    mlflow.end_run()\nexcept Exception as e:\n    pass\n\n\"\"\"\nCreates or gets an experiment from /Shared folder\nSets the artifact location to the mounted blob\n\"\"\"\nos.makedirs(ARTIFACT_LOCATION, exist_ok=True)\n\ntry:\n    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME,\n                                             artifact_location=ARTIFACT_LOCATION.replace(\"/dbfs/\", \"dbfs:/\"))\nexcept Exception as e:\n    pass\nfinally:\n    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n    experiment_id = experiment.experiment_id\n\nexperiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\nprint(f\"Experiment: {experiment}\")\n\n\"\"\"\nStarts the mlflow run with the experiment\n\"\"\"\nmlflow.start_run(experiment_id=experiment_id)\nRUN_ID = mlflow.active_run().info.run_id\nprint(f\"Active run_id: {RUN_ID}\")\n\n\"\"\"\nRun train/evaluate graph\n\"\"\"\ndf = evaluate.run()\ndisplay(df)\n\nmlflow.end_run()\n</pre> import os import mlflow from flypipe.examples.pipeline.model.demo.train.evaluate import evaluate from flypipe.examples.pipeline.model.demo.config import _config  ARTIFACT_LOCATION = _config[\"artifact_location\"] EXPERIMENT_PATH = _config[\"experiment_path\"] EXPERIMENT_NAME = _config[\"experiment_name\"]   experiment_name = os.path.join(EXPERIMENT_PATH, EXPERIMENT_NAME) # Ends any actve mlflow run try:     mlflow.end_run() except Exception as e:     pass  \"\"\" Creates or gets an experiment from /Shared folder Sets the artifact location to the mounted blob \"\"\" os.makedirs(ARTIFACT_LOCATION, exist_ok=True)  try:     experiment_id = mlflow.create_experiment(EXPERIMENT_NAME,                                              artifact_location=ARTIFACT_LOCATION.replace(\"/dbfs/\", \"dbfs:/\")) except Exception as e:     pass finally:     experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)     experiment_id = experiment.experiment_id  experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME) print(f\"Experiment: {experiment}\")  \"\"\" Starts the mlflow run with the experiment \"\"\" mlflow.start_run(experiment_id=experiment_id) RUN_ID = mlflow.active_run().info.run_id print(f\"Active run_id: {RUN_ID}\")  \"\"\" Run train/evaluate graph \"\"\" df = evaluate.run() display(df)  mlflow.end_run() In\u00a0[\u00a0]: Copied! <pre>from flypipe.examples.pipeline.model.demo.config import config\nfrom flypipe.examples.pipeline.model.demo.predict.predict import predict\nimport pandas as pd\n\nconfig_df = pd.DataFrame(data=[{\n    \"artifact_location\": ARTIFACT_LOCATION,\n    \"production_run_id\":RUN_ID,\n}])\n\ndf = predict.run(inputs={\n    config: config_df\n})\n\ndisplay(df)\n</pre> from flypipe.examples.pipeline.model.demo.config import config from flypipe.examples.pipeline.model.demo.predict.predict import predict import pandas as pd  config_df = pd.DataFrame(data=[{     \"artifact_location\": ARTIFACT_LOCATION,     \"production_run_id\":RUN_ID, }])  df = predict.run(inputs={     config: config_df })  display(df) In\u00a0[\u00a0]: Copied! <pre>from flypipe.examples.pipeline.model.demo.config import config\nfrom flypipe.examples.pipeline.model.demo.predict.predict import predict\nfrom flypipe.examples.pipeline.model.demo.data import data\nimport pandas as pd\n\nconfig_df = pd.DataFrame(data=[{\n    \"artifact_location\": ARTIFACT_LOCATION,\n    \"production_run_id\":RUN_ID,\n}])\n\n# Hard coded data to be predicted\ndf = pd.DataFrame(data = {\n        'sepal_length': [6.6],\n        'sepal_width': [3.1],\n        'petal_length': [5.1],\n        'petal_width': [2.4]\n    })\n\n# Run the predictions\npredictions = (\n    predict\n    .run(inputs={\n        config: config_df,\n        data: df\n    })\n)\n\n# Show predictions\ndisplay(predictions)\n</pre> from flypipe.examples.pipeline.model.demo.config import config from flypipe.examples.pipeline.model.demo.predict.predict import predict from flypipe.examples.pipeline.model.demo.data import data import pandas as pd  config_df = pd.DataFrame(data=[{     \"artifact_location\": ARTIFACT_LOCATION,     \"production_run_id\":RUN_ID, }])  # Hard coded data to be predicted df = pd.DataFrame(data = {         'sepal_length': [6.6],         'sepal_width': [3.1],         'petal_length': [5.1],         'petal_width': [2.4]     })  # Run the predictions predictions = (     predict     .run(inputs={         config: config_df,         data: df     }) )  # Show predictions display(predictions)"},{"location":"notebooks/how-to/ml-model-with-imports/#ml-model-demo","title":"ML model <code>demo</code>\u00b6","text":"<p>Predicts the type of iris plant.</p>"},{"location":"notebooks/how-to/ml-model-with-imports/#graph","title":"Graph\u00b6","text":""},{"location":"notebooks/how-to/ml-model-with-imports/#how-to-train","title":"How to train?\u00b6","text":""},{"location":"notebooks/how-to/ml-model-with-imports/#how-to-run-predictions","title":"How to run predictions?\u00b6","text":"<p>Note: as <code>config</code> node contains information of the run used for production, the code bellow can be simplified as</p> <pre>from flypipe.examples.pipeline.model.demo.predict.predict import predict\ndf = predict.run()\ndisplay(df)\n</pre>"},{"location":"notebooks/how-to/ml-model-with-imports/#predict-using-your-own-inputs","title":"Predict using your own inputs?\u00b6","text":""},{"location":"notebooks/how-to/spark-streaming/","title":"Spark Streaming","text":"In\u00a0[\u00a0]: Copied! <pre>import shutil\n\nspark.sql(\nf\"\"\"\n    CREATE DATABASE IF NOT EXISTS flypipe\n    LOCATION '/data/warehouse/flypipe'\n\"\"\"\n)\n\nspark.sql(\"drop table if exists flypipe.total_sales\")\nshutil.rmtree(\"/spark-warehouse/flypipe/total_sales\", ignore_errors=True)\nshutil.rmtree(\"/data/tmp/stream/_checkpoints\", ignore_errors=True)\nshutil.rmtree(\"/data/tmp/stream/json\", ignore_errors=True)\n</pre> import shutil  spark.sql( f\"\"\"     CREATE DATABASE IF NOT EXISTS flypipe     LOCATION '/data/warehouse/flypipe' \"\"\" )  spark.sql(\"drop table if exists flypipe.total_sales\") shutil.rmtree(\"/spark-warehouse/flypipe/total_sales\", ignore_errors=True) shutil.rmtree(\"/data/tmp/stream/_checkpoints\", ignore_errors=True) shutil.rmtree(\"/data/tmp/stream/json\", ignore_errors=True) In\u00a0[\u00a0]: Copied! <pre>import json\nfrom time import time\nimport os\nimport uuid\nimport random\nfrom pprint import pprint\n\nJSON_LOCATION = \"/data/tmp/stream/json\"\n\nos.makedirs(JSON_LOCATION, exist_ok=True)\n\ndef add_sale(quantity):\n    \n    for _ in range(quantity):\n        sale_id = str(uuid.uuid4())\n        out_file = open(f\"{JSON_LOCATION}/{sale_id}.json\", \"w\")\n        \n        data = {\n            'sale_id': sale_id,\n            'product_id': random.randrange(1, 5, 1),\n            'price': random.randrange(100, 1000, 1),\n            'quantity': random.randrange(1, 10, 1),\n            'sale_datetime': int(time())\n        }\n        \n\n        json.dump(data, out_file)\n        \n        print(f\"\\nAdded {JSON_LOCATION}/{sale_id}.json\")\n        pprint(data)\n             \n        \nadd_sale(5)\n</pre> import json from time import time import os import uuid import random from pprint import pprint  JSON_LOCATION = \"/data/tmp/stream/json\"  os.makedirs(JSON_LOCATION, exist_ok=True)  def add_sale(quantity):          for _ in range(quantity):         sale_id = str(uuid.uuid4())         out_file = open(f\"{JSON_LOCATION}/{sale_id}.json\", \"w\")                  data = {             'sale_id': sale_id,             'product_id': random.randrange(1, 5, 1),             'price': random.randrange(100, 1000, 1),             'quantity': random.randrange(1, 10, 1),             'sale_datetime': int(time())         }                   json.dump(data, out_file)                  print(f\"\\nAdded {JSON_LOCATION}/{sale_id}.json\")         pprint(data)                        add_sale(5) In\u00a0[\u00a0]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Decimal, String\nfrom flypipe.datasource.spark import Spark\nimport pyspark.sql.functions as F\n\n@node(\n    type=\"pyspark\",\n    dependencies=[\n        Spark(\"sales\")\n    ],\n    output=Schema(\n     Column(\"product_id\", String(), \"product identifier\"),   \n     Column(\"total_sales\", Decimal(18,2), \"total sales amount\"),\n    )\n)\ndef total_sales_node(sales):\n    df = sales.groupBy(\"product_id\").agg(F.sum(F.col(\"price\") * F.col(\"quantity\")).alias(\"total_sales\"))\n    return df\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Decimal, String from flypipe.datasource.spark import Spark import pyspark.sql.functions as F  @node(     type=\"pyspark\",     dependencies=[         Spark(\"sales\")     ],     output=Schema(      Column(\"product_id\", String(), \"product identifier\"),         Column(\"total_sales\", Decimal(18,2), \"total sales amount\"),     ) ) def total_sales_node(sales):     df = sales.groupBy(\"product_id\").agg(F.sum(F.col(\"price\") * F.col(\"quantity\")).alias(\"total_sales\"))     return df In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql.types import StructType, ArrayType, StructField, StringType, DecimalType, IntegerType, TimestampType\n\n\ndef total_sales(batch_df, batch_id):\n    \n    print(\"Batch dataframe received:\")\n    display(batch_df)\n    \n    total_sales_df = (\n        total_sales_node\n        .run(inputs = {\n            Spark(\"sales\"): batch_df\n        })\n    )\n    \n    print(\"===&gt; Saving dataframe calculated with node `total_sales_node` into table `total_sales`\")\n    \n    (\n      total_sales_df\n      .write\n      .format('delta')\n      .mode('overwrite')\n      .saveAsTable(\"flypipe.total_sales\")\n    )\n    \n    return total_sales_df\n</pre> from pyspark.sql.types import StructType, ArrayType, StructField, StringType, DecimalType, IntegerType, TimestampType   def total_sales(batch_df, batch_id):          print(\"Batch dataframe received:\")     display(batch_df)          total_sales_df = (         total_sales_node         .run(inputs = {             Spark(\"sales\"): batch_df         })     )          print(\"===&gt; Saving dataframe calculated with node `total_sales_node` into table `total_sales`\")          (       total_sales_df       .write       .format('delta')       .mode('overwrite')       .saveAsTable(\"flypipe.total_sales\")     )          return total_sales_df In\u00a0[\u00a0]: Copied! <pre># Create Stream\njson_schema = StructType([\n    StructField(\"sale_id\", StringType(), True),\n    StructField(\"product_id\", StringType(), True),\n    StructField(\"price\", DecimalType(18,2), True),\n    StructField(\"quantity\", IntegerType(), True),\n    StructField(\"sale_datetime\", TimestampType(), True),\n])\n\n\n(\n  spark\n  .readStream\n  .json(JSON_LOCATION, schema=json_schema)\n  .writeStream\n  .trigger(availableNow=True) # &lt;-- Change the trigger as you wish\n  .option(\"checkpointLocation\", \"/data/tmp/stream/_checkpoints/\")\n  .foreachBatch(total_sales)\n  .start()\n)\n\n# Waitting process\nfrom time import sleep\n\nwhile True:\n    try:\n        spark.sql(\"select * from flypipe.total_sales\")\n        break\n    except Exception as e:\n        sleep(2)\n</pre> # Create Stream json_schema = StructType([     StructField(\"sale_id\", StringType(), True),     StructField(\"product_id\", StringType(), True),     StructField(\"price\", DecimalType(18,2), True),     StructField(\"quantity\", IntegerType(), True),     StructField(\"sale_datetime\", TimestampType(), True), ])   (   spark   .readStream   .json(JSON_LOCATION, schema=json_schema)   .writeStream   .trigger(availableNow=True) # &lt;-- Change the trigger as you wish   .option(\"checkpointLocation\", \"/data/tmp/stream/_checkpoints/\")   .foreachBatch(total_sales)   .start() )  # Waitting process from time import sleep  while True:     try:         spark.sql(\"select * from flypipe.total_sales\")         break     except Exception as e:         sleep(2) In\u00a0[\u00a0]: Copied! <pre>display(spark.sql(\"select * from flypipe.total_sales\"))\n</pre> display(spark.sql(\"select * from flypipe.total_sales\"))"},{"location":"notebooks/how-to/spark-streaming/#spark-streaming","title":"Spark Streaming\u00b6","text":"<p>You integrate Flypipe graphs to Spark streaming.</p> <p>One way of doing it is using Spark foreachBatch. Here we create a function <code>total_sales(batch_df, batch_id)</code> that receives the bacth dataframe and calls a Flypipe node with provided inputs.</p>"},{"location":"notebooks/how-to/spark-streaming/#cleaning-environment","title":"Cleaning environment\u00b6","text":""},{"location":"notebooks/how-to/spark-streaming/#adding-json-files-simulating-a-source","title":"Adding json files simulating a source\u00b6","text":""},{"location":"notebooks/how-to/spark-streaming/#flypipe-graph-to-process-the-data","title":"Flypipe graph to process the data\u00b6","text":""},{"location":"notebooks/how-to/spark-streaming/#defines-a-bacth-function-that-wraps-the-flypipe-graph","title":"Defines a bacth function that wraps the Flypipe graph\u00b6","text":""},{"location":"notebooks/how-to/spark-streaming/#sets-up-and-start-the-streaming","title":"Sets up and start the streaming\u00b6","text":""},{"location":"notebooks/how-to/spark-streaming/#display-results","title":"Display results\u00b6","text":""},{"location":"notebooks/miscellaneous/build-erd/","title":"Building ERD","text":"In\u00a0[1]: Copied! <pre>from flypipe.misc.dbml import build_dbml\n\nhelp(build_dbml)\n</pre> from flypipe.misc.dbml import build_dbml  help(build_dbml) <pre>Help on function build_dbml in module flypipe.misc.dbml:\n\nbuild_dbml(nodes: Union[List[&lt;property object at 0xf99150038680&gt;.Node], &lt;property object at 0xf99150038680&gt;.Node], only_nodes_with_tags: Union[List[str], str] = None, only_nodes_with_cache: bool = False, file_path_name=None)\n    Reads flypipe nodes and builds a dbml source code with the relationships, if output columns have been defined.\n    \n    Parameters\n    ----------\n    \n    nodes : Union[List[Node], Node]\n        nodes to be considered to be added to the dbml\n    only_nodes_with_tags : Union[List[str], str], optional, defafult None\n        include only nodes with tags\n    only_nodes_with_cache: bool, optional, default True\n        True: include nodes with cache, False: do not include nodes.\n    \n        Note: the cache objects must have name method implemented, example:\n    \n            class MyCache(Cache):\n                ...\n                @property\n                def name(self):\n                    return &lt;NAME TO BE USED FOR THE DBML TABLE NAME&gt;\n    \n    file_path_name: str, optional, default None\n        full file path name where the dbml table is stored.\n    \n    Returns\n    -------\n    \n    dbml string if file_path_name is None else it will return None\n\n</pre> <p>example:</p> In\u00a0[2]: Copied! <pre>from flypipe import node\nfrom flypipe.cache import Cache\nfrom flypipe.misc.dbml import build_dbml\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nfrom pprint import pprint\n\nclass MyCache(Cache):\n    def __init__(self, cache_name):\n        self.cache_name = cache_name\n\n    @property\n    def color(self): # &lt;-- ADDS THIS COLOR TO THE DBML TABLE\n        return \"#3498DB\"\n\n    @property\n    def name(self): # &lt;-- ADDS THIS NAME TO THE DBML TABLE\n        return self.cache_name\n\n    def read(self, spark):\n        pass\n\n    def write(self, spark, df):\n        pass\n\n    def exists(self, spark=None):\n        pass\n                \n@node(\n    type=\"pandas\",\n    description=\"this is node A\",\n    cache=MyCache(\"cache table a\"),\n    output=Schema(\n        Column(\"node_a_col1\", String(), \"description node_a_col1\"),\n        Column(\"node_a_col2\", String(), \"description node_a_col2\")\n    )\n)\ndef A():\n    pass\n\n@node(\n    type=\"pandas\",\n    description=\"this is node B\",\n    output=Schema(\n        Column(\"node_b_col1\", String(), \"description node_b_col1\"),\n        Column(\"node_b_col2\", String(), \"description node_b_col2\"),\n        Column(\"node_b_col3\", String(), \"description node_b_col3\")\n    )\n)\ndef B(**dfs):\n    pass\n\ndbml = build_dbml(B)\nprint(f\"{dbml}\")\n</pre> from flypipe import node from flypipe.cache import Cache from flypipe.misc.dbml import build_dbml from flypipe.schema import Schema, Column from flypipe.schema.types import String from pprint import pprint  class MyCache(Cache):     def __init__(self, cache_name):         self.cache_name = cache_name      @property     def color(self): # &lt;-- ADDS THIS COLOR TO THE DBML TABLE         return \"#3498DB\"      @property     def name(self): # &lt;-- ADDS THIS NAME TO THE DBML TABLE         return self.cache_name      def read(self, spark):         pass      def write(self, spark, df):         pass      def exists(self, spark=None):         pass                  @node(     type=\"pandas\",     description=\"this is node A\",     cache=MyCache(\"cache table a\"),     output=Schema(         Column(\"node_a_col1\", String(), \"description node_a_col1\"),         Column(\"node_a_col2\", String(), \"description node_a_col2\")     ) ) def A():     pass  @node(     type=\"pandas\",     description=\"this is node B\",     output=Schema(         Column(\"node_b_col1\", String(), \"description node_b_col1\"),         Column(\"node_b_col2\", String(), \"description node_b_col2\"),         Column(\"node_b_col3\", String(), \"description node_b_col3\")     ) ) def B(**dfs):     pass  dbml = build_dbml(B) print(f\"{dbml}\") <pre>Table b {\n\tnode_b_col1 String() [note: '''description node_b_col1''']\n\tnode_b_col2 String() [note: '''description node_b_col2''']\n\tnode_b_col3 String() [note: '''description node_b_col3''']\n\n\tNote: '''Managed by flypipe node `B`\n\nthis is node B'''\n}\n</pre> In\u00a0[3]: Copied! <pre># Example of relationships.py:\nA.output.node_a_col1.pk = True\nB.output.node_b_col1.pk = True\nB.output.node_b_col2.pk = True\n\nB.output.node_b_col1.many_to_one(A.output.node_a_col1)\n</pre> # Example of relationships.py: A.output.node_a_col1.pk = True B.output.node_b_col1.pk = True B.output.node_b_col2.pk = True  B.output.node_b_col1.many_to_one(A.output.node_a_col1) Out[3]: <pre>\n    Column: node_b_col1\n    Parent: B\n    Data Type: String()\n    Description: 'description node_b_col1\n\tForeign Keys:\n\t\tB.node_b_col1 N:1 A.node_a_col1'\n    PK: True\n        </pre> In\u00a0[4]: Copied! <pre>from flypipe.misc.erd import build_erd_svg\nhelp(build_erd_svg)\n\nbuild_erd_svg(B, \"test.svg\")\n</pre> from flypipe.misc.erd import build_erd_svg help(build_erd_svg)  build_erd_svg(B, \"test.svg\") <pre>Help on function build_erd_svg in module flypipe.misc.erd:\n\nbuild_erd_svg(nodes: Union[List[&lt;property object at 0xf99150038680&gt;.Node], &lt;property object at 0xf99150038680&gt;.Node], output_path, only_nodes_with_tags: Union[List[str], str] = None, only_nodes_with_cache: bool = False)\n    Reads flypipe nodes and builds a SVG image with the ERD diagram\n    \n    Parameters\n    ----------\n    \n    nodes : Union[List[Node], Node]\n        nodes to be considered to be added to the dbml\n    output_path: str\n        full file path name where the svg is stored.\n    only_nodes_with_tags : Union[List[str], str], optional, defafult None\n        include only nodes with tags\n    only_nodes_with_cache: bool, optional, default True\n        True: include nodes with cache, False: do not include nodes.\n    \n        Note: the cache objects must have name method implemented, example:\n    \n            class MyCache(Cache):\n                ...\n                @property\n                def name(self):\n                    return &lt;NAME TO BE USED FOR THE DBML TABLE NAME&gt;\n\n</pre> In\u00a0[5]: Copied! <pre>from IPython.display import SVG, display\n\ndisplay(SVG(filename=\"test.svg\"))  # Replace with your SVG file path\n</pre> from IPython.display import SVG, display  display(SVG(filename=\"test.svg\"))  # Replace with your SVG file path  <p>following is an example of the dbdiagram</p> In\u00a0[6]: Copied! <pre>from IPython.display import IFrame\n\nIFrame(\"https://dbdiagram.io/e/67c9e4b0263d6cf9a07dd726/67c9e4f6263d6cf9a07dde8e\", width=800, height=600)\n</pre> from IPython.display import IFrame  IFrame(\"https://dbdiagram.io/e/67c9e4b0263d6cf9a07dd726/67c9e4f6263d6cf9a07dde8e\", width=800, height=600) Out[6]: In\u00a0[7]: Copied! <pre>from IPython.display import IFrame\n\nIFrame(\"https://dbdocs.io/joseheliomuller/flypipe\", width=1280, height=1000)\n</pre> from IPython.display import IFrame  IFrame(\"https://dbdocs.io/joseheliomuller/flypipe\", width=1280, height=1000) Out[7]:"},{"location":"notebooks/miscellaneous/build-erd/#building-erd","title":"Building ERD\u00b6","text":"<p>Here is described how to build a ERD diagram using dbml-renderer, dbdiagram.io and dbdocs.io.</p>"},{"location":"notebooks/miscellaneous/build-erd/#building-dbml","title":"Building DBML\u00b6","text":"<p>You can build DBML source code using <code>build_dbml</code> from <code>flypipe.misc.dbml</code></p>"},{"location":"notebooks/miscellaneous/build-erd/#build-the-relationships","title":"Build the Relationships\u00b6","text":"<p>You can define the relationships in the column definition:</p> <pre>@node(\n    ...\n    output=Schema(\n        Column(\"node_b_col1\", String(), \"description node_b_col1\", pk=True)\n        .many_to_one(A.output.node_a_col1), # &lt;-- relationship\n        ...\n    )\n)\ndef B(**dfs):\n    ...\n</pre> <p>Important</p> <p>         You can easily cause python errors ``circular imports`` when importing nodes to define the relationships.         In order to avoid circular dependency, it is recommented to keep a separated py file to define the relationships for your graph.     </p>"},{"location":"notebooks/miscellaneous/build-erd/#here-is-how-you-can-avoid-circular-dependecies","title":"Here is how you can avoid circular dependecies:\u00b6","text":""},{"location":"notebooks/miscellaneous/build-erd/#build-erd-diagram-svg-using-dbml-renderer","title":"Build ERD Diagram (SVG) using dbml-renderer\u00b6","text":""},{"location":"notebooks/miscellaneous/build-erd/#dbdiagramio","title":"dbdiagram.io\u00b6","text":"<p>Import the DBML file into dbdiagram</p> <pre>Table b {\n\tnode_b_col1 String() [note: '''description node_b_col1''', PK]\n\tnode_b_col2 String() [note: '''description node_b_col2''', PK]\n\tnode_b_col3 String() [note: '''description node_b_col3''']\n\n\tNote: '''Managed by flypipe node `B`\n\nthis is node B'''\n}\n\nRef : cache_table_a.node_a_col1 &gt; b.node_b_col1\n\nTable cache_table_a [headercolor: #3498DB] {\n\tnode_a_col1 String() [note: '''description node_a_col1''', PK]\n\tnode_a_col2 String() [note: '''description node_a_col2''']\n\n\tNote: '''Managed by flypipe node `A`\n\nthis is node A'''\n}\n</pre>"},{"location":"notebooks/miscellaneous/build-erd/#dbdocsio","title":"dbdocs.io\u00b6","text":"<ol> <li>Follow dbdocs the installation steps</li> <li>Login</li> <li>Publish your DBML file</li> </ol> <p>You can acess this example.</p>"},{"location":"notebooks/miscellaneous/databricks/ml-graphs/","title":"ML Graphs (with mlflow)","text":"In\u00a0[2]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, Integer\n\nimport pandas as pd\nfrom sklearn import datasets\n\n\n@node(\n    type=\"pandas\",\n    description=\"Load Iris dataset\",\n    tags=[\"data\"],\n    output=Schema(\n      Column('sepal_length', Float(), 'sepal length'),  \n      Column('sepal_width', Float(), 'sepal width'),  \n      Column('petal_length', Float(), 'petal length'),  \n      Column('petal_width', Float(), 'petal width'),  \n      Column('target', Integer(), '0: Setosa, 1: Versicolour, and 2: Virginica'),\n    ))\ndef data():\n    iris = datasets.load_iris()\n    df = pd.DataFrame(data = {\n        'sepal_length': iris.data[:,0],\n        'sepal_width': iris.data[:,1],\n        'petal_length': iris.data[:,2],\n        'petal_width': iris.data[:,3],\n        'target': iris.target\n    })\n    \n    return df\n    \ndata.run()\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, Integer  import pandas as pd from sklearn import datasets   @node(     type=\"pandas\",     description=\"Load Iris dataset\",     tags=[\"data\"],     output=Schema(       Column('sepal_length', Float(), 'sepal length'),         Column('sepal_width', Float(), 'sepal width'),         Column('petal_length', Float(), 'petal length'),         Column('petal_width', Float(), 'petal width'),         Column('target', Integer(), '0: Setosa, 1: Versicolour, and 2: Virginica'),     )) def data():     iris = datasets.load_iris()     df = pd.DataFrame(data = {         'sepal_length': iris.data[:,0],         'sepal_width': iris.data[:,1],         'petal_length': iris.data[:,2],         'petal_width': iris.data[:,3],         'target': iris.target     })          return df      data.run() Out[2]: sepal_length sepal_width petal_length petal_width target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 2 146 6.3 2.5 5.0 1.9 2 147 6.5 3.0 5.2 2.0 2 148 6.2 3.4 5.4 2.3 2 149 5.9 3.0 5.1 1.8 2 <p>150 rows \u00d7 5 columns</p> In\u00a0[3]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, String\nfrom sklearn.model_selection import train_test_split\n\n@node(\n    type=\"pandas\",\n    description=\"Split train (70%) and test (30%) data\",\n    tags=[\"data\", \"split\"],\n    dependencies=[\n        data.select(\n            'sepal_length',\n            'sepal_width',\n            'petal_length',\n            'petal_width',\n            'target',\n        )\n    ],\n    output=Schema(\n      Column('data_type', String(), 'train (70%), test (30%)'),  \n      data.output.get(\"sepal_length\"),\n      data.output.get(\"sepal_width\"),\n      data.output.get(\"petal_length\"),\n      data.output.get(\"petal_width\"),\n      data.output.get(\"target\"),\n      \n    ))\ndef split(data):\n    data['data_type'] = \"train\"\n    \n    X_cols = [\n        'sepal_length',\n        'sepal_width',\n        'petal_length',\n        'petal_width'\n    ]\n    y_col = 'target'\n    \n    X_train, X_test, y_train, y_test = train_test_split(data[X_cols], \n                                                        data[y_col], \n                                                        test_size=0.3, \n                                                        random_state=1)\n\n    X_train['data_type'] = 'train'\n    X_train['target'] = y_train\n    \n    X_test['data_type'] = 'test'\n    X_test['target'] = y_test\n    \n    data = pd.concat([X_train, X_test])\n    return data\n    \ndf = split.run()\ndisplay(df)\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, String from sklearn.model_selection import train_test_split  @node(     type=\"pandas\",     description=\"Split train (70%) and test (30%) data\",     tags=[\"data\", \"split\"],     dependencies=[         data.select(             'sepal_length',             'sepal_width',             'petal_length',             'petal_width',             'target',         )     ],     output=Schema(       Column('data_type', String(), 'train (70%), test (30%)'),         data.output.get(\"sepal_length\"),       data.output.get(\"sepal_width\"),       data.output.get(\"petal_length\"),       data.output.get(\"petal_width\"),       data.output.get(\"target\"),            )) def split(data):     data['data_type'] = \"train\"          X_cols = [         'sepal_length',         'sepal_width',         'petal_length',         'petal_width'     ]     y_col = 'target'          X_train, X_test, y_train, y_test = train_test_split(data[X_cols],                                                          data[y_col],                                                          test_size=0.3,                                                          random_state=1)      X_train['data_type'] = 'train'     X_train['target'] = y_train          X_test['data_type'] = 'test'     X_test['target'] = y_test          data = pd.concat([X_train, X_test])     return data      df = split.run() display(df)  data_type sepal_length sepal_width petal_length petal_width target 118 train 7.7 2.6 6.9 2.3 2 18 train 5.7 3.8 1.7 0.3 0 4 train 5.0 3.6 1.4 0.2 0 45 train 4.8 3.0 1.4 0.3 0 59 train 5.2 2.7 3.9 1.4 1 ... ... ... ... ... ... ... 112 test 6.8 3.0 5.5 2.1 2 17 test 5.1 3.5 1.4 0.3 0 119 test 6.0 2.2 5.0 1.5 2 103 test 6.3 2.9 5.6 1.8 2 58 test 6.6 2.9 4.6 1.3 1 <p>150 rows \u00d7 6 columns</p> In\u00a0[4]: Copied! <pre>import os\nARTIFACT_LOCATION = \"/data/tmp/artifacts/\"\nos.makedirs(ARTIFACT_LOCATION, exist_ok=True)\n</pre> import os ARTIFACT_LOCATION = \"/data/tmp/artifacts/\" os.makedirs(ARTIFACT_LOCATION, exist_ok=True) In\u00a0[5]: Copied! <pre>import os\nimport pickle \nimport mlflow\n\nfrom flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, String\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n@node(\n    type=\"pandas\",\n    description=\"Fits a standard scaler\",\n    tags=[\"data\", \"train\", \"scaler\"],\n    dependencies=[\n        split.select(\n            'data_type',\n            'sepal_length',\n            'sepal_width',\n            'petal_length',\n            'petal_width',\n            'target',\n        )\n    ],\n    output=Schema(\n      Column('data_type', String(), 'train (70%), test (30%)'),  \n      split.output.get(\"sepal_length\"),\n      split.output.get(\"sepal_width\"),\n      split.output.get(\"petal_length\"),\n      split.output.get(\"petal_width\"),\n      split.output.get(\"target\"),      \n    ))    \ndef fit_scale(split):\n    \n    X_cols = [\n        'sepal_length',\n        'sepal_width',\n        'petal_length',\n        'petal_width'\n    ]\n    \n    scaler = StandardScaler()\n    scaler = scaler.fit(split[split['data_type']=='train'][X_cols])\n    \n    if mlflow.active_run():\n        artifact_path = f\"{ARTIFACT_LOCATION}{mlflow.active_run().info.run_id}/model\"\n        if not os.path.exists(artifact_path):\n            os.makedirs(artifact_path, exist_ok=True)\n\n        pickle.dump(scaler, open(os.path.join(artifact_path, 'scaler.pkl'), 'wb'))\n    \n    split[X_cols] = scaler.transform(split[X_cols])\n    \n    return split\n    \ndf = fit_scale.run()\ndisplay(df.head(10))\n</pre> import os import pickle  import mlflow  from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, String from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler  @node(     type=\"pandas\",     description=\"Fits a standard scaler\",     tags=[\"data\", \"train\", \"scaler\"],     dependencies=[         split.select(             'data_type',             'sepal_length',             'sepal_width',             'petal_length',             'petal_width',             'target',         )     ],     output=Schema(       Column('data_type', String(), 'train (70%), test (30%)'),         split.output.get(\"sepal_length\"),       split.output.get(\"sepal_width\"),       split.output.get(\"petal_length\"),       split.output.get(\"petal_width\"),       split.output.get(\"target\"),           ))     def fit_scale(split):          X_cols = [         'sepal_length',         'sepal_width',         'petal_length',         'petal_width'     ]          scaler = StandardScaler()     scaler = scaler.fit(split[split['data_type']=='train'][X_cols])          if mlflow.active_run():         artifact_path = f\"{ARTIFACT_LOCATION}{mlflow.active_run().info.run_id}/model\"         if not os.path.exists(artifact_path):             os.makedirs(artifact_path, exist_ok=True)          pickle.dump(scaler, open(os.path.join(artifact_path, 'scaler.pkl'), 'wb'))          split[X_cols] = scaler.transform(split[X_cols])          return split      df = fit_scale.run() display(df.head(10)) data_type sepal_length sepal_width petal_length petal_width target 118 train 2.260502 -1.050897 1.776229 1.423710 2 18 train -0.118974 1.827647 -1.144919 -1.142634 0 4 train -0.951790 1.347889 -1.313447 -1.270951 0 45 train -1.189738 -0.091382 -1.313447 -1.142634 0 59 train -0.713843 -0.811018 0.090951 0.268855 1 39 train -0.832816 0.868132 -1.257271 -1.270951 0 36 train -0.356921 1.108011 -1.369623 -1.270951 0 117 train 2.260502 1.827647 1.663877 1.295393 2 139 train 1.308712 0.148496 0.933590 1.167075 2 107 train 1.784607 -0.331261 1.439174 0.782124 2 In\u00a0[6]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, String, Integer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom mlflow.models.signature import infer_signature\n\n@node(\n    type=\"pandas\",\n    description=\"Model training using SVM\",\n    tags=[\"model\", \"svm\"],\n    dependencies=[\n        fit_scale.select(\n            'data_type',\n            'sepal_length',\n            'sepal_width',\n            'petal_length',\n            'petal_width',\n            'target',\n        ).alias(\"df\")\n    ],\n    output=Schema(\n      Column('data_type', String(), 'train (70%), test (30%)'),  \n      fit_scale.output.get(\"sepal_length\"),\n      fit_scale.output.get(\"sepal_width\"),\n      fit_scale.output.get(\"petal_length\"),\n      fit_scale.output.get(\"petal_width\"),\n      fit_scale.output.get(\"target\"),      \n      Column('prediction', Integer(), 'prediction'),  \n    ))\ndef train_svm_model(df):\n    \n    X_cols = [\n        'sepal_length',\n        'sepal_width',\n        'petal_length',\n        'petal_width'\n    ]\n    \n    X_train = df[df['data_type']=='train']\n    y_train = X_train['target']\n    X_train = X_train[X_cols]\n    \n    clf = svm.SVC().fit(X_train, y_train)\n        \n    if mlflow.active_run():\n        signature = infer_signature(X_train, y_train)\n        mlflow.sklearn.log_model(clf, \n                                 \"model\", \n                                 signature=signature, \n                                 input_example=X_train.head(5))\n  \n\n    df['prediction'] = clf.predict(df[X_cols])\n    return df\n    \ndf = train_svm_model.run()\ndisplay(df)\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, String, Integer from sklearn.model_selection import train_test_split from sklearn import svm from mlflow.models.signature import infer_signature  @node(     type=\"pandas\",     description=\"Model training using SVM\",     tags=[\"model\", \"svm\"],     dependencies=[         fit_scale.select(             'data_type',             'sepal_length',             'sepal_width',             'petal_length',             'petal_width',             'target',         ).alias(\"df\")     ],     output=Schema(       Column('data_type', String(), 'train (70%), test (30%)'),         fit_scale.output.get(\"sepal_length\"),       fit_scale.output.get(\"sepal_width\"),       fit_scale.output.get(\"petal_length\"),       fit_scale.output.get(\"petal_width\"),       fit_scale.output.get(\"target\"),             Column('prediction', Integer(), 'prediction'),       )) def train_svm_model(df):          X_cols = [         'sepal_length',         'sepal_width',         'petal_length',         'petal_width'     ]          X_train = df[df['data_type']=='train']     y_train = X_train['target']     X_train = X_train[X_cols]          clf = svm.SVC().fit(X_train, y_train)              if mlflow.active_run():         signature = infer_signature(X_train, y_train)         mlflow.sklearn.log_model(clf,                                   \"model\",                                   signature=signature,                                   input_example=X_train.head(5))         df['prediction'] = clf.predict(df[X_cols])     return df      df = train_svm_model.run() display(df) data_type sepal_length sepal_width petal_length petal_width target prediction 118 train 2.260502 -1.050897 1.776229 1.423710 2 2 18 train -0.118974 1.827647 -1.144919 -1.142634 0 0 4 train -0.951790 1.347889 -1.313447 -1.270951 0 0 45 train -1.189738 -0.091382 -1.313447 -1.142634 0 0 59 train -0.713843 -0.811018 0.090951 0.268855 1 1 ... ... ... ... ... ... ... ... 112 test 1.189738 -0.091382 0.989766 1.167075 2 2 17 test -0.832816 1.108011 -1.313447 -1.142634 0 0 119 test 0.237948 -2.010411 0.708887 0.397172 2 1 103 test 0.594869 -0.331261 1.045942 0.782124 2 2 58 test 0.951790 -0.331261 0.484183 0.140538 1 1 <p>150 rows \u00d7 7 columns</p> In\u00a0[7]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, String, Integer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom mlflow.models.signature import infer_signature\nfrom sklearn.metrics import f1_score\n\n@node(\n    type=\"pandas\",\n    description=\"Model training using SVM\",\n    tags=[\"model\", \"svm\"],\n    dependencies=[\n        train_svm_model.select(\n            'data_type',\n            'target',\n            'prediction'\n        ).alias(\"df\")\n    ],\n    output=Schema(\n      Column('data_type', String(), 'all, train or test'),  \n      Column('metric', String(), 'score metric'),  \n      Column('value', Float(), 'value of the metric'),        \n    ))\ndef evaluate(df):\n    result = pd.DataFrame(columns=['data_type', 'metric', 'value'])\n    \n    # All data\n    score = f1_score(df['target'], df['prediction'], average='macro')\n    result.loc[result.shape[0]] = ['all', 'f1_score macro', score]\n    \n    # Train data\n    df_ = df[df['data_type']=='train']\n    score = f1_score(df_['target'], df_['prediction'], average='macro')\n    result.loc[result.shape[0]] = ['train', 'f1_score macro', score]\n    \n    # Test data\n    df_ = df[df['data_type']=='test']\n    score = f1_score(df_['target'], df_['prediction'], average='macro')\n    result.loc[result.shape[0]] = ['test', 'f1_score macro', score]\n    \n    return result\n    \ndf = evaluate.run()\ndisplay(df)\n\ndisplayHTML(evaluate.html())\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, String, Integer from sklearn.model_selection import train_test_split from sklearn import svm from mlflow.models.signature import infer_signature from sklearn.metrics import f1_score  @node(     type=\"pandas\",     description=\"Model training using SVM\",     tags=[\"model\", \"svm\"],     dependencies=[         train_svm_model.select(             'data_type',             'target',             'prediction'         ).alias(\"df\")     ],     output=Schema(       Column('data_type', String(), 'all, train or test'),         Column('metric', String(), 'score metric'),         Column('value', Float(), 'value of the metric'),             )) def evaluate(df):     result = pd.DataFrame(columns=['data_type', 'metric', 'value'])          # All data     score = f1_score(df['target'], df['prediction'], average='macro')     result.loc[result.shape[0]] = ['all', 'f1_score macro', score]          # Train data     df_ = df[df['data_type']=='train']     score = f1_score(df_['target'], df_['prediction'], average='macro')     result.loc[result.shape[0]] = ['train', 'f1_score macro', score]          # Test data     df_ = df[df['data_type']=='test']     score = f1_score(df_['target'], df_['prediction'], average='macro')     result.loc[result.shape[0]] = ['test', 'f1_score macro', score]          return result      df = evaluate.run() display(df)  displayHTML(evaluate.html()) data_type metric value 0 all f1_score macro 0.973323 1 train f1_score macro 0.980475 2 test f1_score macro 0.955840 Out[7]: In\u00a0[8]: Copied! <pre>import os\nimport mlflow\nfrom mlflow import log_metric, log_param, log_artifacts\nfrom mlflow.exceptions import MlflowException\n\n\n# Ends any actve mlflow run\ntry:\n    mlflow.end_run()\nexcept Exception as e:\n    pass\n\n\"\"\"\nCreates or gets an experiment from /Shared folder\nSets the artifact location to the mounted blob\n\"\"\"\n\ntry:\n    experiment_id = mlflow.create_experiment('flypipe_demo', artifact_location=ARTIFACT_LOCATION)\nexcept MlflowException as m:\n    pass\nfinally:\n    experiment = mlflow.get_experiment_by_name('flypipe_demo')\n    experiment_id = experiment.experiment_id\n    \n\n\"\"\"\nStarts the mlflow run with the experiment\n\"\"\"    \nmlflow.start_run(experiment_id=experiment_id) \nRUN_ID = mlflow.active_run().info.run_id\nprint(f\"Training run_id: {RUN_ID}\")\n\ndf = evaluate.run()\n\ndisplay(df)\nmlflow.end_run()\n</pre> import os import mlflow from mlflow import log_metric, log_param, log_artifacts from mlflow.exceptions import MlflowException   # Ends any actve mlflow run try:     mlflow.end_run() except Exception as e:     pass  \"\"\" Creates or gets an experiment from /Shared folder Sets the artifact location to the mounted blob \"\"\"  try:     experiment_id = mlflow.create_experiment('flypipe_demo', artifact_location=ARTIFACT_LOCATION) except MlflowException as m:     pass finally:     experiment = mlflow.get_experiment_by_name('flypipe_demo')     experiment_id = experiment.experiment_id       \"\"\" Starts the mlflow run with the experiment \"\"\"     mlflow.start_run(experiment_id=experiment_id)  RUN_ID = mlflow.active_run().info.run_id print(f\"Training run_id: {RUN_ID}\")  df = evaluate.run()  display(df) mlflow.end_run() <pre>Training run_id: 68c20c5c0ca34d83aff24293ae20ed24\n</pre> data_type metric value 0 all f1_score macro 0.973323 1 train f1_score macro 0.980475 2 test f1_score macro 0.955840 In\u00a0[9]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, String\nfrom sklearn.model_selection import train_test_split\n\n@node(\n    type=\"pandas\",\n    description=\"Split train (70%) and test (30%) data\",\n    tags=[\"data\", \"split\"],\n    dependencies=[\n        data.select(\n            'sepal_length',\n            'sepal_width',\n            'petal_length',\n            'petal_width',\n        )\n    ],\n    output=Schema(\n      data.output.get(\"sepal_length\"),\n      data.output.get(\"sepal_width\"),\n      data.output.get(\"petal_length\"),\n      data.output.get(\"petal_width\"),\n    ))\ndef scale(data):\n    \n    X_cols = [\n        'sepal_length',\n        'sepal_width',\n        'petal_length',\n        'petal_width'\n    ]\n    \n    with open(f'{ARTIFACT_LOCATION}{RUN_ID}/model/scaler.pkl', 'rb') as fp:\n        scaler = pickle.load(fp)\n        data[X_cols] = scaler.transform(data[X_cols])\n        \n    return data\n\ndf = scale.run()\ndisplay(df)\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, String from sklearn.model_selection import train_test_split  @node(     type=\"pandas\",     description=\"Split train (70%) and test (30%) data\",     tags=[\"data\", \"split\"],     dependencies=[         data.select(             'sepal_length',             'sepal_width',             'petal_length',             'petal_width',         )     ],     output=Schema(       data.output.get(\"sepal_length\"),       data.output.get(\"sepal_width\"),       data.output.get(\"petal_length\"),       data.output.get(\"petal_width\"),     )) def scale(data):          X_cols = [         'sepal_length',         'sepal_width',         'petal_length',         'petal_width'     ]          with open(f'{ARTIFACT_LOCATION}{RUN_ID}/model/scaler.pkl', 'rb') as fp:         scaler = pickle.load(fp)         data[X_cols] = scaler.transform(data[X_cols])              return data  df = scale.run() display(df) sepal_length sepal_width petal_length petal_width 0 -0.832816 1.108011 -1.313447 -1.270951 1 -1.070764 -0.091382 -1.313447 -1.270951 2 -1.308712 0.388375 -1.369623 -1.270951 3 -1.427685 0.148496 -1.257271 -1.270951 4 -0.951790 1.347889 -1.313447 -1.270951 ... ... ... ... ... 145 1.070764 -0.091382 0.821239 1.423710 146 0.594869 -1.290775 0.708887 0.910441 147 0.832816 -0.091382 0.821239 1.038758 148 0.475895 0.868132 0.933590 1.423710 149 0.118974 -0.091382 0.765063 0.782124 <p>150 rows \u00d7 4 columns</p> In\u00a0[10]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Float, String\nfrom sklearn.model_selection import train_test_split\n\n@node(\n    type=\"pandas\",\n    description=\"Split train (70%) and test (30%) data\",\n    tags=[\"data\", \"split\"],\n    dependencies=[\n        scale.select(\n            'sepal_length',\n            'sepal_width',\n            'petal_length',\n            'petal_width'\n        ).alias(\"df\")\n    ],\n    output=Schema(\n      Column('prediction', Integer(), 'prediction'),  \n      \n    ))\ndef predict(df):\n    model_path = f'runs:/{RUN_ID}/model'\n    loaded_model = mlflow.pyfunc.load_model(model_path)\n\n    df['prediction'] = loaded_model.predict(df)\n    return df\n\ndf = predict.run()\ndisplay(df)\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import Float, String from sklearn.model_selection import train_test_split  @node(     type=\"pandas\",     description=\"Split train (70%) and test (30%) data\",     tags=[\"data\", \"split\"],     dependencies=[         scale.select(             'sepal_length',             'sepal_width',             'petal_length',             'petal_width'         ).alias(\"df\")     ],     output=Schema(       Column('prediction', Integer(), 'prediction'),              )) def predict(df):     model_path = f'runs:/{RUN_ID}/model'     loaded_model = mlflow.pyfunc.load_model(model_path)      df['prediction'] = loaded_model.predict(df)     return df  df = predict.run() display(df)  prediction 0 0 1 0 2 0 3 0 4 0 ... ... 145 2 146 2 147 2 148 2 149 2 <p>150 rows \u00d7 1 columns</p> In\u00a0[11]: Copied! <pre>@node(\n    type=\"pandas\",\n    description=\"Graph to train and predict Iris Data set\",\n    dependencies=[\n        evaluate,\n        predict\n    ])\ndef graph(evaluate, predict):\n    raise NotImplemented('Not supposed to run, only used to display the graph')\n    \ndisplayHTML(graph.html())\n</pre> @node(     type=\"pandas\",     description=\"Graph to train and predict Iris Data set\",     dependencies=[         evaluate,         predict     ]) def graph(evaluate, predict):     raise NotImplemented('Not supposed to run, only used to display the graph')      displayHTML(graph.html()) Out[11]: In\u00a0[12]: Copied! <pre># Hard coded data to be predicted\ndf = pd.DataFrame(data = {\n        'sepal_length': [6.6],\n        'sepal_width': [3.1],\n        'petal_length': [5.1],\n        'petal_width': [2.4]\n    })\n\n# Run the predictions\npredictions = (\n    predict\n    .run(inputs={\n        data: df\n    })\n)\n\n# Show predictions\ndisplay(predictions)\n\n# How the execution graph\ndisplayHTML(\n    predict\n    .html(inputs={\n        data: df\n    })\n)\n</pre> # Hard coded data to be predicted df = pd.DataFrame(data = {         'sepal_length': [6.6],         'sepal_width': [3.1],         'petal_length': [5.1],         'petal_width': [2.4]     })  # Run the predictions predictions = (     predict     .run(inputs={         data: df     }) )  # Show predictions display(predictions)  # How the execution graph displayHTML(     predict     .html(inputs={         data: df     }) ) prediction 0 2 Out[12]:"},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#ml-graphs-with-mlflow","title":"ML Graphs (with mlflow)\u00b6","text":"<p>Flypipe allows creation of graphs to train ML models and run predictions.</p> <p>Here as example, we are building the following graph to train, evaluate and predict.</p> <p>Training Graph</p> <ul> <li>data: loads sklearn iris dataset into a dataframe</li> <li>split: splits the data into train/test data</li> <li>fit_scale: fit and scale the data using sklearn Standard Scaler</li> <li>train_svm_model: trains a sklearn SVM model on train data and returns the prediction</li> <li>evaluate: calculates evaluation metrics</li> </ul> <p>Prediction Graph</p> <ul> <li>scale: scales the data using the scaler fit on node <code>fit_scale</code></li> <li>predict: loads the SVM model trained in the node <code>train_svm_model</code> and does the predictions</li> </ul> <p>Governance</p> <ul> <li>graph: dummy node used to plot all related graphs to the model</li> </ul> <p>In this section, we are using mlflow to save and loand ML artifacts</p>"},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#training-graph","title":"Training Graph\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#data","title":"Data\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#split-data-as-train-70-and-test-30","title":"Split data as train (70%) and test (30%)\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#fit-and-scale","title":"Fit and Scale\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#train-svm-model","title":"Train SVM Model\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#evaluate","title":"Evaluate\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#executing-training-evaluation","title":"Executing Training &amp; Evaluation\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#prediction-graph","title":"Prediction Graph\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#scale-data-for-prediction","title":"Scale data for prediction\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#predict","title":"Predict\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#document-training-and-prediction-graphs","title":"Document Training and Prediction Graphs\u00b6","text":""},{"location":"notebooks/miscellaneous/databricks/ml-graphs/#run-predictions-with-provided-data","title":"Run predictions with provided data\u00b6","text":"<p>We can make use of <code>inputs</code> when running the node <code>predict</code>. It allow us to give a custom data to be scaled and retrieve the predictions.</p> <p>This feature is useful when making analysis or using the same pipeline in other environments like APIs (discussed later on).</p> <p>Here we are giving 1 example of sepal and petal lenght and widths. Note that the execution graph will skip <code>data</code> node and use the data we are providing as input.</p>"},{"location":"notebooks/quick_start/databricks/","title":"Databricks","text":"<p>Below we have a very simple transformation pipeline setup that shows how Flypipe might be used. Given the names of various fruits, we will do some minor cleaning of the data and add two columns- color and category.</p> In\u00a0[6]: Copied! <pre>%pip install flypipe\n</pre> %pip install flypipe <pre>Requirement already satisfied: flypipe in /usr/local/lib/python3.9/site-packages (4.3.2)\nRequirement already satisfied: Jinja2&gt;=3.1 in /usr/local/lib/python3.9/site-packages (from flypipe) (3.1.6)\nRequirement already satisfied: networkx&gt;=3.1 in /usr/local/lib/python3.9/site-packages (from flypipe) (3.2.1)\nRequirement already satisfied: tabulate&gt;=0.9 in /usr/local/lib/python3.9/site-packages (from flypipe) (0.9.0)\nRequirement already satisfied: sparkleframe&gt;=0.2 in /usr/local/lib/python3.9/site-packages (from flypipe) (0.3.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.9/site-packages (from Jinja2&gt;=3.1-&gt;flypipe) (3.0.2)\nRequirement already satisfied: polars&gt;=1.30 in /usr/local/lib/python3.9/site-packages (from sparkleframe&gt;=0.2-&gt;flypipe) (1.30.0)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[7]: Copied! <pre>df = spark.createDataFrame(\n    schema=(\"_fruit\",),\n    data=[\n        (\"ORANGE\",),\n        (\"WATERMELON\",),\n        (\"LEMON\",),\n    ]\n)\n\ndf.createOrReplaceTempView(\"table\")\n\ndisplay(df)\n</pre> df = spark.createDataFrame(     schema=(\"_fruit\",),     data=[         (\"ORANGE\",),         (\"WATERMELON\",),         (\"LEMON\",),     ] )  df.createOrReplaceTempView(\"table\")  display(df) _fruit ORANGE WATERMELON LEMON In\u00a0[8]: Copied! <pre>from flypipe import node\nfrom flypipe.datasource.spark import Spark\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\n\nimport pyspark.sql.functions as F\n@node(\n    type=\"pyspark\",\n    dependencies=[\n        Spark(\"table\").select(\"_fruit\").alias(\"df\")\n    ],\n    output=Schema(\n     Column(\"fruit\", String(), \"fruit name\"),\n    )\n)\ndef clean(df):\n    df = df.withColumnRenamed('_fruit', 'fruit')\n    df = df.withColumn('fruit', F.lower(F.col('fruit')))\n    return df\n\n\n\n@node(\n    type=\"pyspark\",\n    dependencies=[\n       clean.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        Column(\"fruit\", String(), \"fruit name\"),\n        Column(\"color\", String(), \"color of the fruit\"),\n    )\n)\ndef color(df):\n    \n    replacements = {\n        \"blackberry\": \"black\",\n        \"strawberry\": \"red\",\n        \"orange\": \"orange\",\n        \"watermelon\": \"red\",\n        \"lemon\": \"yellow\",\n        \"plum\": \"purple\",\n    }\n    \n    df = df.withColumn(\"color\", F.col(\"fruit\"))\n    df = df.replace(list(replacements.keys()), list(replacements.values()), \"color\")\n    return df\n\n\n\n@node(\n    type=\"pyspark\",\n    dependencies=[\n       clean.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        Column(\"fruit\", String(), \"fruit name\"),\n        Column(\"category\", String(), \"category of the fruit\"),\n    )\n)\ndef category(df):\n    \n    replacements = {\n        \"blackberry\": \"berry\",\n        \"strawberry\": \"berry\",\n        \"orange\": \"citrus\",\n        \"watermelon\": \"misc\",\n        \"lemon\": \"citrus\",\n        \"plum\": \"stonefruit\",\n    }\n    \n    df = df.withColumn(\"category\", F.col(\"fruit\"))\n    df = df.replace(list(replacements.keys()), list(replacements.values()), \"category\")\n    return df\n\n\n\n\n@node(\n    type=\"pyspark\",\n    dependencies=[\n       color.select(\"fruit\", \"color\"),\n       category.select(\"fruit\", \"category\")  \n    ],\n    output=Schema(\n        Column(\"fruit\", String(), \"fruit description\"),\n        Column(\"color\", String(), \"color of the fruit\"),\n        Column(\"category\", String(), \"category of the fruit\"),\n    )\n)\ndef fruits(color, category):\n    return color.join(category, on=\"fruit\", how=\"left\")\n</pre> from flypipe import node from flypipe.datasource.spark import Spark from flypipe.schema import Schema, Column from flypipe.schema.types import String  import pyspark.sql.functions as F @node(     type=\"pyspark\",     dependencies=[         Spark(\"table\").select(\"_fruit\").alias(\"df\")     ],     output=Schema(      Column(\"fruit\", String(), \"fruit name\"),     ) ) def clean(df):     df = df.withColumnRenamed('_fruit', 'fruit')     df = df.withColumn('fruit', F.lower(F.col('fruit')))     return df    @node(     type=\"pyspark\",     dependencies=[        clean.select(\"fruit\").alias(\"df\")     ],     output=Schema(         Column(\"fruit\", String(), \"fruit name\"),         Column(\"color\", String(), \"color of the fruit\"),     ) ) def color(df):          replacements = {         \"blackberry\": \"black\",         \"strawberry\": \"red\",         \"orange\": \"orange\",         \"watermelon\": \"red\",         \"lemon\": \"yellow\",         \"plum\": \"purple\",     }          df = df.withColumn(\"color\", F.col(\"fruit\"))     df = df.replace(list(replacements.keys()), list(replacements.values()), \"color\")     return df    @node(     type=\"pyspark\",     dependencies=[        clean.select(\"fruit\").alias(\"df\")     ],     output=Schema(         Column(\"fruit\", String(), \"fruit name\"),         Column(\"category\", String(), \"category of the fruit\"),     ) ) def category(df):          replacements = {         \"blackberry\": \"berry\",         \"strawberry\": \"berry\",         \"orange\": \"citrus\",         \"watermelon\": \"misc\",         \"lemon\": \"citrus\",         \"plum\": \"stonefruit\",     }          df = df.withColumn(\"category\", F.col(\"fruit\"))     df = df.replace(list(replacements.keys()), list(replacements.values()), \"category\")     return df     @node(     type=\"pyspark\",     dependencies=[        color.select(\"fruit\", \"color\"),        category.select(\"fruit\", \"category\")       ],     output=Schema(         Column(\"fruit\", String(), \"fruit description\"),         Column(\"color\", String(), \"color of the fruit\"),         Column(\"category\", String(), \"category of the fruit\"),     ) ) def fruits(color, category):     return color.join(category, on=\"fruit\", how=\"left\")  In\u00a0[9]: Copied! <pre>displayHTML(fruits.html())\n</pre> displayHTML(fruits.html()) Out[9]: In\u00a0[5]: Copied! <pre>df = fruits.run(spark)\ndisplay(df)\n</pre> df = fruits.run(spark) display(df) fruitcolorcategory orangeorangecitrus lemonyellowcitrus watermelonredmisc"},{"location":"notebooks/quick_start/databricks/#databricks","title":"Databricks\u00b6","text":""},{"location":"notebooks/quick_start/databricks/#install-flypipe","title":"Install flypipe\u00b6","text":""},{"location":"notebooks/quick_start/databricks/#create-a-temporary-view-representing-a-table","title":"Create a temporary view representing a table\u00b6","text":""},{"location":"notebooks/quick_start/databricks/#create-a-graph","title":"Create a graph\u00b6","text":""},{"location":"notebooks/quick_start/databricks/#execution-graph","title":"Execution Graph\u00b6","text":""},{"location":"notebooks/quick_start/databricks/#running-a-pipeline","title":"Running a pipeline\u00b6","text":""},{"location":"notebooks/quick_start/pandas/","title":"Pandas","text":"<p>Below we have a very simple transformation pipeline setup that shows how Flypipe might be used. Given the names of various fruits, we will do some minor cleaning of the data and add two columns- color and category.</p> In\u00a0[1]: Copied! <pre>from flypipe import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pandas as pd\n\n# Create the Graph\n@node(\n    type=\"pandas\",\n    output=Schema(\n     Column(\"_fruit\", String(), \"_fruit\"),\n    )\n)\ndef table():\n    return pd.DataFrame(data={'_fruit': ['ORANGE', 'WATERMELON', 'LEMON']})\n\n\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n        table.select(\"_fruit\").alias(\"df\")\n    ],\n    output=Schema(\n     Column(\"fruit\", String(), \"fruit name\"),\n    )\n)\ndef clean(df):\n    df = df.rename(columns={'_fruit': 'fruit'})\n    df['fruit'] = df['fruit'].str.lower()\n    return df\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n       clean.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        Column(\"fruit\", String(), \"fruit name\"),\n        Column(\"color\", String(), \"color of the fruit\"),\n    )\n)\ndef color(df):\n    \n    replacements = {\n        \"blackberry\": \"black\",\n        \"strawberry\": \"red\",\n        \"orange\": \"orange\",\n        \"watermelon\": \"red\",\n        \"lemon\": \"yellow\",\n        \"plum\": \"purple\",\n    }\n    \n    df['color'] = df['fruit']\n    df = df.replace({\"color\": replacements})\n    return df\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n       clean.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        Column(\"fruit\", String(), \"fruit name\"),\n        Column(\"category\", String(), \"category of the fruit\"),\n    )\n)\ndef category(df):\n    \n    replacements = {\n        \"blackberry\": \"berry\",\n        \"strawberry\": \"berry\",\n        \"orange\": \"citrus\",\n        \"watermelon\": \"misc\",\n        \"lemon\": \"citrus\",\n        \"plum\": \"stonefruit\",\n    }\n    \n    df['category'] = df['fruit']\n    df = df.replace({\"category\": replacements})\n    return df\n\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n       color.select(\"fruit\", \"color\"),\n       category.select(\"fruit\", \"category\")  \n    ],\n    output=Schema(\n        Column(\"fruit\", String(), \"fruit name\"),\n        Column(\"color\", String(), \"color of the fruit\"),\n        Column(\"category\", String(), \"category of the fruit\"),\n    )\n)\ndef fruits(color, category):\n    return color.merge(category, on=\"fruit\", how=\"left\")\n</pre> from flypipe import node from flypipe.schema import Schema, Column from flypipe.schema.types import String import pandas as pd  # Create the Graph @node(     type=\"pandas\",     output=Schema(      Column(\"_fruit\", String(), \"_fruit\"),     ) ) def table():     return pd.DataFrame(data={'_fruit': ['ORANGE', 'WATERMELON', 'LEMON']})    @node(     type=\"pandas\",     dependencies=[         table.select(\"_fruit\").alias(\"df\")     ],     output=Schema(      Column(\"fruit\", String(), \"fruit name\"),     ) ) def clean(df):     df = df.rename(columns={'_fruit': 'fruit'})     df['fruit'] = df['fruit'].str.lower()     return df  @node(     type=\"pandas\",     dependencies=[        clean.select(\"fruit\").alias(\"df\")     ],     output=Schema(         Column(\"fruit\", String(), \"fruit name\"),         Column(\"color\", String(), \"color of the fruit\"),     ) ) def color(df):          replacements = {         \"blackberry\": \"black\",         \"strawberry\": \"red\",         \"orange\": \"orange\",         \"watermelon\": \"red\",         \"lemon\": \"yellow\",         \"plum\": \"purple\",     }          df['color'] = df['fruit']     df = df.replace({\"color\": replacements})     return df  @node(     type=\"pandas\",     dependencies=[        clean.select(\"fruit\").alias(\"df\")     ],     output=Schema(         Column(\"fruit\", String(), \"fruit name\"),         Column(\"category\", String(), \"category of the fruit\"),     ) ) def category(df):          replacements = {         \"blackberry\": \"berry\",         \"strawberry\": \"berry\",         \"orange\": \"citrus\",         \"watermelon\": \"misc\",         \"lemon\": \"citrus\",         \"plum\": \"stonefruit\",     }          df['category'] = df['fruit']     df = df.replace({\"category\": replacements})     return df   @node(     type=\"pandas\",     dependencies=[        color.select(\"fruit\", \"color\"),        category.select(\"fruit\", \"category\")       ],     output=Schema(         Column(\"fruit\", String(), \"fruit name\"),         Column(\"color\", String(), \"color of the fruit\"),         Column(\"category\", String(), \"category of the fruit\"),     ) ) def fruits(color, category):     return color.merge(category, on=\"fruit\", how=\"left\")  In\u00a0[2]: Copied! <pre>displayHTML(fruits.html())\n</pre> displayHTML(fruits.html()) Out[2]: In\u00a0[3]: Copied! <pre># Run \ndf = fruits.run()\nprint(df)\n</pre> # Run  df = fruits.run() print(df) <pre>        fruit   color category\n0      orange  orange   citrus\n1  watermelon     red     misc\n2       lemon  yellow   citrus\n</pre>"},{"location":"notebooks/quick_start/pandas/#pandas","title":"Pandas\u00b6","text":""},{"location":"notebooks/usage/cache/","title":"How it works?","text":"<p>Some nodes can represent tables or can be quite expensive to re-compute on every run. In these cases, it is possible to cache or persist the output of the node as you want.</p> <p>If you want to enable cache for a specific node you have to crate a <code>class</code> that tells Flypipe how to <code>read</code>, <code>write</code> and check if the cache or persisted data <code>exists</code>.</p> <p>Example:</p> <pre>from flypipe.cache import Cache\n\nimport pandas as pd\n\n\nclass MyCustomPersistance(Cache):\n    def __init__(self, csv_path_name: str):\n        self.csv_path_name = csv_path_name\n\n    def read(self, spark):\n        \"\"\"\n        Reads the persisted/cached data into a dataframe\n        \"\"\"\n        return pd.read_csv(self.csv_path_name)\n\n    def write(self, spark, df):\n        \"\"\"\n        Cache or persist the data\n        \"\"\"\n        df.to_csv(self.csv_path_name, index=False)\n        \n    def exists(self, spark):\n        \"\"\"\n        Check if the data has been cached or persisted.\n        \"\"\"\n        return os.path.exists(self.csv_path_name)\n</pre> <p>Having defined your cache/persistance class, you can start marking nodes to be cached, for instance:</p> <pre>@node(\n    ...\n    cache = MyCustomPersistance(\"data.csv\")\n    ...\n)\ndef t0():\n    ...\n</pre> In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nfrom flypipe.cache import Cache\n\n\nclass SaveAsCSV(Cache):\n    def __init__(self, csv_path_name: str):\n        self.csv_path_name = csv_path_name\n\n    def read(self, spark):\n        print(f\"Reading CSV `{self.csv_path_name}`...\")\n        return pd.read_csv(self.csv_path_name)\n\n    def write(self, spark, df):\n        print(f\"Writing CSV `{self.csv_path_name}`...\")\n        df.to_csv(self.csv_path_name, index=False)\n        \n    def exists(self, spark):\n        csv_exists = os.path.exists(self.csv_path_name)\n        print(f\"CSV `{self.csv_path_name}` exists?\", csv_exists)\n        return os.path.exists(self.csv_path_name)\n</pre> import os import pandas as pd from flypipe.cache import Cache   class SaveAsCSV(Cache):     def __init__(self, csv_path_name: str):         self.csv_path_name = csv_path_name      def read(self, spark):         print(f\"Reading CSV `{self.csv_path_name}`...\")         return pd.read_csv(self.csv_path_name)      def write(self, spark, df):         print(f\"Writing CSV `{self.csv_path_name}`...\")         df.to_csv(self.csv_path_name, index=False)              def exists(self, spark):         csv_exists = os.path.exists(self.csv_path_name)         print(f\"CSV `{self.csv_path_name}` exists?\", csv_exists)         return os.path.exists(self.csv_path_name) In\u00a0[35]: Copied! <pre>from typing import List\nfrom flypipe.cache import Cache\n\n\nclass SparkTable(Cache):\n    def __init__(self, \n                 table_name: str, \n                 schema: str,\n                 merge_keys: List[str] = None,\n                 partition_columns: List[str] = None,\n                 schema_location: str = None):\n        \n        self.table_name = table_name\n        self.schema = schema\n        self.merge_keys = merge_keys\n        self.partition_columns = partition_columns\n        self.schema_location = schema_location\n    \n    @property\n    def table(self):\n        return f\"{self.schema}.{self.table_name}\"\n    \n    def read(self, spark):\n        return spark.table(self.table)\n\n    def write(self, spark, df):\n        \n        # check if database exists\n        if not spark.catalog.databaseExists(self.schema):\n            print(f\"Creating database `{self.schema}`\")\n            location = f\"LOCATION '{self.schema_location}'\" if self.schema_location else \"\"\n            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.schema} {location}\")\n            \n        # check if table exists\n        if not spark.catalog.tableExists(self.table_name, self.schema):\n            print(f\"Creating table `{self.table}`\")\n            df = df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\")\n            \n            if self.partition_columns:\n                df = df.partitionBy(*self.partition_columns)\n                \n            df.saveAsTable(self.table)            \n        else:\n            # table already exists, merge into\n            print(f\"Merging into table `{self.table}`\")\n            df.createOrReplaceTempView(\"updates\")\n            keys = \" AND \".join([f\"s.{col} = t.{col}\" for col in self.merge_keys])\n            \n            merge_query = f\"\"\"\n                MERGE INTO {self.table} t\n                USING updates s\n                ON {keys}\n                WHEN MATCHED THEN UPDATE SET *\n                WHEN NOT MATCHED THEN INSERT *\n            \"\"\"\n            df._jdf.sparkSession().sql(merge_query)\n        \n    def exists(self, spark):\n        table_exists = spark.catalog.tableExists(self.table_name, self.schema)\n        print(f\"Table {self.table} exists?\", table_exists)\n        return table_exists\n</pre> from typing import List from flypipe.cache import Cache   class SparkTable(Cache):     def __init__(self,                   table_name: str,                   schema: str,                  merge_keys: List[str] = None,                  partition_columns: List[str] = None,                  schema_location: str = None):                  self.table_name = table_name         self.schema = schema         self.merge_keys = merge_keys         self.partition_columns = partition_columns         self.schema_location = schema_location          @property     def table(self):         return f\"{self.schema}.{self.table_name}\"          def read(self, spark):         return spark.table(self.table)      def write(self, spark, df):                  # check if database exists         if not spark.catalog.databaseExists(self.schema):             print(f\"Creating database `{self.schema}`\")             location = f\"LOCATION '{self.schema_location}'\" if self.schema_location else \"\"             spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.schema} {location}\")                      # check if table exists         if not spark.catalog.tableExists(self.table_name, self.schema):             print(f\"Creating table `{self.table}`\")             df = df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\")                          if self.partition_columns:                 df = df.partitionBy(*self.partition_columns)                              df.saveAsTable(self.table)                     else:             # table already exists, merge into             print(f\"Merging into table `{self.table}`\")             df.createOrReplaceTempView(\"updates\")             keys = \" AND \".join([f\"s.{col} = t.{col}\" for col in self.merge_keys])                          merge_query = f\"\"\"                 MERGE INTO {self.table} t                 USING updates s                 ON {keys}                 WHEN MATCHED THEN UPDATE SET *                 WHEN NOT MATCHED THEN INSERT *             \"\"\"             df._jdf.sparkSession().sql(merge_query)              def exists(self, spark):         table_exists = spark.catalog.tableExists(self.table_name, self.schema)         print(f\"Table {self.table} exists?\", table_exists)         return table_exists In\u00a0[36]: Copied! <pre>import os\n\n# Replace 'path_to_file' with the actual file path you want to delete\nfile_path = '/tmp/data.csv'\n\n# Check if file exists before trying to delete it\nif os.path.isfile(file_path):\n    os.remove(file_path)\n    print(f\"File '{file_path}' has been deleted.\")\nelse:\n    print(f\"File '{file_path}' does not exist.\")\n\nprint(\"Dropping table tmp.my_table\")\nspark.sql(\"drop table if exists tmp.my_table\")\n</pre> import os  # Replace 'path_to_file' with the actual file path you want to delete file_path = '/tmp/data.csv'  # Check if file exists before trying to delete it if os.path.isfile(file_path):     os.remove(file_path)     print(f\"File '{file_path}' has been deleted.\") else:     print(f\"File '{file_path}' does not exist.\")  print(\"Dropping table tmp.my_table\") spark.sql(\"drop table if exists tmp.my_table\") <pre>File '/tmp/data.csv' has been deleted.\nDropping table tmp.my_table\n</pre> Out[36]: In\u00a0[37]: Copied! <pre>import pandas as pd\nfrom flypipe import node\nfrom flypipe.cache import Cache \nimport pyspark.sql.functions as F\n\n@node(\n    type=\"pandas\",\n    cache = SaveAsCSV(\"/tmp/data.csv\")\n)\ndef csv_cache():\n    return pd.DataFrame(data={\"id\": [1, 2], \"sales\":[100.0, 34.1]})\n\n@node(\n    type=\"pyspark\",\n    cache = SparkTable(\"my_table\", \"tmp\", merge_keys=[\"id\"], schema_location=\"/tmp\"),\n    dependencies=[csv_cache.select(\"id\", \"sales\").alias(\"df\")]\n)\ndef spark_cache(df):\n    return df.withColumn(\"above_50\", F.col(\"sales\") &gt; 50.0)\n\n\n@node(\n    type=\"pyspark\",\n    dependencies=[\n        spark_cache.select(\"id\", \"sales\", \"above_50\").alias(\"df\"),\n    ]\n)\ndef t0(df):\n    return df\n</pre> import pandas as pd from flypipe import node from flypipe.cache import Cache  import pyspark.sql.functions as F  @node(     type=\"pandas\",     cache = SaveAsCSV(\"/tmp/data.csv\") ) def csv_cache():     return pd.DataFrame(data={\"id\": [1, 2], \"sales\":[100.0, 34.1]})  @node(     type=\"pyspark\",     cache = SparkTable(\"my_table\", \"tmp\", merge_keys=[\"id\"], schema_location=\"/tmp\"),     dependencies=[csv_cache.select(\"id\", \"sales\").alias(\"df\")] ) def spark_cache(df):     return df.withColumn(\"above_50\", F.col(\"sales\") &gt; 50.0)   @node(     type=\"pyspark\",     dependencies=[         spark_cache.select(\"id\", \"sales\", \"above_50\").alias(\"df\"),     ] ) def t0(df):     return df In\u00a0[38]: Copied! <pre>displayHTML(t0.html(spark))\n</pre> displayHTML(t0.html(spark)) <pre>Table tmp.my_table exists? False\nCSV `/tmp/data.csv` exists? False\n</pre> Out[38]: In\u00a0[39]: Copied! <pre>t0.run(spark)\n</pre> t0.run(spark) <pre>Table tmp.my_table exists? False\nCSV `/tmp/data.csv` exists? False\nWriting CSV `/tmp/data.csv`...\nCreating table `tmp.my_table`\n</pre> Out[39]: above_50idsales true1100.0 false234.1 <p>After the 1st run, all caches will be saved</p> In\u00a0[40]: Copied! <pre>displayHTML(t0.html(spark))\n</pre> displayHTML(t0.html(spark)) <pre>Table tmp.my_table exists? True\n</pre> Out[40]: In\u00a0[41]: Copied! <pre>t0.run(spark)\n</pre> t0.run(spark) <pre>Table tmp.my_table exists? True\n</pre> <pre>                                                                                </pre> Out[41]: above_50idsales true1100.0 false234.1 <p>Flypipe will only load necessary caches, for instance, loading the cache of node <code>csv_cache</code> was skipped, as only cache of <code>spark_cache</code> was necessary to run <code>t0</code>.</p> In\u00a0[42]: Copied! <pre>from flypipe.cache import CacheMode\n\ndisplayHTML(\n    t0.html(\n        spark,\n        cache={\n            csv_cache: CacheMode.MERGE,\n            spark_cache: CacheMode.MERGE\n        }\n))\n</pre> from flypipe.cache import CacheMode  displayHTML(     t0.html(         spark,         cache={             csv_cache: CacheMode.MERGE,             spark_cache: CacheMode.MERGE         } )) Out[42]: <p>Currently <code>tmp.my_table</code> data is:</p> In\u00a0[43]: Copied! <pre>%%sql\nselect * from tmp.my_table\n</pre> %%sql select * from tmp.my_table <pre>+--------+---+-----+\n|above_50|id |sales|\n+--------+---+-----+\n|true    |1  |100.0|\n|false   |2  |34.1 |\n+--------+---+-----+\n\n</pre> <pre>None</pre> Out[43]: <pre>''</pre> In\u00a0[44]: Copied! <pre>import pandas as pd\nfrom flypipe.cache import CacheMode\n\nt0.run(\n    spark,\n    cache={\n        csv_cache: CacheMode.MERGE,\n        spark_cache: CacheMode.MERGE\n    },\n    inputs={\n        csv_cache: pd.DataFrame(data={\"id\": [1, 3], \"sales\":[17.25, 547.39]})\n    }\n)\n</pre> import pandas as pd from flypipe.cache import CacheMode  t0.run(     spark,     cache={         csv_cache: CacheMode.MERGE,         spark_cache: CacheMode.MERGE     },     inputs={         csv_cache: pd.DataFrame(data={\"id\": [1, 3], \"sales\":[17.25, 547.39]})     } ) <pre>Merging into table `tmp.my_table`\n</pre> Out[44]: above_50idsales false117.25 true3547.39 <p>Checking the data in <code>tmp.my_table</code> we can see that:</p> <ul> <li>sales changed from <code>100.0</code> to <code>17.25</code> and <code>above_50</code> to <code>false</code>.</li> <li>row of id 2 remaining unchanged</li> <li>added 1 row id 3</li> </ul> In\u00a0[45]: Copied! <pre>%%sql\nselect * from tmp.my_table\n</pre> %%sql select * from tmp.my_table <pre>                                                                                </pre> <pre>+--------+---+------+\n|above_50|id |sales |\n+--------+---+------+\n|false   |1  |17.25 |\n|false   |2  |34.1  |\n|true    |3  |547.39|\n+--------+---+------+\n\n</pre> <pre>None</pre> Out[45]: <pre>''</pre>"},{"location":"notebooks/usage/cache/#cachepersistance-workflow","title":"Cache/Persistance workflow\u00b6","text":"<p>For every node that has cache set up, Flypipe will do the following:</p> <p>Node has cache? \u2003Yes -&gt; cache exists (runs the method <code>exists</code>)? \u2003\u2003Yes -&gt; runs <code>read</code> method and returns the dataframe \u2003\u2003No -&gt; runs the node, collects the output dataframe and runs <code>write</code> method.</p>"},{"location":"notebooks/usage/cache/#example-1-csv-persistence","title":"Example 1: CSV persistence\u00b6","text":""},{"location":"notebooks/usage/cache/#example-2-spark-persistence","title":"Example 2: Spark persistence\u00b6","text":""},{"location":"notebooks/usage/cache/#execution-graph","title":"Execution Graph\u00b6","text":""},{"location":"notebooks/usage/cache/#cleaning-environment","title":"cleaning environment\u00b6","text":""},{"location":"notebooks/usage/cache/#1st-run","title":"1st run\u00b6","text":"<p>When no cache exists all cache nodes will be active</p>"},{"location":"notebooks/usage/cache/#subsequent-runs","title":"Subsequent runs\u00b6","text":"<p>As caches have been saved, the nodes will be inactive as the caches will be loaded on the fly</p>"},{"location":"notebooks/usage/cache/#merging-data","title":"Merging Data\u00b6","text":"<p>Often we need to merge the data, insert new rows and update rows if data already exists. This behaviour will happen accordingly to <code>merge_keys</code>.</p> <pre>@node(\n    ...\n    cache = SparkTable(\n        \"my_table\", \n        \"tmp\", \n        merge_keys=[\"id\"], # &lt;--\n        schema_location=\"/tmp\"),\n    ...\n)\ndef spark_cache(df):\n    ...\n</pre> <p>Rows with non-existent ids in <code>tmp.my_table</code> will be added to <code>tmp.my_table</code>, rows with existent ids, will be updated.</p> <p>Independent of the write mode (insert or update) the rows needs to me re-transformed by the graph, so nodes that were previously <code>skipped</code>, shall be <code>active</code> by changing the <code>CacheMode</code> type.</p>"},{"location":"notebooks/usage/data-source/","title":"Data source","text":"In\u00a0[\u00a0]: Copied! <pre>df = spark.createDataFrame(\n    data=[\n        (\"LEMON\", \"Yellow\",), \n        (\"LIME\", \"Green\",)\n    ], schema=[\"fruit\", \"color\"])\n\ndf.createOrReplaceTempView(\"fruits_table\")\n\ndisplay(df)\n</pre> df = spark.createDataFrame(     data=[         (\"LEMON\", \"Yellow\",),          (\"LIME\", \"Green\",)     ], schema=[\"fruit\", \"color\"])  df.createOrReplaceTempView(\"fruits_table\")  display(df) In\u00a0[\u00a0]: Copied! <pre>from flypipe.node import node\nfrom flypipe.datasource.spark import Spark\n\n@node(\n    type=\"pyspark\",\n    dependencies=[Spark(\"fruits_table\").select(\"fruit\")]\n)\ndef my_fruits(fruits_table):\n    return fruits_table\n\ndf = my_fruits.run(spark)\ndisplay(df)              \n\ndisplayHTML(my_fruits.html())\n</pre> from flypipe.node import node from flypipe.datasource.spark import Spark  @node(     type=\"pyspark\",     dependencies=[Spark(\"fruits_table\").select(\"fruit\")] ) def my_fruits(fruits_table):     return fruits_table  df = my_fruits.run(spark) display(df)                displayHTML(my_fruits.html()) In\u00a0[\u00a0]: Copied! <pre>import os\nimport re\nimport pandas as pd\n\nfrom flypipe.node import node\nfrom flypipe.node_type import NodeType\n\n\ndef CSV(path_csv):\n    @node(\n        type=\"pandas\",\n        description=f\"Loading CSV file at {path_csv}\",\n        tags=[\"datasource\", \"csv\"]\n    )\n    def load_csv():\n        return pd.read_csv(open(path_csv, 'r'))\n\n    file_name = os.path.basename(path_csv)\n    file_name = re.sub(\"[^\\da-zA-Z]\", \"_\", file_name)\n    key = re.sub(\"[^\\da-zA-Z]\", \"_\", path_csv)\n\n    load_csv.function.__name__ = file_name\n    load_csv.key = key\n    load_csv.node_type = NodeType.DATASOURCE\n    return load_csv\n</pre> import os import re import pandas as pd  from flypipe.node import node from flypipe.node_type import NodeType   def CSV(path_csv):     @node(         type=\"pandas\",         description=f\"Loading CSV file at {path_csv}\",         tags=[\"datasource\", \"csv\"]     )     def load_csv():         return pd.read_csv(open(path_csv, 'r'))      file_name = os.path.basename(path_csv)     file_name = re.sub(\"[^\\da-zA-Z]\", \"_\", file_name)     key = re.sub(\"[^\\da-zA-Z]\", \"_\", path_csv)      load_csv.function.__name__ = file_name     load_csv.key = key     load_csv.node_type = NodeType.DATASOURCE     return load_csv In\u00a0[\u00a0]: Copied! <pre>import pytest\nimport pandas as pd\nfrom pyspark_test import assert_pyspark_df_equal\nfrom pandas.testing import assert_frame_equal\n\nfrom flypipe.node import node\nfrom flypipe.datasource.spark import Spark\nfrom flypipe.schema.column import Column\nfrom flypipe.schema.schema import Schema\nfrom flypipe.schema.types import Long\n\n#Fixtures\n@pytest.fixture(scope=\"function\")\ndef dummy_df():\n    import pandas as pd\n    \n    #dummy dataframe\n    df = pd.DataFrame(columns=[\"color\", \"fruit\"])\n    df.loc[0] = [\"yellow\", \"lemon\"]\n    df.loc[0] = [\"green\", \"lime\"]\n    \n    #save dataframe \n    df.to_csv(\"/tmp/test.csv\", index=False)\n    \n    return df\n\n\nclass TestPyspark:\n    \n    def test_(self, dummy_df):\n        \"\"\"\n        Test a CSV datasource\n        \"\"\"\n        \n        @node(\n            type=\"pandas\",\n            dependencies=[\n                CSV(\"/tmp/test.csv\").select([\"fruit\", \"color\"])\n            ]\n        )\n        def t1(test_csv):\n            return test_csv\n\n        df = t1.run()\n        assert_frame_equal(dummy_df, df)\n        \nimport ipytest\nipytest.run()\n</pre> import pytest import pandas as pd from pyspark_test import assert_pyspark_df_equal from pandas.testing import assert_frame_equal  from flypipe.node import node from flypipe.datasource.spark import Spark from flypipe.schema.column import Column from flypipe.schema.schema import Schema from flypipe.schema.types import Long  #Fixtures @pytest.fixture(scope=\"function\") def dummy_df():     import pandas as pd          #dummy dataframe     df = pd.DataFrame(columns=[\"color\", \"fruit\"])     df.loc[0] = [\"yellow\", \"lemon\"]     df.loc[0] = [\"green\", \"lime\"]          #save dataframe      df.to_csv(\"/tmp/test.csv\", index=False)          return df   class TestPyspark:          def test_(self, dummy_df):         \"\"\"         Test a CSV datasource         \"\"\"                  @node(             type=\"pandas\",             dependencies=[                 CSV(\"/tmp/test.csv\").select([\"fruit\", \"color\"])             ]         )         def t1(test_csv):             return test_csv          df = t1.run()         assert_frame_equal(dummy_df, df)          import ipytest ipytest.run()"},{"location":"notebooks/usage/data-source/#spark","title":"Spark\u00b6","text":""},{"location":"notebooks/usage/data-source/#adding-your-own-data-source","title":"Adding your own data source\u00b6","text":""},{"location":"notebooks/usage/data-source/#testing","title":"Testing\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/","title":"Multi Node Graphs","text":"In\u00a0[1]: Copied! <pre>#Dummy raw data\n\ndf = spark.createDataFrame(\n    schema=(\"_fruit\",),\n    data=[\n        (\"ORANGE\",),\n        (\"WATERMELON\",),\n        (\"LEMON\",),\n    ]\n)\ndf.createOrReplaceTempView(\"table\")\ndisplay(df)\n</pre> #Dummy raw data  df = spark.createDataFrame(     schema=(\"_fruit\",),     data=[         (\"ORANGE\",),         (\"WATERMELON\",),         (\"LEMON\",),     ] ) df.createOrReplaceTempView(\"table\") display(df) _fruit ORANGE WATERMELON LEMON In\u00a0[2]: Copied! <pre>from flypipe.node import node\nfrom flypipe.datasource.spark import Spark\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pyspark.sql.functions as F\n\n@node(\n    type=\"pyspark\",\n    dependencies=[\n        Spark(\"table\").select(\"_fruit\").alias(\"df\")\n    ],\n    output=Schema(\n     Column(\"fruit\", String(), \"fruit description\"),\n    )\n)\ndef rename_clean(df):\n    df = df.withColumnRenamed('_fruit', 'fruit')\n    df = df.withColumn('fruit', F.lower(F.col('fruit')))\n    return df\n</pre> from flypipe.node import node from flypipe.datasource.spark import Spark from flypipe.schema import Schema, Column from flypipe.schema.types import String import pyspark.sql.functions as F  @node(     type=\"pyspark\",     dependencies=[         Spark(\"table\").select(\"_fruit\").alias(\"df\")     ],     output=Schema(      Column(\"fruit\", String(), \"fruit description\"),     ) ) def rename_clean(df):     df = df.withColumnRenamed('_fruit', 'fruit')     df = df.withColumn('fruit', F.lower(F.col('fruit')))     return df In\u00a0[3]: Copied! <pre>rename_clean.run(spark)\n</pre> rename_clean.run(spark) Out[3]: fruit orange watermelon lemon In\u00a0[4]: Copied! <pre>@node(\n    type=\"pandas_on_spark\",\n    dependencies=[\n       rename_clean.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        rename_clean.output.get(\"fruit\"),\n        Column(\"category\", String(), \"category of the fruit\"),\n    )\n)\ndef category(df):\n\n    replacements = {\n        \"orange\": \"citric\",\n        \"watermelon\": \"sweet\",\n        \"lemon\": \"citric\",\n    }\n    \n    df['category'] = df['fruit']\n    df = df.replace({'category': replacements})\n    return df\n\n\n@node(\n    type=\"pandas_on_spark\",\n    dependencies=[\n       rename_clean.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        rename_clean.output.get(\"fruit\"),\n        Column(\"color\", String(), \"color of the fruit\"),\n    )\n)\ndef color(df):\n    replacements = {\n        \"orange\": \"orange\",\n        \"watermelon\": \"red\",\n        \"lemon\": \"yellow\",\n    }\n    df['color'] = df['fruit']\n    df = df.replace({'color': replacements})\n    return df\n</pre> @node(     type=\"pandas_on_spark\",     dependencies=[        rename_clean.select(\"fruit\").alias(\"df\")     ],     output=Schema(         rename_clean.output.get(\"fruit\"),         Column(\"category\", String(), \"category of the fruit\"),     ) ) def category(df):      replacements = {         \"orange\": \"citric\",         \"watermelon\": \"sweet\",         \"lemon\": \"citric\",     }          df['category'] = df['fruit']     df = df.replace({'category': replacements})     return df   @node(     type=\"pandas_on_spark\",     dependencies=[        rename_clean.select(\"fruit\").alias(\"df\")     ],     output=Schema(         rename_clean.output.get(\"fruit\"),         Column(\"color\", String(), \"color of the fruit\"),     ) ) def color(df):     replacements = {         \"orange\": \"orange\",         \"watermelon\": \"red\",         \"lemon\": \"yellow\",     }     df['color'] = df['fruit']     df = df.replace({'color': replacements})     return df In\u00a0[5]: Copied! <pre>color.run(spark)\n</pre> color.run(spark) Out[5]: fruit color 0 orange orange 1 watermelon red 2 lemon yellow In\u00a0[6]: Copied! <pre>@node(\n    type=\"pandas\",\n    dependencies=[\n       color.select(\"fruit\", \"color\"),\n       category.select(\"fruit\", \"category\")\n    ],\n    output=Schema(\n        color.output.get(\"fruit\"),\n        color.output.get(\"color\"),\n        category.output.get(\"category\"),\n    )\n)\ndef fruits(color, category):\n    return color.merge(category, on=\"fruit\", how=\"left\")\n</pre> @node(     type=\"pandas\",     dependencies=[        color.select(\"fruit\", \"color\"),        category.select(\"fruit\", \"category\")     ],     output=Schema(         color.output.get(\"fruit\"),         color.output.get(\"color\"),         category.output.get(\"category\"),     ) ) def fruits(color, category):     return color.merge(category, on=\"fruit\", how=\"left\") In\u00a0[7]: Copied! <pre>fruits.run(spark)\n</pre> fruits.run(spark) Out[7]: fruit color category 0 orange orange citric 1 watermelon red sweet 2 lemon yellow citric <p>The code above created a graph with 3 types of nodes</p> <ul> <li>table: <code>pyspark</code> node</li> <li>clean: <code>pandas_on_spark</code> node</li> <li>color: <code>pandas_on_spark</code> node</li> <li>category: <code>pandas</code> node</li> </ul> <p>As showed in the graph bellow</p> In\u00a0[8]: Copied! <pre>displayHTML(fruits.html())\n</pre> displayHTML(fruits.html()) Out[8]: <p>When running this graph the following processes will be executed:</p> <ol> <li>spark will query the table and select the columns specified in node <code>table</code></li> <li>before running node <code>rename_clean</code>, flypipe will convert the ouput pyspark dataframe from node <code>table</code> to a pandas_on_spark dataframe</li> <li>node <code>rename_clean</code> is processed</li> <li>nodes <code>color</code> and <code>category</code> will be processed using the output pandas_on_spark dataframe output by node <code>rename_clean</code></li> <li>once nodes <code>color</code> and <code>category</code> are processed, each dataframe is converted to <code>pandas</code> dataframe as node <code>fruits</code> is of the type pandas</li> <li>node <code>fruits</code> is processed and its dataframe output is returned</li> </ol> <p>Flypipe managages all these sequences and transformations for the user.</p> In\u00a0[9]: Copied! <pre>df = fruits.run(spark)\ndisplay(df)\n</pre> df = fruits.run(spark) display(df) fruit color category 0 orange orange citric 1 watermelon red sweet 2 lemon yellow citric In\u00a0[10]: Copied! <pre>fruit_table_df = spark.createDataFrame(data=[(\"ORANGE\",), (\"WATERMELON\",), (\"LEMON\",),], schema=[\"_fruit\"])\n\ndf = fruits.run(inputs={ Spark(\"table\"): fruit_table_df })\ndisplay(df)\n\ndisplayHTML(fruits.html(inputs={ Spark(\"table\"): fruit_table_df }))\n</pre> fruit_table_df = spark.createDataFrame(data=[(\"ORANGE\",), (\"WATERMELON\",), (\"LEMON\",),], schema=[\"_fruit\"])  df = fruits.run(inputs={ Spark(\"table\"): fruit_table_df }) display(df)  displayHTML(fruits.html(inputs={ Spark(\"table\"): fruit_table_df })) fruit color category 0 orange orange citric 1 watermelon red sweet 2 lemon yellow citric Out[10]: <p>Note: as <code>table</code> node has been provided as input, the execution of node <code>table</code> is <code>skipped</code> and the provided input is used to calculated its predecessors</p> In\u00a0[11]: Copied! <pre>import pandas as pd\nfruit_table_df = pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]})\n\ndf = fruits.run(spark, inputs={ Spark(\"table\"): fruit_table_df })\ndisplay(df)\n\ndisplayHTML(fruits.html(inputs={ Spark(\"table\"): fruit_table_df }))\n</pre> import pandas as pd fruit_table_df = pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]})  df = fruits.run(spark, inputs={ Spark(\"table\"): fruit_table_df }) display(df)  displayHTML(fruits.html(inputs={ Spark(\"table\"): fruit_table_df })) fruit color category 0 orange orange citric 1 watermelon red sweet 2 lemon yellow citric Out[11]: <p>Note: even though <code>table</code> node is of type <code>pyspark</code> and the provided input is of type Pandas, Flypipe will convert it to the appropriate node type down the graph execution</p> In\u00a0[12]: Copied! <pre>df = fruits.run(spark, pandas_on_spark_use_pandas=True)\ndisplay(df)\n\ndisplayHTML(fruits.html(pandas_on_spark_use_pandas=True))\n</pre> df = fruits.run(spark, pandas_on_spark_use_pandas=True) display(df)  displayHTML(fruits.html(pandas_on_spark_use_pandas=True)) fruit color category 0 orange orange citric 1 watermelon red sweet 2 lemon yellow citric Out[12]: In\u00a0[13]: Copied! <pre>fruit_table_df = spark.createDataFrame(pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]}))\n\ndf = fruits.run(pandas_on_spark_use_pandas=True, inputs={ Spark(\"table\"): fruit_table_df })\ndisplay(df)\n\ndisplayHTML(fruits.html(inputs={ Spark(\"table\"): fruit_table_df }, pandas_on_spark_use_pandas=True))\n</pre> fruit_table_df = spark.createDataFrame(pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]}))  df = fruits.run(pandas_on_spark_use_pandas=True, inputs={ Spark(\"table\"): fruit_table_df }) display(df)  displayHTML(fruits.html(inputs={ Spark(\"table\"): fruit_table_df }, pandas_on_spark_use_pandas=True)) fruit color category 0 orange orange citric 1 watermelon red sweet 2 lemon yellow citric Out[13]: In\u00a0[14]: Copied! <pre>rename_clean_df = pd.DataFrame(data={\"fruit\": [\"orange\", \"watermelon\", \"lemon\"]})\n\ndf = fruits.run( \n    pandas_on_spark_use_pandas=True, \n    inputs={ \n        rename_clean: rename_clean_df \n    })\n\ndisplay(df)\n\ndisplayHTML(fruits.html(inputs={ rename_clean: rename_clean_df }, pandas_on_spark_use_pandas=True))\n</pre> rename_clean_df = pd.DataFrame(data={\"fruit\": [\"orange\", \"watermelon\", \"lemon\"]})  df = fruits.run(      pandas_on_spark_use_pandas=True,      inputs={          rename_clean: rename_clean_df      })  display(df)  displayHTML(fruits.html(inputs={ rename_clean: rename_clean_df }, pandas_on_spark_use_pandas=True)) fruit color category 0 orange orange citric 1 watermelon red sweet 2 lemon yellow citric Out[14]: In\u00a0[15]: Copied! <pre>import pandas as pd\nfrom textwrap import wrap\n\nresults = pd.DataFrame(columns=['experiment', 'running_time_ms'])\n\nexperiment = \"1. Default (as implemented)\"\nprint(experiment)\nresult = %timeit -o fruits.run(spark)\nresults.loc[results.shape[0]] = [experiment, result.average]\n\nexperiment = \"2. Providing an existent `pyspark` dataframe as input\"\nprint(experiment)\nfruit_table_df = spark.createDataFrame(pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]}))\nresult = %timeit -o fruits.run(inputs={ Spark(\"table\"): fruit_table_df })\nresults.loc[results.shape[0]] = [experiment, result.average]\n\nexperiment = \"3. Providing `pandas` dataframe as input\"\nprint(experiment)\nfruit_table_df = pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]})\nresult = %timeit -o fruits.run(spark, inputs={ Spark(\"table\"): fruit_table_df })\nresults.loc[results.shape[0]] = [experiment, result.average]\n\nexperiment = \"4. `pandas_on_spark` nodes as `pandas`\"\nprint(experiment)\nresult = %timeit -o fruits.run(spark, pandas_on_spark_use_pandas=True)\nresults.loc[results.shape[0]] = [experiment, result.average]\n\nexperiment = \"5. `pandas_on_spark` nodes as `pandas` + provided `pyspark` dataframe as input\"\nprint(experiment)\nfruit_table_df = spark.createDataFrame(pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]}))\nresult = %timeit -o fruits.run(pandas_on_spark_use_pandas=True, inputs={ Spark(\"table\"): fruit_table_df })\nresults.loc[results.shape[0]] = [experiment, result.average]\n\nexperiment = \"6. `pandas_on_spark` nodes as `pandas` + provided `pandas` dataframe as input\"\nprint(experiment)\nrename_clean_df = pd.DataFrame(data={\"fruit\": [\"orange\", \"watermelon\", \"lemon\"]})\nresult = %timeit -o fruits.run(pandas_on_spark_use_pandas=True, inputs={ rename_clean: rename_clean_df })\nresults.loc[results.shape[0]] = [experiment, result.average]\n\nresults['experiment'] = results['experiment'].apply(lambda x: '\\n'.join(wrap(x, 20)))\nresults['running_time_ms'] = results['running_time_ms'] * 1000\nresults['running_time_ms'] = round(results['running_time_ms'], 1)\n</pre> import pandas as pd from textwrap import wrap  results = pd.DataFrame(columns=['experiment', 'running_time_ms'])  experiment = \"1. Default (as implemented)\" print(experiment) result = %timeit -o fruits.run(spark) results.loc[results.shape[0]] = [experiment, result.average]  experiment = \"2. Providing an existent `pyspark` dataframe as input\" print(experiment) fruit_table_df = spark.createDataFrame(pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]})) result = %timeit -o fruits.run(inputs={ Spark(\"table\"): fruit_table_df }) results.loc[results.shape[0]] = [experiment, result.average]  experiment = \"3. Providing `pandas` dataframe as input\" print(experiment) fruit_table_df = pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]}) result = %timeit -o fruits.run(spark, inputs={ Spark(\"table\"): fruit_table_df }) results.loc[results.shape[0]] = [experiment, result.average]  experiment = \"4. `pandas_on_spark` nodes as `pandas`\" print(experiment) result = %timeit -o fruits.run(spark, pandas_on_spark_use_pandas=True) results.loc[results.shape[0]] = [experiment, result.average]  experiment = \"5. `pandas_on_spark` nodes as `pandas` + provided `pyspark` dataframe as input\" print(experiment) fruit_table_df = spark.createDataFrame(pd.DataFrame(data={\"_fruit\": [\"ORANGE\", \"WATERMELON\", \"LEMON\"]})) result = %timeit -o fruits.run(pandas_on_spark_use_pandas=True, inputs={ Spark(\"table\"): fruit_table_df }) results.loc[results.shape[0]] = [experiment, result.average]  experiment = \"6. `pandas_on_spark` nodes as `pandas` + provided `pandas` dataframe as input\" print(experiment) rename_clean_df = pd.DataFrame(data={\"fruit\": [\"orange\", \"watermelon\", \"lemon\"]}) result = %timeit -o fruits.run(pandas_on_spark_use_pandas=True, inputs={ rename_clean: rename_clean_df }) results.loc[results.shape[0]] = [experiment, result.average]  results['experiment'] = results['experiment'].apply(lambda x: '\\n'.join(wrap(x, 20))) results['running_time_ms'] = results['running_time_ms'] * 1000 results['running_time_ms'] = round(results['running_time_ms'], 1) <pre>1. Default (as implemented)\n239 ms \u00b1 21 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n2. Providing an existent `pyspark` dataframe as input\n102 ms \u00b1 16.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n3. Providing `pandas` dataframe as input\n88.1 ms \u00b1 1.36 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n4. `pandas_on_spark` nodes as `pandas`\n62.3 ms \u00b1 330 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n5. `pandas_on_spark` nodes as `pandas` + provided `pyspark` dataframe as input\n16.2 ms \u00b1 199 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n6. `pandas_on_spark` nodes as `pandas` + provided `pandas` dataframe as input\n3.13 ms \u00b1 229 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(16, 4))\npal = sns.color_palette(\"Blues_d\", len(results))\nbarchart = (\n    sns.barplot(x=results['experiment'], \n                y=results['running_time_ms'], \n                palette=np.array(pal[::-1]),\n                ax=ax)\n                .set(title='Running time (ms) by different ways of running the graph')\n)\n\nfor container in ax.containers:\n    ax.bar_label(container, fmt='%.1f ms')\n</pre> import matplotlib.pyplot as plt import seaborn as sns import numpy as np  fig, ax = plt.subplots(figsize=(16, 4)) pal = sns.color_palette(\"Blues_d\", len(results)) barchart = (     sns.barplot(x=results['experiment'],                  y=results['running_time_ms'],                  palette=np.array(pal[::-1]),                 ax=ax)                 .set(title='Running time (ms) by different ways of running the graph') )  for container in ax.containers:     ax.bar_label(container, fmt='%.1f ms')"},{"location":"notebooks/usage/multiple-node-types/#multi-node-type-graphs","title":"Multi Node Type Graphs\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#adding-pyspark-node","title":"adding <code>pyspark</code> node\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#running-a-pyspark-node","title":"running a <code>pyspark</code> node\u00b6","text":"<p>It wil node require <code>spark</code> session</p>"},{"location":"notebooks/usage/multiple-node-types/#adding-pandas_on_spark-node","title":"adding <code>pandas_on_spark</code> node\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#running-a-pandas_on_spark-node","title":"running a <code>pandas_on_spark</code> node\u00b6","text":"<p>Because your graph is dependent on spark operations, it will also require a <code>spark</code> session</p>"},{"location":"notebooks/usage/multiple-node-types/#adding-pandas-node","title":"adding <code>pandas</code> node\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#running-a-pandas-node","title":"running a <code>pandas</code> node\u00b6","text":"<p>Because your graph is dependent on spark operations, it will also require a <code>spark</code> session</p>"},{"location":"notebooks/usage/multiple-node-types/#putting-all-together","title":"Putting all together\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#advantages-of-mixed-node-types-pipelines","title":"Advantages of mixed node types pipelines\u00b6","text":"<ul> <li>end-to-end unification of different teams pipelines (data -&gt; feture engineering -&gt; modelling -&gt; inference -&gt; ...) [^1]</li> <li>can lead to much faster results if running nodes <code>pandas_on_spark</code> as <code>pandas</code></li> <li>same pipeline can be used within APIs to run on the fly transformations</li> </ul> <p>[^1]: Different teams can use different syntaxes, i.e. <code>pyspark</code> is commnly used by data engineers, and <code>pandas</code> is commonly used by machine learning and data science teams</p>"},{"location":"notebooks/usage/multiple-node-types/#different-ways-of-running-a-graph","title":"Different ways of running a graph\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#1-default-as-implemented","title":"1. Default (as implemented)\u00b6","text":"<p>Tip: <code>spark</code> is not required if your graph executes only <code>pandas</code> nodes</p>"},{"location":"notebooks/usage/multiple-node-types/#2-providing-an-existent-pyspark-dataframe-as-input","title":"2. Providing an existent <code>pyspark</code> dataframe as input\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#3-providing-pandas-dataframe-as-input","title":"3. Providing <code>pandas</code> dataframe as input\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#4-pandas_on_spark-nodes-as-pandas","title":"4. <code>pandas_on_spark</code> nodes as <code>pandas</code>\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#5-pandas_on_spark-nodes-as-pandas-provided-pyspark-dataframe-as-input","title":"5. <code>pandas_on_spark</code> nodes as <code>pandas</code> + provided <code>pyspark</code> dataframe as input\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#6-pandas_on_spark-nodes-as-pandas-provided-pandas-dataframe-as-input","title":"6. <code>pandas_on_spark</code> nodes as <code>pandas</code> + provided <code>pandas</code> dataframe as input\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#benchmark","title":"Benchmark\u00b6","text":""},{"location":"notebooks/usage/multiple-node-types/#choosing-the-right-run-type","title":"Choosing the right run type\u00b6","text":"<p>These are only advices and it will dependend on the graph you are running and its purpose (if development, production big amount of data, production speed and small amount of data, etc).</p> <ol> <li>Default (as implemented)</li> </ol> <ul> <li>Running pipelines</li> <li>Big volume of data</li> </ul> <ol> <li>Providing an existent <code>pyspark</code> dataframe as input</li> </ol> <ul> <li>Debugging failed pipelines</li> <li>Last check before finishing development</li> </ul> <ol> <li>Providing <code>pandas</code> dataframe as input</li> </ol> <ul> <li>Developing pipelines</li> <li>Extract a small set of data into a Pandas dataframe will be faster to run the pipeline multiple times during development</li> </ul> <ol> <li><code>pandas_on_spark</code> nodes as <code>pandas</code></li> </ol> <ul> <li>Pipelines with small amount of data</li> </ul> <ol> <li><code>pandas_on_spark</code> nodes as <code>pandas</code> + provided <code>pyspark</code> dataframe as input</li> </ol> <ul> <li>Pipelines with small amount of data</li> </ul> <ol> <li><code>pandas_on_spark</code> nodes as <code>pandas</code> + provided <code>pandas</code> dataframe as input</li> </ol> <ul> <li>When speed is important</li> <li>Small amount of data</li> <li>On demand feature generation</li> <li>Within APIs (running pipeline with data sent in requests)</li> <li>Unit tests (will avoid use of a spark session)</li> </ul>"},{"location":"notebooks/usage/node-function/","title":"Node Function Examples","text":"In\u00a0[1]: Copied! <pre>df = spark.createDataFrame(\n    data=[\n        (\"LEMON\", \"Yellow\",), \n        (\"LIME\", \"Green\",)\n    ], schema=[\"fruit\", \"color\"])\n\ndf.createOrReplaceTempView(\"fruits_table\")\n\ndisplay(df)\n</pre> df = spark.createDataFrame(     data=[         (\"LEMON\", \"Yellow\",),          (\"LIME\", \"Green\",)     ], schema=[\"fruit\", \"color\"])  df.createOrReplaceTempView(\"fruits_table\")  display(df) fruitcolor LEMONYellow LIMEGreen In\u00a0[2]: Copied! <pre>from flypipe import node\nfrom flypipe import node_function\nfrom flypipe.datasource.spark import Spark\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pyspark.sql.functions as F\n\n@node_function(\n    requested_columns=True,\n    node_dependencies=[\n        Spark(\"fruits_table\")\n    ]\n)\ndef fruits_function(requested_columns):\n    \n    print(f\"Raw columns queried: {requested_columns}\")\n    \n    @node(\n        type=\"pyspark\",\n        dependencies=[\n            Spark(\"fruits_table\").select(requested_columns)\n        ],\n        output=Schema([\n         Column(col, String(), col) for col in requested_columns   \n        ])\n    )\n    def lower(fruits_table):\n        for col in requested_columns:\n            print(f\"lower case column `{col}`\")\n            fruits_table = fruits_table.withColumn(col, F.lower(col))\n        \n        return fruits_table\n    \n    return lower\n</pre> from flypipe import node from flypipe import node_function from flypipe.datasource.spark import Spark from flypipe.schema import Schema, Column from flypipe.schema.types import String import pyspark.sql.functions as F  @node_function(     requested_columns=True,     node_dependencies=[         Spark(\"fruits_table\")     ] ) def fruits_function(requested_columns):          print(f\"Raw columns queried: {requested_columns}\")          @node(         type=\"pyspark\",         dependencies=[             Spark(\"fruits_table\").select(requested_columns)         ],         output=Schema([          Column(col, String(), col) for col in requested_columns            ])     )     def lower(fruits_table):         for col in requested_columns:             print(f\"lower case column `{col}`\")             fruits_table = fruits_table.withColumn(col, F.lower(col))                  return fruits_table          return lower In\u00a0[3]: Copied! <pre>@node(\n    type=\"pyspark\",\n    dependencies=[fruits_function.select(\"fruit\")]\n)\ndef my_fruits(fruits_function):\n    return fruits_function\n\ndf = my_fruits.run(spark)\ndisplay(df)              \n\ndisplayHTML(my_fruits.html())\n</pre> @node(     type=\"pyspark\",     dependencies=[fruits_function.select(\"fruit\")] ) def my_fruits(fruits_function):     return fruits_function  df = my_fruits.run(spark) display(df)                displayHTML(my_fruits.html()) <pre>Raw columns queried: ['fruit']\nlower case column `fruit`\n</pre> fruit lemon lime <pre>Raw columns queried: ['fruit']\n</pre> Out[3]: In\u00a0[4]: Copied! <pre>@node(\n    type=\"pyspark\",\n    dependencies=[fruits_function.select(\"fruit\",  \"color\")]\n)\ndef my_fruits(fruits_function):\n    return fruits_function\n\ndf = my_fruits.run(spark)\ndisplay(df)   \n           \ndisplayHTML(my_fruits.html())\n</pre> @node(     type=\"pyspark\",     dependencies=[fruits_function.select(\"fruit\",  \"color\")] ) def my_fruits(fruits_function):     return fruits_function  df = my_fruits.run(spark) display(df)                displayHTML(my_fruits.html()) <pre>Raw columns queried: ['color', 'fruit']\nlower case column `color`\nlower case column `fruit`\n</pre> colorfruit yellowlemon greenlime <pre>Raw columns queried: ['color', 'fruit']\n</pre> Out[4]: In\u00a0[5]: Copied! <pre>from flypipe import node\nfrom flypipe import node_function\nfrom flypipe.datasource.spark import Spark\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pyspark.sql.functions as F\n\n@node_function(\n    requested_columns=True,\n    node_dependencies=[\n        Spark(\"fruits_table\")\n    ]\n)\ndef fruits_function_expanded(requested_columns):\n    \n    print(f\"Raw columns queried: {requested_columns}\")\n    datasource = Spark(\"fruits_table\").select(requested_columns)\n    \n    last_transformation = datasource\n    for col in requested_columns:\n        \n        @node(\n            type=\"pyspark\",\n            dependencies=[\n                datasource.alias(\"df\")\n            ],\n            output=Schema([\n             Column(col, String(), col) for col in requested_columns   \n            ])\n        )\n        def lower_fn(df):\n            print(f\"lower case column `{col}`\")\n            df = df.withColumn(col, F.lower(col))\n            return df\n        \n        lower_fn.function.__name__ = f\"lower_{col}\"\n        last_transformation = lower_fn\n    \n    return last_transformation\n</pre> from flypipe import node from flypipe import node_function from flypipe.datasource.spark import Spark from flypipe.schema import Schema, Column from flypipe.schema.types import String import pyspark.sql.functions as F  @node_function(     requested_columns=True,     node_dependencies=[         Spark(\"fruits_table\")     ] ) def fruits_function_expanded(requested_columns):          print(f\"Raw columns queried: {requested_columns}\")     datasource = Spark(\"fruits_table\").select(requested_columns)          last_transformation = datasource     for col in requested_columns:                  @node(             type=\"pyspark\",             dependencies=[                 datasource.alias(\"df\")             ],             output=Schema([              Column(col, String(), col) for col in requested_columns                ])         )         def lower_fn(df):             print(f\"lower case column `{col}`\")             df = df.withColumn(col, F.lower(col))             return df                  lower_fn.function.__name__ = f\"lower_{col}\"         last_transformation = lower_fn          return last_transformation In\u00a0[6]: Copied! <pre>@node(\n    type=\"pyspark\",\n    dependencies=[fruits_function_expanded.select(\"fruit\",  \"color\").alias(\"df\")]\n)\ndef my_fruits_expanded(df):\n    return df\n\ndf = my_fruits_expanded.run(spark)\ndisplay(df)   \n           \ndisplayHTML(my_fruits_expanded.html())        \n</pre> @node(     type=\"pyspark\",     dependencies=[fruits_function_expanded.select(\"fruit\",  \"color\").alias(\"df\")] ) def my_fruits_expanded(df):     return df  df = my_fruits_expanded.run(spark) display(df)                displayHTML(my_fruits_expanded.html())         <pre>Raw columns queried: ['color', 'fruit']\nlower case column `fruit`\n</pre> colorfruit Yellowlemon Greenlime <pre>Raw columns queried: ['color', 'fruit']\n</pre> Out[6]:"},{"location":"notebooks/usage/node-function/#node-function-examples","title":"Node Function Examples\u00b6","text":"<p>A node function allows you to create nodes programmatically.</p> <p>It is useful when you need nodes to behave in different ways, depending on specific conditions.</p>"},{"location":"notebooks/usage/node-function/#dynamic-transformations","title":"Dynamic transformations\u00b6","text":"<p>Suppose you want to lower case the columns of the dataframe above, however this dataframe can contain hundreds of columns, therefore, you only want to apply transformations on columns requested by the graph.</p> <p>In this example, if a node is importing this dataframe and selecting only column <code>fruit</code>, then only the raw column <code>fruit</code> should be queried and applied lower case transformation.</p>"},{"location":"notebooks/usage/node-function/#selecting-fruit","title":"Selecting <code>fruit</code>\u00b6","text":""},{"location":"notebooks/usage/node-function/#selecting-fruit-and-flavour","title":"Selecting <code>fruit</code> and <code>flavour</code>\u00b6","text":""},{"location":"notebooks/usage/node-function/#expanded-graph","title":"Expanded graph\u00b6","text":"<p>Observe that the graph for both operations did not change, but the number of transformations inside node fruits changed.</p> <p>We can make use of node functions to expand this operations</p>"},{"location":"notebooks/usage/node-function/#selecting-fruit-and-flavour","title":"Selecting <code>fruit</code> and <code>flavour</code>\u00b6","text":""},{"location":"notebooks/usage/node-parameters/","title":"Node Parameters","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nfrom flypipe.node import node\n\n@node(type=\"pandas\")\ndef t1(param1):\n    print(f\"param1: `{param1}`\")\n    return pd.DataFrame()\n\n@node(\n    type=\"pandas\",\n    dependencies=[t1]\n)\ndef t2(t1):\n    return t1\n    \ndf = t2.run(parameters={t1: {\"param1\": \"hello\"}})\n</pre> import pandas as pd from flypipe.node import node  @node(type=\"pandas\") def t1(param1):     print(f\"param1: `{param1}`\")     return pd.DataFrame()  @node(     type=\"pandas\",     dependencies=[t1] ) def t2(t1):     return t1      df = t2.run(parameters={t1: {\"param1\": \"hello\"}}) <pre>param1: `hello`\n</pre> <p>Note just like any python function argument we can give a default value to use if no value is provided at runtime.</p> In\u00a0[2]: Copied! <pre>import pandas as pd\nfrom flypipe.node import node\n\n@node(type=\"pandas\")\ndef t1(param1=None):\n    print(f\"param1: `{param1}`\")\n    return pd.DataFrame()\n    \ndf = t1()\n</pre> import pandas as pd from flypipe.node import node  @node(type=\"pandas\") def t1(param1=None):     print(f\"param1: `{param1}`\")     return pd.DataFrame()      df = t1() <pre>param1: `None`\n</pre> In\u00a0[3]: Copied! <pre>from flypipe import node_function\n\n@node_function()\ndef t1_func(node_selector=\"t1\"):\n    \n    @node(type=\"pandas\")\n    def t1():\n        return pd.DataFrame()\n    \n    if node_selector == \"t1\":\n        return t1\n    \n    @node(\n        type=\"pandas\",\n        dependencies=[t1]\n    )\n    def t2():\n        return pd.DataFrame()\n    \n    return t2, t1\n\ndisplayHTML(t1_func.html())    \n</pre> from flypipe import node_function  @node_function() def t1_func(node_selector=\"t1\"):          @node(type=\"pandas\")     def t1():         return pd.DataFrame()          if node_selector == \"t1\":         return t1          @node(         type=\"pandas\",         dependencies=[t1]     )     def t2():         return pd.DataFrame()          return t2, t1  displayHTML(t1_func.html())     Out[3]: In\u00a0[4]: Copied! <pre>displayHTML(\n    t1_func.html(\n        parameters={\n            t1_func: {'node_selector': 't2'}\n        }\n    ))\n</pre>  displayHTML(     t1_func.html(         parameters={             t1_func: {'node_selector': 't2'}         }     )) Out[4]: <p>train a model with different splits of data (train, test and validation)</p> In\u00a0[5]: Copied! <pre>from flypipe.node import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import Date\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n@node(\n    type=\"pandas\",\n    output=Schema([\n        Column(\"date\", Date(), \"date\"),\n    ])\n)\ndef my_model_features():\n    \n    date = datetime(2022,1,1)\n    dates = [date + timedelta(days=day) for day in range(11)]\n    \n    return pd.DataFrame(data={\"date\": dates})\n\n@node(\n    type=\"pandas\",\n    dependencies=[my_model_features]\n    \n)\ndef tag_data(my_model_features, split={'train': datetime(2022,1,4), \"test\": datetime(2022,1,7)}):\n    print(f\"\"\"\n    Split data\n    ----------\n    train: date &lt;= {split['train'].date()}\n    test: {split['train'].date()} &lt; date &lt;= {split['test'].date()}\n    val: {split['test'].date()} &lt; date\n    \"\"\")\n    \n    my_model_features[\"tag\"] = \"val\"\n    my_model_features.loc[my_model_features['date'] &lt;= split['test'], \"tag\"] = \"test\"\n    my_model_features.loc[my_model_features['date'] &lt;= split['train'], \"tag\"] = \"train\"    \n    \n    return my_model_features\n\n@node(\n    type=\"pandas\",\n    dependencies=[tag_data]\n    \n)\ndef my_model(tag_data):    \n    return tag_data\n\ndf = my_model.run()\ndisplay(df)\n</pre> from flypipe.node import node from flypipe.schema import Schema, Column from flypipe.schema.types import Date import pandas as pd from datetime import datetime, timedelta  @node(     type=\"pandas\",     output=Schema([         Column(\"date\", Date(), \"date\"),     ]) ) def my_model_features():          date = datetime(2022,1,1)     dates = [date + timedelta(days=day) for day in range(11)]          return pd.DataFrame(data={\"date\": dates})  @node(     type=\"pandas\",     dependencies=[my_model_features]      ) def tag_data(my_model_features, split={'train': datetime(2022,1,4), \"test\": datetime(2022,1,7)}):     print(f\"\"\"     Split data     ----------     train: date &lt;= {split['train'].date()}     test: {split['train'].date()} &lt; date &lt;= {split['test'].date()}     val: {split['test'].date()} &lt; date     \"\"\")          my_model_features[\"tag\"] = \"val\"     my_model_features.loc[my_model_features['date'] &lt;= split['test'], \"tag\"] = \"test\"     my_model_features.loc[my_model_features['date'] &lt;= split['train'], \"tag\"] = \"train\"              return my_model_features  @node(     type=\"pandas\",     dependencies=[tag_data]      ) def my_model(tag_data):         return tag_data  df = my_model.run() display(df) <pre>\n    Split data\n    ----------\n    train: date &lt;= 2022-01-04\n    test: 2022-01-04 &lt; date &lt;= 2022-01-07\n    val: 2022-01-07 &lt; date\n    \n</pre> date tag 0 2022-01-01 train 1 2022-01-02 train 2 2022-01-03 train 3 2022-01-04 train 4 2022-01-05 test 5 2022-01-06 test 6 2022-01-07 test 7 2022-01-08 val 8 2022-01-09 val 9 2022-01-10 val 10 2022-01-11 val <p>running with different splits</p> In\u00a0[6]: Copied! <pre>df = my_model.run(parameters={\n     tag_data: {\n         'split': {'train': datetime(2022,1,2), \"test\": datetime(2022,1,4)}\n     }\n})\ndisplay(df)\n</pre> df = my_model.run(parameters={      tag_data: {          'split': {'train': datetime(2022,1,2), \"test\": datetime(2022,1,4)}      } }) display(df) <pre>\n    Split data\n    ----------\n    train: date &lt;= 2022-01-02\n    test: 2022-01-02 &lt; date &lt;= 2022-01-04\n    val: 2022-01-04 &lt; date\n    \n</pre> date tag 0 2022-01-01 train 1 2022-01-02 train 2 2022-01-03 test 3 2022-01-04 test 4 2022-01-05 val 5 2022-01-06 val 6 2022-01-07 val 7 2022-01-08 val 8 2022-01-09 val 9 2022-01-10 val 10 2022-01-11 val"},{"location":"notebooks/usage/node-parameters/#node-parameters","title":"Node Parameters\u00b6","text":"<p>Up to this point we've dealt with defining nodes and composing them together into pipelines. One issue is that all of the logic is static, the pipelines are unable to changed by external sources. Node Parameters are the solution to this- they comprise named arguments of a node that are defined at the point where the pipeline is run.</p>"},{"location":"notebooks/usage/node-parameters/#definition","title":"Definition\u00b6","text":""},{"location":"notebooks/usage/node-parameters/#parameters-and-node-functions","title":"Parameters and node functions\u00b6","text":"<p>Parameters can also be used with node functions, together you have more flexibility on dynamically changing not only node behaviours but also graph compositions</p>"},{"location":"notebooks/usage/node-parameters/#changing-parameter-node_selector","title":"Changing parameter <code>node_selector</code>\u00b6","text":""},{"location":"notebooks/usage/node-parameters/#suggested-use-cases-for-node-parameters","title":"Suggested use cases for node parameters\u00b6","text":"<ul> <li>you need the node to behave differently accordingly to some settings</li> <li>train a model inside a node with changeable splits of data between train, test and validation</li> <li>run nodes that execute customisable versions of an mlflow model by encoding the model run id into a node parameter</li> </ul>"},{"location":"notebooks/usage/node/","title":"Examples","text":"In\u00a0[1]: Copied! <pre>from flypipe.node import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pandas as pd\n@node(\n    type=\"pandas\",\n    description=\"Outputs a dataframe with fruit names\",\n    tags=[\"fruit\"],\n    output=Schema([\n        Column(\"fruit\", String(), \"name of the fruit\"),\n    ])\n)\ndef fruits():\n    return pd.DataFrame(data={\"fruit\": [\"mango\", \"lemon\"]})\n\nfruits.run()\n</pre> from flypipe.node import node from flypipe.schema import Schema, Column from flypipe.schema.types import String import pandas as pd @node(     type=\"pandas\",     description=\"Outputs a dataframe with fruit names\",     tags=[\"fruit\"],     output=Schema([         Column(\"fruit\", String(), \"name of the fruit\"),     ]) ) def fruits():     return pd.DataFrame(data={\"fruit\": [\"mango\", \"lemon\"]})  fruits.run()  Out[1]: fruit 0 mango 1 lemon In\u00a0[2]: Copied! <pre>from flypipe.node import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pandas as pd\n\n@node(\n    type=\"pandas_on_spark\",\n    description=\"Only outputs a pandas dataframe\",\n    tags=[\"flavour\", \"pandas_on_spark\"],\n    dependencies = [\n        fruits.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        fruits.output.get(\"fruit\"),\n        Column(\"flavour\", String(), \"fruit flavour\")\n    )\n)\ndef flavour_pandas_spark(df):\n    flavours = {'mango': 'sweet', 'lemon': 'citric'}\n    df['flavour'] = df['fruit']\n    df = df.replace({'flavour': flavours})\n    return df\n\nflavour_pandas_spark.run(spark)\n</pre> from flypipe.node import node from flypipe.schema import Schema, Column from flypipe.schema.types import String import pandas as pd  @node(     type=\"pandas_on_spark\",     description=\"Only outputs a pandas dataframe\",     tags=[\"flavour\", \"pandas_on_spark\"],     dependencies = [         fruits.select(\"fruit\").alias(\"df\")     ],     output=Schema(         fruits.output.get(\"fruit\"),         Column(\"flavour\", String(), \"fruit flavour\")     ) ) def flavour_pandas_spark(df):     flavours = {'mango': 'sweet', 'lemon': 'citric'}     df['flavour'] = df['fruit']     df = df.replace({'flavour': flavours})     return df  flavour_pandas_spark.run(spark) Out[2]: fruit flavour 0 mango sweet 1 lemon citric In\u00a0[3]: Copied! <pre>from flypipe.node import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pyspark.sql.functions as F\n\n@node(\n    type=\"pyspark\",\n    description=\"Only outputs a pandas dataframe\",\n    tags=[\"flavour\", \"pyspark\"],\n    dependencies = [\n        fruits.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        fruits.output.get(\"fruit\"),\n        Column(\"flavour\", String(), \"fruit flavour\")\n    )\n)\ndef flavour_pyspark(df):\n    fruits = ['mango', 'lemon']\n    flavours = ['sweet', 'citric']\n    \n    df = df.withColumn('flavour', F.col('fruit'))\n    df = df.replace(fruits, flavours, 'flavour')\n    return df\n\nflavour_pyspark.run(spark)\n</pre> from flypipe.node import node from flypipe.schema import Schema, Column from flypipe.schema.types import String import pyspark.sql.functions as F  @node(     type=\"pyspark\",     description=\"Only outputs a pandas dataframe\",     tags=[\"flavour\", \"pyspark\"],     dependencies = [         fruits.select(\"fruit\").alias(\"df\")     ],     output=Schema(         fruits.output.get(\"fruit\"),         Column(\"flavour\", String(), \"fruit flavour\")     ) ) def flavour_pyspark(df):     fruits = ['mango', 'lemon']     flavours = ['sweet', 'citric']          df = df.withColumn('flavour', F.col('fruit'))     df = df.replace(fruits, flavours, 'flavour')     return df  flavour_pyspark.run(spark) Out[3]: fruitflavour mangosweet lemoncitric In\u00a0[4]: Copied! <pre>from flypipe.node import node\n\n@node(\n    type='spark_sql',\n    dependencies=[fruits.select('fruit')]\n)\ndef flavour_spark_sql(fruits):\n    return f\"\"\"\n    SELECT fruit, CASE WHEN fruit='mango' THEN 'sweet' ELSE 'citric' END as flavour from {fruits}\n    \"\"\"\n\nflavour_spark_sql.run(spark=spark)\n</pre> from flypipe.node import node  @node(     type='spark_sql',     dependencies=[fruits.select('fruit')] ) def flavour_spark_sql(fruits):     return f\"\"\"     SELECT fruit, CASE WHEN fruit='mango' THEN 'sweet' ELSE 'citric' END as flavour from {fruits}     \"\"\"  flavour_spark_sql.run(spark=spark) Out[4]: fruitflavour mangosweet lemoncitric In\u00a0[5]: Copied! <pre>from flypipe.node import node\nfrom flypipe.schema import Schema, Column\nfrom flypipe.schema.types import String\nimport pandas as pd\n\n@node(\n    type=\"pandas\",\n    description=\"Only outputs a pandas dataframe\",\n    tags=[\"flavour\", \"pandas\"],\n    dependencies = [\n        fruits.select(\"fruit\").alias(\"df\")\n    ],\n    output=Schema(\n        fruits.output.get(\"fruit\"),\n        Column(\"flavour\", String(), \"fruit flavour\")\n    )\n)\ndef flavour_pandas(df):\n    flavours = {'mango': 'sweet', 'lemon': 'citric'}\n    df['flavour'] = df['fruit']\n    df = df.replace({'flavour': flavours})\n    return df\n\nflavour_pandas.run()\n</pre> from flypipe.node import node from flypipe.schema import Schema, Column from flypipe.schema.types import String import pandas as pd  @node(     type=\"pandas\",     description=\"Only outputs a pandas dataframe\",     tags=[\"flavour\", \"pandas\"],     dependencies = [         fruits.select(\"fruit\").alias(\"df\")     ],     output=Schema(         fruits.output.get(\"fruit\"),         Column(\"flavour\", String(), \"fruit flavour\")     ) ) def flavour_pandas(df):     flavours = {'mango': 'sweet', 'lemon': 'citric'}     df['flavour'] = df['fruit']     df = df.replace({'flavour': flavours})     return df  flavour_pandas.run() Out[5]: fruit flavour 0 mango sweet 1 lemon citric"},{"location":"notebooks/usage/node/#examples","title":"Examples\u00b6","text":""},{"location":"notebooks/usage/node/#pandas-node","title":"<code>pandas</code> node\u00b6","text":""},{"location":"notebooks/usage/node/#pandas_on_spark-node","title":"<code>pandas_on_spark</code> node\u00b6","text":""},{"location":"notebooks/usage/node/#pyspark-node","title":"<code>pyspark</code> node\u00b6","text":""},{"location":"notebooks/usage/node/#spark_sql-node","title":"<code>spark_sql</code> node\u00b6","text":""},{"location":"notebooks/usage/node/#dependencies","title":"dependencies\u00b6","text":""},{"location":"notebooks/usage/preprocess/","title":"Working Example","text":"In\u00a0[1]: Copied! <pre>from flypipe import node\nimport pandas as pd\nfrom datetime import datetime\n\n@node(type=\"pandas\")\ndef raw_sales():\n    return pd.DataFrame(data={\n        \"product\": [\"apple\", \"banana\", \"orange\"], \n        \"price\": [5.33, 1.2, 7.5],\n        \"datetime_sale\": [datetime(2025, 1, 1, 10, 55, 32), datetime(2025, 1, 3, 13, 15, 22), datetime(2025, 1, 4, 1, 5, 1)]\n    })\ndf = raw_sales.run()\ndisplay(df)\n</pre> from flypipe import node import pandas as pd from datetime import datetime  @node(type=\"pandas\") def raw_sales():     return pd.DataFrame(data={         \"product\": [\"apple\", \"banana\", \"orange\"],          \"price\": [5.33, 1.2, 7.5],         \"datetime_sale\": [datetime(2025, 1, 1, 10, 55, 32), datetime(2025, 1, 3, 13, 15, 22), datetime(2025, 1, 4, 1, 5, 1)]     }) df = raw_sales.run() display(df) product price datetime_sale 0 apple 5.33 2025-01-01 10:55:32 1 banana 1.20 2025-01-03 13:15:22 2 orange 7.50 2025-01-04 01:05:01 Preprocess function In\u00a0[2]: Copied! <pre>def cdc_changes(df):\n    sales_from_datetime = datetime(2025, 1, 3, 0, 0, 0)\n    print(f\"==&gt; Getting cdc_changes from {sales_from_datetime}\")\n    return df[df['datetime_sale'] &gt;= sales_from_datetime]\n</pre> def cdc_changes(df):     sales_from_datetime = datetime(2025, 1, 3, 0, 0, 0)     print(f\"==&gt; Getting cdc_changes from {sales_from_datetime}\")     return df[df['datetime_sale'] &gt;= sales_from_datetime] In\u00a0[3]: Copied! <pre>@node(\n    type=\"pandas\",\n    dependencies=[\n        raw_sales.preprocess(cdc_changes).alias(\"df_raw\")\n    ]\n)\ndef sales(df_raw):\n    return df_raw\n    \ndf = sales.run()\ndisplay(df)\n</pre> @node(     type=\"pandas\",     dependencies=[         raw_sales.preprocess(cdc_changes).alias(\"df_raw\")     ] ) def sales(df_raw):     return df_raw      df = sales.run() display(df) <pre>==&gt; Getting cdc_changes from 2025-01-03 00:00:00\n</pre> product price datetime_sale 1 banana 1.2 2025-01-03 13:15:22 2 orange 7.5 2025-01-04 01:05:01 In\u00a0[4]: Copied! <pre>from flypipe.mode import PreprocessMode\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n        raw_sales.preprocess(cdc_changes).alias(\"df_raw\")\n    ]\n)\ndef sales(df_raw):\n    return df_raw\n    \ndf = sales.run(preprocess=PreprocessMode.DISABLE)\ndisplay(df)\n</pre> from flypipe.mode import PreprocessMode  @node(     type=\"pandas\",     dependencies=[         raw_sales.preprocess(cdc_changes).alias(\"df_raw\")     ] ) def sales(df_raw):     return df_raw      df = sales.run(preprocess=PreprocessMode.DISABLE) display(df) product price datetime_sale 0 apple 5.33 2025-01-01 10:55:32 1 banana 1.20 2025-01-03 13:15:22 2 orange 7.50 2025-01-04 01:05:01 In\u00a0[5]: Copied! <pre>from flypipe.mode import PreprocessMode\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n        raw_sales.preprocess(cdc_changes).alias(\"df_raw\")\n    ]\n)\ndef sales(df_raw):\n    return df_raw\n    \ndf = sales.run(preprocess={    \n    # node: {node_dependency: PreprocessMode.DISABLE}\n    sales: {raw_sales: PreprocessMode.DISABLE}\n})\ndisplay(df)\n</pre> from flypipe.mode import PreprocessMode  @node(     type=\"pandas\",     dependencies=[         raw_sales.preprocess(cdc_changes).alias(\"df_raw\")     ] ) def sales(df_raw):     return df_raw      df = sales.run(preprocess={         # node: {node_dependency: PreprocessMode.DISABLE}     sales: {raw_sales: PreprocessMode.DISABLE} }) display(df) product price datetime_sale 0 apple 5.33 2025-01-01 10:55:32 1 banana 1.20 2025-01-03 13:15:22 2 orange 7.50 2025-01-04 01:05:01 In\u00a0[6]: Copied! <pre>import os\nfrom flypipe.config import config_context\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n        raw_sales.alias(\"df_raw\")\n    ]\n)\ndef other_sales(df_raw):\n    return df_raw\n\n\n# with context was used here only to show how global processes work, in production use environment variables\nwith config_context(\n    default_dependencies_preprocess_module=\"preprocess_function\",\n    default_dependencies_preprocess_function=\"global_preprocess\"\n):\n    df = other_sales.run()\n    display(df)\n</pre> import os from flypipe.config import config_context  @node(     type=\"pandas\",     dependencies=[         raw_sales.alias(\"df_raw\")     ] ) def other_sales(df_raw):     return df_raw   # with context was used here only to show how global processes work, in production use environment variables with config_context(     default_dependencies_preprocess_module=\"preprocess_function\",     default_dependencies_preprocess_function=\"global_preprocess\" ):     df = other_sales.run()     display(df) <pre>==&gt; Global Preprocess\n</pre> product price datetime_sale 1 banana 1.2 2025-01-03 13:15:22 2 orange 7.5 2025-01-04 01:05:01 <p>as you can see bellow, <code>flypipe</code> still uses <code>cdc_function</code> to preprocess the dependency of <code>sales</code> node</p> In\u00a0[7]: Copied! <pre>import os\nfrom flypipe.config import config_context\n\n@node(\n    type=\"pandas\",\n    dependencies=[\n        raw_sales.alias(\"df_raw\")\n    ]\n)\ndef other_sales(df_raw):\n    return df_raw\n\n\n# with context was used here only to show how global processes work, in production use environment variables\nwith config_context(\n    default_dependencies_preprocess_module=\"preprocess_function\",\n    default_dependencies_preprocess_function=\"global_preprocess\"\n):\n    df = sales.run()\n    display(df)\n</pre> import os from flypipe.config import config_context  @node(     type=\"pandas\",     dependencies=[         raw_sales.alias(\"df_raw\")     ] ) def other_sales(df_raw):     return df_raw   # with context was used here only to show how global processes work, in production use environment variables with config_context(     default_dependencies_preprocess_module=\"preprocess_function\",     default_dependencies_preprocess_function=\"global_preprocess\" ):     df = sales.run()     display(df) <pre>==&gt; Getting cdc_changes from 2025-01-03 00:00:00\n</pre> product price datetime_sale 1 banana 1.2 2025-01-03 13:15:22 2 orange 7.5 2025-01-04 01:05:01 In\u00a0[8]: Copied! <pre>def preprocess_1(df):\n    datetime_sales = datetime(2025, 1, 3, 0, 0, 0)\n    print(f\"==&gt; Applying preprocess_1 (filter datime_sale from `{datetime_sales}`)\")\n    return df[df['datetime_sale'] &gt;= datetime_sales]\n\ndef preprocess_2(df):\n    datetime_sales = datetime(2025, 1, 4, 0, 0, 0)\n    print(f\"==&gt; Applying preprocess_2 (filter datime_sale from `{datetime_sales}`)\")\n    return df[df['datetime_sale'] &gt;= datetime_sales]\n</pre> def preprocess_1(df):     datetime_sales = datetime(2025, 1, 3, 0, 0, 0)     print(f\"==&gt; Applying preprocess_1 (filter datime_sale from `{datetime_sales}`)\")     return df[df['datetime_sale'] &gt;= datetime_sales]  def preprocess_2(df):     datetime_sales = datetime(2025, 1, 4, 0, 0, 0)     print(f\"==&gt; Applying preprocess_2 (filter datime_sale from `{datetime_sales}`)\")     return df[df['datetime_sale'] &gt;= datetime_sales] In\u00a0[9]: Copied! <pre>@node(\n    type=\"pandas\",\n    dependencies=[\n        raw_sales.preprocess(preprocess_1, preprocess_2).alias(\"df_raw\")\n    ]\n)\ndef chaining(df_raw):\n    return df_raw\n    \ndf = chaining.run()\ndisplay(df)\n</pre> @node(     type=\"pandas\",     dependencies=[         raw_sales.preprocess(preprocess_1, preprocess_2).alias(\"df_raw\")     ] ) def chaining(df_raw):     return df_raw      df = chaining.run() display(df) <pre>==&gt; Applying preprocess_1 (filter datime_sale from `2025-01-03 00:00:00`)\n==&gt; Applying preprocess_2 (filter datime_sale from `2025-01-04 00:00:00`)\n</pre> product price datetime_sale 2 orange 7.5 2025-01-04 01:05:01 <p>reverting the order fo the preprocess functions, reverts the callings</p> In\u00a0[10]: Copied! <pre>@node(\n    type=\"pandas\",\n    dependencies=[\n        raw_sales.preprocess(preprocess_2, preprocess_1).alias(\"df_raw\")\n    ]\n)\ndef chaining(df_raw):\n    return df_raw\n    \ndf = chaining.run()\ndisplay(df)\n</pre> @node(     type=\"pandas\",     dependencies=[         raw_sales.preprocess(preprocess_2, preprocess_1).alias(\"df_raw\")     ] ) def chaining(df_raw):     return df_raw      df = chaining.run() display(df) <pre>==&gt; Applying preprocess_2 (filter datime_sale from `2025-01-04 00:00:00`)\n==&gt; Applying preprocess_1 (filter datime_sale from `2025-01-03 00:00:00`)\n</pre> product price datetime_sale 2 orange 7.5 2025-01-04 01:05:01"},{"location":"notebooks/usage/preprocess/#disabling-preprocessing","title":"Disabling preprocessing\u00b6","text":""},{"location":"notebooks/usage/preprocess/#all-nodes-dependencies","title":"All nodes dependencies\u00b6","text":""},{"location":"notebooks/usage/preprocess/#specific-node-dependencies","title":"Specific node dependencies\u00b6","text":""},{"location":"notebooks/usage/preprocess/#enable-preprocess-for-all-dependencies-by-default","title":"Enable preprocess for all dependencies by default\u00b6","text":""},{"location":"notebooks/usage/preprocess/#chaining-preprocessing-functions","title":"Chaining preprocessing functions\u00b6","text":""},{"location":"notebooks/usage/preprocess_function/","title":"Preprocess function","text":"In\u00a0[\u00a0]: Copied! <pre>from datetime import datetime\n</pre> from datetime import datetime In\u00a0[\u00a0]: Copied! <pre>def global_preprocess(df):\n    sales_from_datetime = datetime(2025, 1, 3, 0, 0, 0)\n    print(f\"==&gt; Global Preprocess\")\n    return df[df['datetime_sale'] &gt;= sales_from_datetime]\n</pre> def global_preprocess(df):     sales_from_datetime = datetime(2025, 1, 3, 0, 0, 0)     print(f\"==&gt; Global Preprocess\")     return df[df['datetime_sale'] &gt;= sales_from_datetime]"},{"location":"notebooks/usage/sparkleframe/","title":"Low-Latency Transformations with Sparkleframe","text":"In\u00a0[1]: Copied! <pre>from time import time\nfrom flypipe import node\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder.getOrCreate()\n\n@node(\n    type=\"pyspark\",\n    spark_context=True,\n)\ndef my_node(spark):\n    return spark.createDataFrame([{\"a\": 1}])\n\n@node(\n    type=\"pyspark\",\n    dependencies=[my_node.alias(\"df\")]\n)\ndef add_10(df):\n    return df.withColumn(\"a\", col(\"a\") + 10)\n\nstart = time()\ndf = add_10.run(spark)\nprint(df.toPandas())\nprint(f\"\\nType of dataframe returned {type(df)}\")\nprint(f\"===&gt; Time taken: {round((time() - start)*1000, 2)} milliseconds\")\n</pre> from time import time from flypipe import node from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.getOrCreate()  @node(     type=\"pyspark\",     spark_context=True, ) def my_node(spark):     return spark.createDataFrame([{\"a\": 1}])  @node(     type=\"pyspark\",     dependencies=[my_node.alias(\"df\")] ) def add_10(df):     return df.withColumn(\"a\", col(\"a\") + 10)  start = time() df = add_10.run(spark) print(df.toPandas()) print(f\"\\nType of dataframe returned {type(df)}\") print(f\"===&gt; Time taken: {round((time() - start)*1000, 2)} milliseconds\") <pre>    a\n0  11\n\nType of dataframe returned &lt;class 'pyspark.sql.dataframe.DataFrame'&gt;\n===&gt; Time taken: 1909.74 milliseconds\n</pre> In\u00a0[2]: Copied! <pre>from sparkleframe.activate import activate\nactivate()\n</pre> from sparkleframe.activate import activate activate() <p>Run the same code again, and the transformation will execute using Polars instead of Spark:</p> In\u00a0[3]: Copied! <pre>from time import time\nfrom flypipe import node\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder.getOrCreate()\n\n@node(\n    type=\"pyspark\",\n    spark_context=True,\n)\ndef my_node(spark):\n    return spark.createDataFrame([{\"a\": 1}])\n\n@node(\n    type=\"pyspark\",\n    dependencies=[my_node.alias(\"df\")]\n)\ndef add_10(df):\n    return df.withColumn(\"a\", col(\"a\") + 10)\n\nstart = time()\ndf = add_10.run(spark)\nprint(df.toPandas())\nprint(f\"\\nType of dataframe returned {type(df)}\")\nprint(f\"===&gt; Time taken: {round((time() - start)*1000, 2)} milliseconds\")\n</pre> from time import time from flypipe import node from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.getOrCreate()  @node(     type=\"pyspark\",     spark_context=True, ) def my_node(spark):     return spark.createDataFrame([{\"a\": 1}])  @node(     type=\"pyspark\",     dependencies=[my_node.alias(\"df\")] ) def add_10(df):     return df.withColumn(\"a\", col(\"a\") + 10)  start = time() df = add_10.run(spark) print(df.toPandas()) print(f\"\\nType of dataframe returned {type(df)}\") print(f\"===&gt; Time taken: {round((time() - start)*1000, 2)} milliseconds\") <pre>    a\n0  11\n\nType of dataframe returned &lt;class 'sparkleframe.polarsdf.dataframe.DataFrame'&gt;\n===&gt; Time taken: 33.05 milliseconds\n</pre> <p>Notes:</p> <ul> <li>Sparkleframe is still under development, and not all PySpark operations are currently supported.</li> <li>If you encounter any transformation that is not implemented, please open an issue on GitHub so it can be prioritized.</li> <li>Sparkleframe is especially useful for unit testing, feature prototyping, or serving small pipelines in microservices.</li> </ul> <p>You can learn more about the design motivation behind Sparkleframe in this discussion thread.</p>"},{"location":"notebooks/usage/sparkleframe/#low-latency-transformations-with-sparkleframe","title":"Low-Latency Transformations with Sparkleframe\u00b6","text":"<p>Apache Spark is designed for distributed, large-scale data processing, but it is not optimized for low-latency use cases. There are scenarios, however, where you need to quickly re-compute certain data\u2014for example, regenerating features for a machine learning model in real time or near-real time.</p> <p>For such cases, you can leverage Sparkleframe.</p> <p>What Is Sparkleframe?</p> <p>Sparkleframe is an experimental backend for Flypipe that maps PySpark transformations to Polars DataFrame operations. Polars is a high-performance, multi-threaded DataFrame library written in Rust, and is significantly faster than Spark for small to medium-scale data in local execution.</p> <p>By activating Sparkleframe, your existing Flypipe nodes defined as <code>type=\"pyspark\"</code> will execute using Polars under the hood\u2014without any changes to your transformation logic.</p>"},{"location":"notebooks/usage/sparkleframe/#basic-example-spark-vs-sparkleframe","title":"Basic Example: Spark vs Sparkleframe\u00b6","text":"<p>Below is a simple example using Flypipe and a Spark node:</p>"},{"location":"notebooks/usage/sparkleframe/#enabling-sparkleframe","title":"Enabling Sparkleframe\u00b6","text":"<p>To switch to the Sparkleframe backend and drastically reduce execution time, just activate it at the beginning of your script:</p>"},{"location":"notebooks/usage/unittests/","title":"Unittests","text":"In\u00a0[\u00a0]: Copied! <pre>import pytest\nimport pandas as pd\nfrom pyspark_test import assert_pyspark_df_equal\nfrom pandas.testing import assert_frame_equal\n\nfrom flypipe.node import node\nfrom flypipe.datasource.spark import Spark\nfrom flypipe.schema.column import Column\nfrom flypipe.schema.schema import Schema\nfrom flypipe.schema.types import Long\n\n#Fixtures\n@pytest.fixture(scope=\"function\")\ndef spark():\n    # If running local, please set up your Spark environment\n    from flypipe.tests.spark import build_spark\n    spark = build_spark()\n    # create a temporary view\n    (\n        spark.createDataFrame(\n            schema=(\"c1\", \"c2\", \"c3\"), data=[(1, 2, 3)]\n        ).createOrReplaceTempView(\"dummy_table\")\n    )\n    return spark\n</pre> import pytest import pandas as pd from pyspark_test import assert_pyspark_df_equal from pandas.testing import assert_frame_equal  from flypipe.node import node from flypipe.datasource.spark import Spark from flypipe.schema.column import Column from flypipe.schema.schema import Schema from flypipe.schema.types import Long  #Fixtures @pytest.fixture(scope=\"function\") def spark():     # If running local, please set up your Spark environment     from flypipe.tests.spark import build_spark     spark = build_spark()     # create a temporary view     (         spark.createDataFrame(             schema=(\"c1\", \"c2\", \"c3\"), data=[(1, 2, 3)]         ).createOrReplaceTempView(\"dummy_table\")     )     return spark In\u00a0[\u00a0]: Copied! <pre>class TestPyspark:\n    \n    def test_(self, spark):\n        \"\"\"\n        Pyspark node test\n        \"\"\"\n        \n        expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(1,)])\n\n        @node(\n            type=\"pyspark\",\n            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n            output=Schema([Column(\"c1\", Long())]),\n        )\n        def t1(dummy_table):\n            return dummy_table\n\n        df = t1.run(spark)\n        assert_pyspark_df_equal(df, expected_df)\n        \n    def test_pypsark_with_provided_inputs(self, spark):\n        \"\"\"\n        Pyspark node test with provided inputs\n        \"\"\"\n        \n        dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})\n        expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(10,)])\n\n        @node(\n            type=\"pyspark\",\n            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n            output=Schema([Column(\"c1\", Long())]),\n        )\n        def t1(dummy_table):\n            return dummy_table\n\n        df = t1.run(\n            spark,\n            inputs={\n                Spark(\"dummy_table\"): dummy_table_df\n            }\n        )\n        \n        assert_pyspark_df_equal(df, expected_df)\n</pre> class TestPyspark:          def test_(self, spark):         \"\"\"         Pyspark node test         \"\"\"                  expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(1,)])          @node(             type=\"pyspark\",             dependencies=[Spark(\"dummy_table\").select(\"c1\")],             output=Schema([Column(\"c1\", Long())]),         )         def t1(dummy_table):             return dummy_table          df = t1.run(spark)         assert_pyspark_df_equal(df, expected_df)              def test_pypsark_with_provided_inputs(self, spark):         \"\"\"         Pyspark node test with provided inputs         \"\"\"                  dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})         expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(10,)])          @node(             type=\"pyspark\",             dependencies=[Spark(\"dummy_table\").select(\"c1\")],             output=Schema([Column(\"c1\", Long())]),         )         def t1(dummy_table):             return dummy_table          df = t1.run(             spark,             inputs={                 Spark(\"dummy_table\"): dummy_table_df             }         )                  assert_pyspark_df_equal(df, expected_df) In\u00a0[\u00a0]: Copied! <pre>class TestPandasOnSpark:\n    \n    def test_pandas_on_spark_node(self, spark):\n        \"\"\"\n        Pandas on Spark node test\n        \"\"\"\n        \n        expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(1,)]).pandas_api()\n\n        @node(\n            type=\"pandas_on_spark\",\n            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n            output=Schema([Column(\"c1\", Long())]),\n        )\n        def t1(dummy_table):\n            return dummy_table\n\n        df = t1.run(spark)\n        assert_pyspark_df_equal(df.to_spark(), expected_df.to_spark())\n        \n    def test_pandas_on_spark_node_without_spark_context(self):\n        \"\"\"\n        Pandas on Spark node test\n        \"\"\"\n        \n        dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})\n        expected_df = pd.DataFrame(data={'c1': [10]})\n\n        @node(\n            type=\"pandas_on_spark\",\n            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n            output=Schema([Column(\"c1\", Long())]),\n        )\n        def t1(dummy_table):\n            return dummy_table\n\n        df = t1.run(\n            pandas_on_spark_use_pandas=True, # &lt;-- \n            inputs={\n                Spark(\"dummy_table\"): dummy_table_df\n            }\n        )\n        assert_frame_equal(df, expected_df)        \n</pre> class TestPandasOnSpark:          def test_pandas_on_spark_node(self, spark):         \"\"\"         Pandas on Spark node test         \"\"\"                  expected_df = spark.createDataFrame(schema=(\"c1\",), data=[(1,)]).pandas_api()          @node(             type=\"pandas_on_spark\",             dependencies=[Spark(\"dummy_table\").select(\"c1\")],             output=Schema([Column(\"c1\", Long())]),         )         def t1(dummy_table):             return dummy_table          df = t1.run(spark)         assert_pyspark_df_equal(df.to_spark(), expected_df.to_spark())              def test_pandas_on_spark_node_without_spark_context(self):         \"\"\"         Pandas on Spark node test         \"\"\"                  dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})         expected_df = pd.DataFrame(data={'c1': [10]})          @node(             type=\"pandas_on_spark\",             dependencies=[Spark(\"dummy_table\").select(\"c1\")],             output=Schema([Column(\"c1\", Long())]),         )         def t1(dummy_table):             return dummy_table          df = t1.run(             pandas_on_spark_use_pandas=True, # &lt;--              inputs={                 Spark(\"dummy_table\"): dummy_table_df             }         )         assert_frame_equal(df, expected_df)         In\u00a0[\u00a0]: Copied! <pre>class TestPandasNode:\n    \n    def test_(self, spark):\n        \"\"\"\n        Pandas node test\n        \"\"\"\n        \n        expected_df = pd.DataFrame(data={'c1': [1]})\n\n        @node(\n            type=\"pandas\",\n            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n            output=Schema([Column(\"c1\", Long())]),\n        )\n        def t1(dummy_table):\n            return dummy_table\n\n        df = t1.run(spark)\n        assert_frame_equal(df, expected_df)\n        \n        \n    def test_pandas_with_provided_inputs(self):\n        \"\"\"\n        Pandas node test with provided inputs\n        \n        NOTE: observe that spark is not used here\n        \"\"\"\n        \n        dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})\n        expected_df = pd.DataFrame(data={'c1': [10]})\n\n        @node(\n            type=\"pandas\",\n            dependencies=[Spark(\"dummy_table\").select(\"c1\")],\n            output=Schema([Column(\"c1\", Long())]),\n        )\n        def t1(dummy_table):\n            return dummy_table\n\n        df = t1.run(\n            inputs={\n                Spark(\"dummy_table\"): dummy_table_df\n            }\n        )\n        assert_frame_equal(df, expected_df)\n</pre> class TestPandasNode:          def test_(self, spark):         \"\"\"         Pandas node test         \"\"\"                  expected_df = pd.DataFrame(data={'c1': [1]})          @node(             type=\"pandas\",             dependencies=[Spark(\"dummy_table\").select(\"c1\")],             output=Schema([Column(\"c1\", Long())]),         )         def t1(dummy_table):             return dummy_table          df = t1.run(spark)         assert_frame_equal(df, expected_df)                       def test_pandas_with_provided_inputs(self):         \"\"\"         Pandas node test with provided inputs                  NOTE: observe that spark is not used here         \"\"\"                  dummy_table_df = pd.DataFrame(data={'c1': [10], 'c2': [20], 'c3': [30]})         expected_df = pd.DataFrame(data={'c1': [10]})          @node(             type=\"pandas\",             dependencies=[Spark(\"dummy_table\").select(\"c1\")],             output=Schema([Column(\"c1\", Long())]),         )         def t1(dummy_table):             return dummy_table          df = t1.run(             inputs={                 Spark(\"dummy_table\"): dummy_table_df             }         )         assert_frame_equal(df, expected_df) <p>Running tests (jupyter notebooks only)</p> <p><code>pip install ipytest&gt;=0.13.0</code></p> In\u00a0[\u00a0]: Copied! <pre>import ipytest\nipytest.run()\n</pre> import ipytest ipytest.run()"},{"location":"notebooks/usage/unittests/#unit-testing-a-node","title":"Unit testing a node\u00b6","text":"<p>Requirements:</p> <ul> <li>pytest&gt;=7.1.3</li> <li>pytest-mock&gt;=3.9.0</li> <li>pyspark-test&gt;=0.2.0</li> </ul>"},{"location":"notebooks/usage/unittests/#pyspark-node","title":"Pyspark Node\u00b6","text":""},{"location":"notebooks/usage/unittests/#pandas-on-spark-node","title":"Pandas on Spark Node\u00b6","text":""},{"location":"notebooks/usage/unittests/#pandas-node","title":"Pandas Node\u00b6","text":""}]}